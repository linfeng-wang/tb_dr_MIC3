{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/nn_model_script_emb_test.py - starting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f59bbd39970>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "print('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/nn_model_script_emb_test.py - starting')\n",
    "\n",
    "from array import array\n",
    "from cmath import nan\n",
    "from pyexpat import model\n",
    "import statistics\n",
    "from tkinter.ttk import Separator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchviz import make_dot\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import variable\n",
    "from itertools import chain\n",
    "from sklearn import metrics as met\n",
    "import pickle\n",
    "from icecream import ic\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from importlib import reload\n",
    "# import util\n",
    "# import model_torch_simple\n",
    "# from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "#%%\n",
    "seed = 42\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# train_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/aa_data_train_gene.csv', delimiter = ',')\n",
    "# train_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/mic_aa_train_hml.csv')\n",
    "# train_target = train_target[['EMB_MIC']]\n",
    "# # don't touch test data, split out validation data from training data during training\n",
    "# # test_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_EMB/aa_data_test_pca4k.csv', delimiter = ',')\n",
    "# test_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/aa_data_test_gene.csv', delimiter = ',')\n",
    "# test_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/mic_aa_test_hml.csv')\n",
    "# test_target = test_target[['EMB_MIC']]\n",
    "\n",
    "# all_data = np.concatenate((train_data, test_data), axis=0)\n",
    "# all_target = pd.concat((train_target, test_target), axis=0)\n",
    "\n",
    "# train_data, test_data, train_target, test_target = train_test_split(all_data, all_target, test_size=0.2, random_state=42, stratify=all_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(aa_array, encoded_mic):\n",
    "    # Encode the target variable\n",
    "    # Perform stratified train-test split\n",
    "    train_data, test_data, train_target, test_target = train_test_split(\n",
    "        aa_array,\n",
    "        encoded_mic,\n",
    "        test_size=0.1,  # 10% for testing\n",
    "        stratify=encoded_mic,  # Ensures the proportion of each class is preserved\n",
    "        random_state=42  # For reproducibility\n",
    "    )\n",
    "    return train_data, test_data, train_target, test_target\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "def data_prep_(cryptic, gene_list, dr_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    variants = variants[variants['type'] != 'non_coding_transcript_exon_variant']\n",
    "    overlap = variants[~variants['sample_id'].isin(cryptic['ENA_RUN'])]\n",
    "    overlap = overlap['sample_id'].unique()\n",
    "    print(overlap)\n",
    "    print(overlap.shape)\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    crypticSNPnames = np.load('crypticSNPnames.npy', allow_pickle=True)\n",
    "    variants = variants[variants['SNP'].isin(crypticSNPnames)]\n",
    "    # print(variants.shape)\n",
    "    # print(variants['sample_id'].unique().shape)\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        return output_list\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "    aa = []\n",
    "    dr = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table\n",
    "    for x in tqdm(overlap):\n",
    "    # for x in tqdm(variants['sample_id'].unique()):\n",
    "        if x in variants['sample_id'].tolist():\n",
    "            aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "            if dr_list[0] in variants[variants['sample_id']==x]['drugs'].unique():\n",
    "                dr.append(1)\n",
    "            else:\n",
    "                dr.append(0)\n",
    "        else:\n",
    "            # aa.append([0]*len(all_snp))\n",
    "            pass\n",
    "        # print('SNP')\n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "    # mic_aa = cryptic[cryptic['ENA_RUN'].isin(overlap)]#.iloc[:,14:27]\n",
    "    # # mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # # print(mic_aa.shape)\n",
    "    # # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(overlap)\n",
    "    # # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    # mic_aa = mic_aa.sort_values([“ENA_RUN”])  ## 'sort' changed to 'sort_values'\n",
    "    # # print(mic_aa.shape)return aa_array, dr#, mic_aa\n",
    "    return aa_array, dr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drs = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/all_sample_drs_cryptic_emb.npy')\n",
    "snps = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/all_sample_snps_cryptic_emb.npy')\n",
    "\n",
    "drs = pd.DataFrame(drs)     \n",
    "train_data, test_data, train_target, test_target = data_split(snps, drs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptic_drs = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_sample_drs_cryptic_emb.npy')\n",
    "cryptic_snps = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_snps_cryptic_emb.npy')\n",
    "\n",
    "cryptic_drs = pd.DataFrame(cryptic_drs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resFeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "#     # Ensure target_min and target_max are scalars\n",
    "#     target_min = target_min.item() if isinstance(target_min, np.ndarray) or isinstance(target_min, pd.Series) else target_min\n",
    "#     target_max = target_max.item() if isinstance(target_max, np.ndarray) or isinstance(target_max, pd.Series) else target_max\n",
    "\n",
    "#     # Create a range based on the scalar values of target_min and target_max\n",
    "#     dilution_range = np.arange(target_min - 1, target_max + 2, 1)\n",
    "    \n",
    "#     # Find the index of the target value\n",
    "#     index = np.where(dilution_range == target)[0][0]  # Use np.where to find the index\n",
    "    \n",
    "#     # Check if prediction is within the acceptable range\n",
    "#     return dilution_range[index - 1] <= pred <= dilution_range[index + 1]\n",
    "\n",
    "# Example usage\n",
    "target_min, target_max = cryptic_drs.min().values, cryptic_drs.max()\n",
    "\n",
    "# Load the data\n",
    "cryptic_drs = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_sample_drs_cryptic_emb.npy')\n",
    "cryptic_snps = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_snps_cryptic_emb.npy')\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "cryptic_drs = pd.DataFrame(cryptic_drs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49758029036515616\n",
      "Doubling Dilution Accuracy: 0.00%\n",
      "AUC: 0.7993598298440237\n",
      "Sensitivity: 0.6605263157894737\n",
      "Specificity: 0.9381933438985737\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Combine the features and target variable\n",
    "data = cryptic_snps\n",
    "target = cryptic_drs\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the XGBoost model\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "target_min, target_max = cryptic_drs.min().values, cryptic_drs.max().values\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(y_pred, y_test)])\n",
    "print(f\"Doubling Dilution Accuracy: {doubling_dilution_accuracy * 100:.2f}%\")\n",
    "\n",
    "#testing\n",
    "cutoff = 4\n",
    "test_target_bi = (np.squeeze(np.array(y_test)) >= cutoff).astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.squeeze(np.array(y_pred)) >= cutoff).astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick test with xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot load file containing pickled data when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_859/3802447148.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcryptic_drs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_sample_drs_cryptic_emb.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcryptic_snps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_snps_cryptic_emb.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                 raise ValueError(\"Cannot load file containing pickled data \"\n\u001b[0m\u001b[1;32m    446\u001b[0m                                  \"when allow_pickle=False\")\n\u001b[1;32m    447\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "cryptic_drs = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_sample_drs_cryptic_emb.npy')\n",
    "cryptic_snps = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_snps_cryptic_emb.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_859/1410657295.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtarget_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcryptic_drs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcryptic_drs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "#     # Ensure target_min and target_max are scalars\n",
    "#     target_min = target_min.item() if isinstance(target_min, np.ndarray) or isinstance(target_min, pd.Series) else target_min\n",
    "#     target_max = target_max.item() if isinstance(target_max, np.ndarray) or isinstance(target_max, pd.Series) else target_max\n",
    "\n",
    "#     # Create a range based on the scalar values of target_min and target_max\n",
    "#     dilution_range = np.arange(target_min - 1, target_max + 2, 1)\n",
    "    \n",
    "#     # Find the index of the target value\n",
    "#     index = np.where(dilution_range == target)[0][0]  # Use np.where to find the index\n",
    "    \n",
    "#     # Check if prediction is within the acceptable range\n",
    "#     return dilution_range[index - 1] <= pred <= dilution_range[index + 1]\n",
    "\n",
    "# Example usage\n",
    "target_min, target_max = cryptic_drs.min().values, cryptic_drs.max()\n",
    "\n",
    "# Load the data\n",
    "cryptic_drs = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_sample_drs_cryptic_emb.npy')\n",
    "cryptic_snps = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_snps_cryptic_emb.npy')\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "cryptic_drs = pd.DataFrame(cryptic_drs)\n",
    "\n",
    "# Combine the features and target variable\n",
    "data = cryptic_snps\n",
    "target = cryptic_drs\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the XGBoost model\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "target_min, target_max = cryptic_drs.min(), cryptic_drs.max() \n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(y_pred, y_test)])\n",
    "print(f\"Doubling Dilution Accuracy: {doubling_dilution_accuracy * 100:.2f}%\")\n",
    "\n",
    "#testing\n",
    "cutoff = 4\n",
    "test_target_bi = (np.squeeze(np.array(y_test)) >= cutoff).astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.squeeze(np.array(y_pred)) >= cutoff).astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4958205015398152\n",
      "Doubling Dilution Accuracy: 100.00%\n",
      "AUC: 0.8141102237657637\n",
      "Sensitivity: 0.6885245901639344\n",
      "Specificity: 0.9396958573675931\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "#     # Ensure target_min and target_max are scalars\n",
    "#     target_min = target_min.item() if isinstance(target_min, np.ndarray) or isinstance(target_min, pd.Series) else target_min\n",
    "#     target_max = target_max.item() if isinstance(target_max, np.ndarray) or isinstance(target_max, pd.Series) else target_max\n",
    "\n",
    "#     # Create a range based on the scalar values of target_min and target_max\n",
    "#     dilution_range = np.arange(target_min - 1, target_max + 2, 1)\n",
    "    \n",
    "#     # Find the index of the target value\n",
    "#     index = np.where(dilution_range == target)[0][0]  # Use np.where to find the index\n",
    "    \n",
    "#     # Check if prediction is within the acceptable range\n",
    "#     return dilution_range[index - 1] <= pred <= dilution_range[index + 1]\n",
    "\n",
    "# Example usage\n",
    "target_min, target_max = cryptic_drs.min().values, cryptic_drs.max()\n",
    "\n",
    "# Load the data\n",
    "cryptic_drs = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_sample_drs_cryptic_emb.npy')\n",
    "cryptic_snps = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_snps_cryptic_emb.npy')\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "cryptic_drs = pd.DataFrame(cryptic_drs)\n",
    "\n",
    "# Combine the features and target variable\n",
    "data = cryptic_snps\n",
    "target = cryptic_drs\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the XGBoost model\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "target_min, target_max = cryptic_drs.min().values, cryptic_drs.max().values\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(y_pred, y_test)])\n",
    "print(f\"Doubling Dilution Accuracy: {doubling_dilution_accuracy * 100:.2f}%\")\n",
    "\n",
    "#testing\n",
    "cutoff = 4\n",
    "test_target_bi = (np.squeeze(np.array(y_test)) >= cutoff).astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.squeeze(np.array(y_pred)) >= cutoff).astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### over/undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before resampling: Counter({1.0: 3173, 2.0: 2378, 3.0: 1206, 4.0: 1086, 0.0: 871, 5.0: 375})\n",
      "Class distribution after resampling: Counter({0.0: 375, 1.0: 375, 2.0: 375, 3.0: 375, 4.0: 375, 5.0: 375})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming your data is in a DataFrame called df, with 'outcome' as the target column\n",
    "# Split features (X) and target (y)\n",
    "X = cryptic_snps\n",
    "y = cryptic_drs\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure that y_train is passed as a 1D array for resampling\n",
    "y_train = y_train.squeeze()  # Convert y_train to a Series (1D array) for compatibility with imblearn\n",
    "\n",
    "# Check class distribution\n",
    "counter = Counter(y_train)\n",
    "print(f\"Class distribution before resampling: {counter}\")\n",
    "\n",
    "# Define oversampling strategy: Oversample minority classes to match the majority class\n",
    "over = RandomOverSampler(sampling_strategy='auto' )\n",
    "\n",
    "# Define undersampling strategy: Undersample the majority class to achieve balance\n",
    "under = RandomUnderSampler(sampling_strategy={key: min(counter.values()) for key in counter})\n",
    "\n",
    "# Create a pipeline for both oversampling and undersampling\n",
    "pipeline = Pipeline(steps=[('o', over), ('u', under)])\n",
    "\n",
    "# Apply oversampling and undersampling to the training data\n",
    "X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "# Checking the new class distribution\n",
    "print(\"Class distribution after resampling:\", Counter(y_resampled))\n",
    "\n",
    "# Convert y_resampled back to a DataFrame with one column (if needed)\n",
    "# y_resampled = pd.DataFrame(y_resampled, columns=['outcome'])\n",
    "\n",
    "# You can now proceed with model training on X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3915530136383634\n",
      "Doubling Dilution Accuracy: 0.00%\n",
      "AUC: 0.8049198953524691\n",
      "Sensitivity: 0.674863387978142\n",
      "Specificity: 0.934976402726796\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert to pandas DataFrame\n",
    "cryptic_drs = pd.DataFrame(cryptic_drs)\n",
    "\n",
    "# Combine the features and target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "X_train, y_train= X_resampled, y_resampled\n",
    "# Create the XGBoost model\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "target_min, target_max = cryptic_drs.min().values, cryptic_drs.max().values\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(y_pred, y_test)])\n",
    "print(f\"Doubling Dilution Accuracy: {doubling_dilution_accuracy * 100:.2f}%\")\n",
    "\n",
    "#testing\n",
    "cutoff = 4\n",
    "test_target_bi = (np.squeeze(np.array(y_test)) >= cutoff).astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.squeeze(np.array(y_pred)) >= cutoff).astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from collections import Counter\n",
    "\n",
    "N_samples = train_data.shape[0]\n",
    "DRUGS = train_target.columns\n",
    "# LOCI = train_data.columns\n",
    "assert set(DRUGS) == set(train_target.columns)\n",
    "N_drugs = len(DRUGS)\n",
    "#%%\n",
    "def my_padding(seq_tuple):\n",
    "    list_x_ = list(seq_tuple)\n",
    "    max_len = len(max(list_x_, key=len))\n",
    "    for i, x in enumerate(list_x_):\n",
    "        list_x_[i] = x + \"N\"*(max_len-len(x))\n",
    "    return list_x_\n",
    "\n",
    "#! faster than my_padding try to incorporate\n",
    "def collate_padded_batch(batch):\n",
    "    # get max length of seqs in batch\n",
    "    max_len = max([x[0].shape[1] for x in batch])\n",
    "    return torch.utils.data.default_collate(\n",
    "        [(F.pad(x[0], (0, max_len - x[0].shape[1])), x[1]) for x in batch] #how does F.pad work\n",
    "    )\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset): #? what's the difference between using inheritance and not?\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_df,\n",
    "        res_df,\n",
    "        # target_loci=LOCI,\n",
    "        target_drugs=DRUGS,\n",
    "        one_hot_dtype=torch.int8,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        # self.seq_df = seq_df[target_loci]\n",
    "        self.seq_df = seq_df\n",
    "        self.res_df = res_df[target_drugs]\n",
    "        # if not self.seq_df.index.equals(self.res_df.index):\n",
    "        #     raise ValueError(\n",
    "        #         \"Indices of sequence and resistance dataframes don't match up\"\n",
    "        #     )\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        numerical index --> get `index`-th sample\n",
    "        string index --> get sample with name `index`\n",
    "        \"\"\"\n",
    "        index = int(index)\n",
    "        if isinstance(index, int):\n",
    "            seqs_comb = self.seq_df[index]\n",
    "            mic = self.res_df.iloc[index, 0]\n",
    "            # res = self.res_df.iloc[index, 1]\n",
    "        elif isinstance(index, str):\n",
    "            seqs_comb = self.seq_df[int(index)]\n",
    "            mic = self.res_df.iloc[index, 0]\n",
    "            # res = self.res_df.iloc[index, 1]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Index needs to be an integer or a sample name present in the dataset\"\n",
    "            )\n",
    "\n",
    "        if self.transform:\n",
    "            res = np.log(res)\n",
    "            \n",
    "            # self.res_mean = self.res_df.mean()\n",
    "            # self.res_std = self.res_df.std()\n",
    "            # res = (res - self.res_mean) / self.res_std\n",
    "            # res = self.transform(res)\n",
    "        return torch.unsqueeze(torch.tensor(seqs_comb).float(), 0),  torch.tensor(mic).long().flatten().squeeze()\n",
    "    def __len__(self):\n",
    "        return self.res_df.shape[0]\n",
    "\n",
    "train_dataset = Dataset(train_data, train_target, one_hot_dtype=torch.float, transform=False)\n",
    "val_dataset = Dataset(test_data, test_target, one_hot_dtype=torch.float, transform=False)\n",
    "cryptic_dataset = Dataset(cryptic_snps, cryptic_drs, one_hot_dtype=torch.float, transform=False)\n",
    "\n",
    "# train_dataset, val_dataset = random_split(training_dataset, [int(len(training_dataset)*0.9), len(training_dataset)-int(len(training_dataset)*0.9)])\n",
    "\n",
    "# train_idx, validation_idx = train_test_split(np.arange(len(train_data)),\n",
    "#                                              test_size=0.1,\n",
    "#                                              random_state=42,\n",
    "#                                              shuffle=True,\n",
    "#                                              stratify=train_target)\n",
    "train_idx, validation_idx = train_test_split(np.arange(len(cryptic_drs)),\n",
    "                                            test_size=0.2,\n",
    "                                            random_state=42,\n",
    "                                            shuffle=True,\n",
    "                                            stratify=cryptic_drs)\n",
    "\n",
    "# Subset dataset for train and val\n",
    "train_dataset_cryptic = Subset(cryptic_dataset, train_idx)\n",
    "test_dataset_cryptic = Subset(cryptic_dataset, validation_idx)    \n",
    "\n",
    "\n",
    "# # Subset dataset for train and val\n",
    "# train_dataset = Subset(training_dataset, train_idx)\n",
    "# val_dataset = Subset(training_dataset, validation_idx)\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# # device = 'cpu'\n",
    "\n",
    "y_true = train_target\n",
    "# y_true = pd.concat([train_target, test_target])\n",
    "\n",
    "column_weight_maps = {}\n",
    "\n",
    "for column in y_true.columns:\n",
    "    column_values = y_true[column].dropna().values\n",
    "    values, counts = np.unique(column_values, return_counts=True)\n",
    "    frequency = counts / len(column_values)\n",
    "    \n",
    "    # Calculate weights as the inverse of frequencies\n",
    "    weights_inverse = 1/frequency\n",
    "    # weights_inverse = 1 - frequency\n",
    "    \n",
    "    # Normalize weights to ensure they sum up to 1\n",
    "    weights_normalized = weights_inverse / np.sum(weights_inverse)\n",
    "    \n",
    "    # Map each MIC value to its corresponding weight\n",
    "    weight_map = {value: weight for value, weight in zip(values, weights_normalized)}\n",
    "    \n",
    "    column_weight_maps[column] = weight_map\n",
    "\n",
    "def get_weighted_masked_cross_entropy_loss(column_weight_maps):\n",
    "    \"\"\"\n",
    "    Creates a loss function that computes a weighted cross entropy loss, taking into account class imbalances.\n",
    "    :param column_weight_maps: Dictionary mapping column names to their corresponding class weight maps.\n",
    "    \"\"\"\n",
    "    def weighted_masked_cross_entropy_loss(y_pred, y_true):\n",
    "        # weighted_losses = torch.Tensor().to(device)\n",
    "        weighted_losses = []\n",
    "        col_weight_map = column_weight_maps\n",
    "        # print(col_weight_map)\n",
    "        mean_weight = np.mean(list(col_weight_map.values())) # just in case if a number is not recognised and the loss doesn't go crazy\n",
    "\n",
    "        # print(y_pred.size())\n",
    "        # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        weights_col = [col_weight_map.get(y.item(), mean_weight) for y in y_true]\n",
    "        # print(weights_col)\n",
    "        # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        loss_fn = F.cross_entropy\n",
    "        col_loss = loss_fn(y_pred, y_true, reduction = 'none').to(device)\n",
    "        \n",
    "        # loss_fn = nn.CrossEntropyLoss(reduction = 'none')\n",
    "        # col_loss = loss_fn(y_pred, y_true)\n",
    "        # print(y_true.dtype)\n",
    "        # print(col_loss)\n",
    "        weights_col = torch.Tensor(weights_col).to(device)\n",
    "        # print(weights_col)\n",
    "        # print(col_loss)\n",
    "        weighted_col_loss = weights_col * col_loss\n",
    "        # print(weighted_col_loss)\n",
    "        weighted_losses.append(weighted_col_loss.mean())\n",
    "\n",
    "        total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        \n",
    "        # for i, column in enumerate(column_weight_maps.keys()):\n",
    "        #     col_weight_map = column_weight_maps[column]\n",
    "        #     print(y_pred.size())\n",
    "        #     # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        #     weights_col = torch.tensor([col_weight_map[y.item()] for y in y_true[:, i]], dtype=torch.float32, device=y_true.device)\n",
    "        #     print(weights_col)\n",
    "        #     # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        #     loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        #     col_loss = loss_fn(y_pred[:, i,], y_true[:, i])\n",
    "            \n",
    "        #     weighted_col_loss = weights_col * col_loss\n",
    "        #     weighted_losses.append(weighted_col_loss.mean())\n",
    "        \n",
    "        # total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        return total_weighted_loss\n",
    "\n",
    "    return weighted_masked_cross_entropy_loss\n",
    "\n",
    "# Also assuming `columns` is a list of your target column names corresponding to y_true and y_pred\n",
    "weighted_cross_entropy_loss_fn_mic = get_weighted_masked_cross_entropy_loss(column_weight_maps[0])\n",
    "# weighted_cross_entropy_loss_fn_bi = get_weighted_masked_cross_entropy_loss(column_weight_maps['EMB_MIC_y'])\n",
    "# loss = weighted_cross_entropy_loss_fn(y_true_tensor, y_pred_logits, columns)\n",
    "\n",
    "def save_to_file(file_path, appendix, epoch, lr, cnndr, fcdr, l2, train_loss, test_loss, optimizer, model):\n",
    "    train_loss = [float(arr) for arr in train_loss]\n",
    "    test_loss = [float(arr) for arr in test_loss]\n",
    "    with open(file_path, \"a\") as f:\n",
    "        f.write(f\"#>> {appendix}, Epoch: {epoch}, LR: {lr}, fcDR: {fcdr}\\n\")\n",
    "        f.write(f\"Train_Loss= {train_loss}\\n\")\n",
    "        f.write(f\"Test_Loss= {test_loss}\\n\")\n",
    "        f.write(f\"lossGraph(Train_Loss, Test_Loss, '{appendix}-Epoch-{epoch}-LR-{lr}-fcDR-{fcdr}')\\n\")\n",
    "\n",
    "    torch.save({\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model': model.state_dict(),\n",
    "    }, f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/seq-{appendix}-{epoch}-{lr}-{cnndr}-{fcdr}-{l2}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder class\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, dropout=0.5):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5000, 2000) ,\n",
    "            nn.Dropout(dropout),    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2000, 500),\n",
    "            nn.Dropout(dropout),    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 100),\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(100, 500),\n",
    "            nn.ReLU(),   \n",
    "            nn.Linear(500, 2000),\n",
    "            nn.Dropout(dropout),    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2000, 5000),\n",
    "            nn.Dropout(dropout),    \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5000, input_size),\n",
    "            nn.Sigmoid()  # Use Sigmoid since input is binary\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "def add_noise(inputs, noise_factor=0.3):\n",
    "    noisy = inputs + noise_factor * torch.randn_like(inputs)\n",
    "    noisy = torch.clamp(noisy, 0., 1.)  # Ensure values stay within [0, 1]\n",
    "    return noisy    \n",
    "# criterion = F.cross_entropy\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Train Loss: 0.0010, Val Loss: 0.0011\n",
      "Epoch [20/100], Train Loss: 0.0004, Val Loss: 0.0006\n",
      "Epoch [30/100], Train Loss: 0.0002, Val Loss: 0.0005\n",
      "Epoch [40/100], Train Loss: 0.0002, Val Loss: 0.0002\n",
      "Epoch [50/100], Train Loss: 0.0001, Val Loss: 0.0007\n",
      "Epoch [60/100], Train Loss: 0.0001, Val Loss: 0.0005\n",
      "Epoch [70/100], Train Loss: 0.0001, Val Loss: 0.0005\n",
      "Epoch [80/100], Train Loss: 0.0001, Val Loss: 0.0013\n",
      "Epoch [90/100], Train Loss: 0.0001, Val Loss: 0.0004\n",
      "Epoch [100/100], Train Loss: 0.0001, Val Loss: 0.0004\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Accuracy: 48.73%\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.02      0.03       199\n",
      "           1       0.49      0.90      0.64       714\n",
      "           2       0.46      0.13      0.20       534\n",
      "           3       0.48      0.46      0.47       271\n",
      "           4       0.47      0.54      0.51       244\n",
      "           5       0.68      0.35      0.46        86\n",
      "\n",
      "    accuracy                           0.49      2048\n",
      "   macro avg       0.46      0.40      0.38      2048\n",
      "weighted avg       0.45      0.49      0.42      2048\n",
      "\n",
      "Doubling Dilution Accuracy: 92.19%\n",
      "AUC: 0.780167213461742\n",
      "Sensitivity: 0.6272727272727273\n",
      "Specificity: 0.9330616996507567\n",
      "XGBoost Accuracy: 49.07%\n",
      "\n",
      "XGBoost Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.03      0.05       199\n",
      "           1       0.50      0.89      0.64       714\n",
      "           2       0.47      0.13      0.20       534\n",
      "           3       0.48      0.49      0.48       271\n",
      "           4       0.47      0.55      0.51       244\n",
      "           5       0.70      0.36      0.48        86\n",
      "\n",
      "    accuracy                           0.49      2048\n",
      "   macro avg       0.49      0.41      0.39      2048\n",
      "weighted avg       0.47      0.49      0.42      2048\n",
      "\n",
      "Doubling Dilution Accuracy: 92.63%\n",
      "AUC: 0.7886760503757011\n",
      "Sensitivity: 0.6454545454545455\n",
      "Specificity: 0.9318975552968568\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.profiler\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=8)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "dropout = 0\n",
    "noise_factor = 0\n",
    "train_loader_cryptic = DataLoader(dataset=train_dataset_cryptic, batch_size=batch_size, num_workers=8, drop_last=True)\n",
    "test_loader_cryptic = DataLoader(dataset=test_dataset_cryptic, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True) \n",
    "for lr in [1e-5]: \n",
    "\n",
    "    # with torch.profiler.profile(\n",
    "    #     activities=[\n",
    "    #         torch.profiler.ProfilerActivity.CPU,\n",
    "    #         torch.profiler.ProfilerActivity.CUDA,\n",
    "    #     ],\n",
    "    #     record_shapes=True,\n",
    "    #     with_stack=True\n",
    "    # ) as prof:\n",
    "        # Parameters\n",
    "    input_size = 1473  # Adjust this based on your input vector length\n",
    "    num_epochs = 100\n",
    "\n",
    "    # Instantiate the model, define loss function and optimizer\n",
    "    model = Autoencoder(input_size, dropout).to(device)  # Move model to GPU\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        for data, _ in train_loader:\n",
    "            # print('new batch', data.shape)\n",
    "            data = data.to(device)  # Move data to GPU\n",
    "            noisy_data = add_noise(data, noise_factor)\n",
    "            output = model(noisy_data)\n",
    "            loss = criterion(output, data)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # Disable gradient computation during validation\n",
    "            for data, _ in val_loader:\n",
    "                data = data.to(device)  # Move data to GPU\n",
    "                output = model(data)\n",
    "                loss = criterion(output, data)\n",
    "                val_loss.append(loss.item())\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss[-1]:.4f}, Val Loss: {val_loss[-1]:.4f}')\n",
    "\n",
    "# prof.export_chrome_trace(\"torch_trace.json\")\n",
    "# Plot train loss and val loss\n",
    "\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(val_loss, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'autoencoder.pth')\n",
    "\n",
    "    print('Training complete.')\n",
    "\n",
    "\n",
    "    compressed_data = []\n",
    "    labels = []\n",
    "\n",
    "    test_compressed_data = []\n",
    "    test_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, label in train_loader_cryptic:\n",
    "            data = data.to(device)  # Move data to GPU\n",
    "            compressed = model.encoder(data)\n",
    "            compressed_data.append(compressed.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "            \n",
    "        for data, label in test_loader_cryptic:\n",
    "            data = data.to(device)  # Move data to GPU\n",
    "            compressed = model.encoder(data)\n",
    "            test_compressed_data.append(compressed.cpu().numpy())\n",
    "            test_labels.append(label.numpy())\n",
    "\n",
    "    compressed_data = np.vstack(compressed_data)\n",
    "    compressed_data = compressed_data.squeeze(axis=1)\n",
    "    labels = np.hstack(labels)\n",
    "\n",
    "    test_compressed_data = np.vstack(test_compressed_data)\n",
    "    test_compressed_data = test_compressed_data.squeeze(axis=1)\n",
    "    test_labels = np.hstack(test_labels)\n",
    "\n",
    "        \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    # Split the compressed data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = compressed_data, test_compressed_data, labels, test_labels\n",
    "#MLP\n",
    "    # Initialize and train the MLP classifier\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(500, 500,100, 100, 50, 50), max_iter=500, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    # Predict using the trained MLP\n",
    "    y_pred = mlp.predict(X_test)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    target_min, target_max = labels.min(), labels.max()\n",
    "\n",
    "    doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(y_pred, y_test)])\n",
    "    print(f\"Doubling Dilution Accuracy: {doubling_dilution_accuracy * 100:.2f}%\")\n",
    "    \n",
    "    #testing\n",
    "    cutoff = 4\n",
    "    test_target_bi = (np.squeeze(np.array(y_test)) >= cutoff).astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "    test_predictions_bi = (np.squeeze(np.array( y_pred)) >= cutoff).astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "    auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "    print(\"AUC:\", auc)\n",
    "\n",
    "    # Calculate confusion matrix components\n",
    "    tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "    # Calculate sensitivity (recall)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "    # Calculate specificity\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(\"Specificity:\", specificity)\n",
    "\n",
    "#XGB\n",
    "    # Initialize and train the XGBoost classifier\n",
    "    xgb_model = XGBClassifier()\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict using the trained XGBoost model\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "    print(f\"XGBoost Accuracy: {accuracy_xgb * 100:.2f}%\")\n",
    "    print(\"\\nXGBoost Classification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred_xgb))\n",
    "    \n",
    "    doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(y_pred_xgb, y_test)])\n",
    "    print(f\"Doubling Dilution Accuracy: {doubling_dilution_accuracy * 100:.2f}%\")\n",
    "    \n",
    "    #testing\n",
    "    cutoff = 4\n",
    "    test_target_bi = (np.squeeze(np.array(y_test)) >= cutoff).astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "    test_predictions_bi = (np.squeeze(np.array( y_pred_xgb)) >= cutoff).astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "    auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "    print(\"AUC:\", auc)\n",
    "\n",
    "    # Calculate confusion matrix components\n",
    "    tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "    # Calculate sensitivity (recall)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "    # Calculate specificity\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(\"Specificity:\", specificity)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptic_drs = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_sample_drs_cryptic_emb.npy')\n",
    "cryptic_snps = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_snps_cryptic_emb.npy')\n",
    "\n",
    "cryptic_drs = pd.DataFrame(cryptic_drs)\n",
    "cryptic_bi = pd.DataFrame(np.array([1 if x >= 4 else 0 for x in cryptic_drs.values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XGBClassifier' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32363/1785233344.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader_cryptic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move data to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mcompressed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mcompressed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompressed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'XGBClassifier' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "compressed_data = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in train_loader_cryptic:\n",
    "        data = data.to(device)  # Move data to GPU\n",
    "        compressed = model.encoder(data)\n",
    "        compressed_data.append(compressed.cpu().numpy())\n",
    "        labels.append(label.numpy())\n",
    "                \n",
    "    for data, label in test_loader_cryptic:\n",
    "        data = data.to(device)  # Move data to GPU\n",
    "        compressed = model.encoder(data)\n",
    "        compressed_data.append(compressed.cpu().numpy())\n",
    "        labels.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_series_all = pd.merge(cryptic_drs, cryptic_bi, left_index=True, right_index=True)\n",
    "train_data, test_data, train_target, test_target = data_split(cryptic_snps, mic_series_all)\n",
    "target_min, target_max = cryptic_drs.min().values, cryptic_drs.max().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from collections import Counter\n",
    "\n",
    "N_samples = train_data.shape[0]\n",
    "DRUGS = train_target.columns\n",
    "# LOCI = train_data.columns\n",
    "assert set(DRUGS) == set(train_target.columns)\n",
    "N_drugs = len(DRUGS)\n",
    "#%%\n",
    "def my_padding(seq_tuple):\n",
    "    list_x_ = list(seq_tuple)\n",
    "    max_len = len(max(list_x_, key=len))\n",
    "    for i, x in enumerate(list_x_):\n",
    "        list_x_[i] = x + \"N\"*(max_len-len(x))\n",
    "    return list_x_\n",
    "\n",
    "#! faster than my_padding try to incorporate\n",
    "def collate_padded_batch(batch):\n",
    "    # get max length of seqs in batch\n",
    "    max_len = max([x[0].shape[1] for x in batch])\n",
    "    return torch.utils.data.default_collate(\n",
    "        [(F.pad(x[0], (0, max_len - x[0].shape[1])), x[1]) for x in batch] #how does F.pad work\n",
    "    )\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset): #? what's the difference between using inheritance and not?\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_df,\n",
    "        res_df,\n",
    "        # target_loci=LOCI,\n",
    "        target_drugs=DRUGS,\n",
    "        one_hot_dtype=torch.int8,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        # self.seq_df = seq_df[target_loci]\n",
    "        self.seq_df = seq_df\n",
    "        self.res_df = res_df[target_drugs]\n",
    "        # if not self.seq_df.index.equals(self.res_df.index):\n",
    "        #     raise ValueError(\n",
    "        #         \"Indices of sequence and resistance dataframes don't match up\"\n",
    "        #     )\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        numerical index --> get `index`-th sample\n",
    "        string index --> get sample with name `index`\n",
    "        \"\"\"\n",
    "        index = int(index)\n",
    "        if isinstance(index, int):\n",
    "            seqs_comb = self.seq_df[index]\n",
    "            mic = self.res_df.iloc[index, 0]\n",
    "            res = self.res_df.iloc[index, 1]\n",
    "        elif isinstance(index, str):\n",
    "            seqs_comb = self.seq_df[int(index)]\n",
    "            mic = self.res_df.iloc[index, 0]\n",
    "            res = self.res_df.iloc[index, 1]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Index needs to be an integer or a sample name present in the dataset\"\n",
    "            )\n",
    "\n",
    "        if self.transform:\n",
    "            res = np.log(res)\n",
    "            \n",
    "            # self.res_mean = self.res_df.mean()\n",
    "            # self.res_std = self.res_df.std()\n",
    "            # res = (res - self.res_mean) / self.res_std\n",
    "            # res = self.transform(res)\n",
    "        return torch.unsqueeze(torch.tensor(seqs_comb).float(), 0),  torch.tensor(mic).long().flatten().squeeze(), torch.tensor(res).long().flatten().squeeze()\n",
    "    def __len__(self):\n",
    "        return self.res_df.shape[0]\n",
    "\n",
    "training_dataset = Dataset(train_data, train_target, one_hot_dtype=torch.float, transform=False)\n",
    "# train_dataset, val_dataset = random_split(training_dataset, [int(len(training_dataset)*0.9), len(training_dataset)-int(len(training_dataset)*0.9)])\n",
    "\n",
    "train_idx, validation_idx = train_test_split(np.arange(len(train_data)),\n",
    "                                             test_size=0.1,\n",
    "                                             random_state=42,\n",
    "                                             shuffle=True,\n",
    "                                             stratify=train_target)\n",
    "\n",
    "# Subset dataset for train and val\n",
    "train_dataset = Subset(training_dataset, train_idx)\n",
    "val_dataset = Subset(training_dataset, validation_idx)\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# # device = 'cpu'\n",
    "\n",
    "y_true = train_target\n",
    "# y_true = pd.concat([train_target, test_target])\n",
    "\n",
    "column_weight_maps = {}\n",
    "\n",
    "for column in y_true.columns:\n",
    "    column_values = y_true[column].dropna().values\n",
    "    values, counts = np.unique(column_values, return_counts=True)\n",
    "    frequency = counts / len(column_values)\n",
    "    \n",
    "    # Calculate weights as the inverse of frequencies\n",
    "    weights_inverse = 1/frequency\n",
    "    # weights_inverse = 1 - frequency\n",
    "    \n",
    "    # Normalize weights to ensure they sum up to 1\n",
    "    weights_normalized = weights_inverse / np.sum(weights_inverse)\n",
    "    \n",
    "    # Map each MIC value to its corresponding weight\n",
    "    weight_map = {value: weight for value, weight in zip(values, weights_normalized)}\n",
    "    \n",
    "    column_weight_maps[column] = weight_map\n",
    "\n",
    "def get_weighted_masked_cross_entropy_loss(column_weight_maps):\n",
    "    \"\"\"\n",
    "    Creates a loss function that computes a weighted cross entropy loss, taking into account class imbalances.\n",
    "    :param column_weight_maps: Dictionary mapping column names to their corresponding class weight maps.\n",
    "    \"\"\"\n",
    "    def weighted_masked_cross_entropy_loss(y_pred, y_true):\n",
    "        # weighted_losses = torch.Tensor().to(device)\n",
    "        weighted_losses = []\n",
    "        col_weight_map = column_weight_maps\n",
    "        # print(col_weight_map)\n",
    "        mean_weight = np.mean(list(col_weight_map.values())) # just in case if a number is not recognised and the loss doesn't go crazy\n",
    "\n",
    "        # print(y_pred.size())\n",
    "        # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        weights_col = [col_weight_map.get(y.item(), mean_weight) for y in y_true]\n",
    "        # print(weights_col)\n",
    "        # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        loss_fn = F.cross_entropy\n",
    "        col_loss = loss_fn(y_pred, y_true, reduction = 'none').to(device)\n",
    "        \n",
    "        # loss_fn = nn.CrossEntropyLoss(reduction = 'none')\n",
    "        # col_loss = loss_fn(y_pred, y_true)\n",
    "        # print(y_true.dtype)\n",
    "        # print(col_loss)\n",
    "        weights_col = torch.Tensor(weights_col).to(device)\n",
    "        # print(weights_col)\n",
    "        # print(col_loss)\n",
    "        weighted_col_loss = weights_col * col_loss\n",
    "        # print(weighted_col_loss)\n",
    "        weighted_losses.append(weighted_col_loss.mean())\n",
    "\n",
    "        total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        \n",
    "        # for i, column in enumerate(column_weight_maps.keys()):\n",
    "        #     col_weight_map = column_weight_maps[column]\n",
    "        #     print(y_pred.size())\n",
    "        #     # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        #     weights_col = torch.tensor([col_weight_map[y.item()] for y in y_true[:, i]], dtype=torch.float32, device=y_true.device)\n",
    "        #     print(weights_col)\n",
    "        #     # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        #     loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        #     col_loss = loss_fn(y_pred[:, i,], y_true[:, i])\n",
    "            \n",
    "        #     weighted_col_loss = weights_col * col_loss\n",
    "        #     weighted_losses.append(weighted_col_loss.mean())\n",
    "        \n",
    "        # total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        return total_weighted_loss\n",
    "\n",
    "    return weighted_masked_cross_entropy_loss\n",
    "\n",
    "# Also assuming `columns` is a list of your target column names corresponding to y_true and y_pred\n",
    "weighted_cross_entropy_loss_fn_mic = get_weighted_masked_cross_entropy_loss(column_weight_maps['EMB_MIC_x'])\n",
    "weighted_cross_entropy_loss_fn_bi = get_weighted_masked_cross_entropy_loss(column_weight_maps['EMB_MIC_y'])\n",
    "# loss = weighted_cross_entropy_loss_fn(y_true_tensor, y_pred_logits, columns)\n",
    "\n",
    "def save_to_file(file_path, appendix, epoch, lr, cnndr, fcdr, l2, train_loss, test_loss, optimizer, model):\n",
    "    train_loss = [float(arr) for arr in train_loss]\n",
    "    test_loss = [float(arr) for arr in test_loss]\n",
    "    with open(file_path, \"a\") as f:\n",
    "        f.write(f\"#>> {appendix}, Epoch: {epoch}, LR: {lr}, fcDR: {fcdr}\\n\")\n",
    "        f.write(f\"Train_Loss= {train_loss}\\n\")\n",
    "        f.write(f\"Test_Loss= {test_loss}\\n\")\n",
    "        f.write(f\"lossGraph(Train_Loss, Test_Loss, '{appendix}-Epoch-{epoch}-LR-{lr}-fcDR-{fcdr}')\\n\")\n",
    "\n",
    "    torch.save({\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model': model.state_dict(),\n",
    "    }, f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/seq-{appendix}-{epoch}-{lr}-{cnndr}-{fcdr}-{l2}.pth')\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        num_classes=6,\n",
    "        num_filters=64,\n",
    "        filter_length=25,\n",
    "        num_conv_layers=2,\n",
    "        filter_scaling_factor=1,  # New parameter\n",
    "        num_dense_neurons=256,\n",
    "        num_dense_layers=2,\n",
    "        conv_dropout_rate=0.0,\n",
    "        dense_dropout_rate=0.2,\n",
    "        l1_strength = 0.1,\n",
    "        return_logits=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_length = filter_length\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.num_dense_layers = num_dense_layers\n",
    "        self.conv_dropout_rate = conv_dropout_rate\n",
    "        self.dense_dropout_rate = dense_dropout_rate\n",
    "        self.return_logits = return_logits\n",
    "        \n",
    "        # now define the actual model\n",
    "        # self.feature_extraction_layer = self._conv_layer(\n",
    "            # in_channels, num_filters, filter_length\n",
    "        # )\n",
    "        self.feature_extraction_layer = self._conv_layer_extract(\n",
    "            in_channels, num_filters, filter_length\n",
    "        )\n",
    "        #dynamic filter scaling from deepram\n",
    "        current_num_filters1 = num_filters\n",
    "        self.conv_layers1 = nn.ModuleList()\n",
    "        for i in range(num_conv_layers):\n",
    "            layer = self._conv_layer(current_num_filters1, int(current_num_filters1 * filter_scaling_factor), 3)\n",
    "            self.conv_layers1.append(layer)\n",
    "            current_num_filters1 = int(current_num_filters1 * filter_scaling_factor)\n",
    "            \n",
    "        current_num_filters2 = 32\n",
    "        self.conv_layers2 = nn.ModuleList()\n",
    "        for i in range(num_conv_layers):\n",
    "            layer = self._conv_layer(current_num_filters1, int(current_num_filters2 * filter_scaling_factor), 3)\n",
    "            self.conv_layers2.append(layer)\n",
    "            current_num_filters1 = current_num_filters2\n",
    "            \n",
    "        self.dense_layers = nn.ModuleList(\n",
    "            self._dense_layer(input_dim, num_dense_neurons)\n",
    "            for input_dim in [53568]\n",
    "            + [num_dense_neurons] * (num_dense_layers - 1) #how does this work?\n",
    "        )\n",
    "        \n",
    "        # self.dense_layers = nn.ModuleList(\n",
    "            # self._dense_layer(input_dim, num_dense_neurons)\n",
    "            # for input_dim in [current_num_filters2]\n",
    "            # + [num_dense_neurons] * (num_dense_layers - 1) #how does this work?\n",
    "        # )\n",
    "        \n",
    "        self.prediction_layer = (\n",
    "            nn.Linear(num_dense_neurons, num_classes)\n",
    "            if return_logits\n",
    "            else nn.Sequential(nn.Linear(num_dense_neurons, num_classes), nn.ReLU()) \n",
    "        )\n",
    "        \n",
    "        self.prediction_layer_bi = (\n",
    "            nn.Linear(num_dense_neurons, 2)\n",
    "            if return_logits\n",
    "            else nn.Sequential(nn.Linear(num_dense_neurons, 2), nn.ReLU()) \n",
    "        )\n",
    "        \n",
    "        self.m = nn.MaxPool1d(3, stride=1)\n",
    "        \n",
    "        self.apply(self.init_weights)    \n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _conv_layer(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(p=self.conv_dropout_rate),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def _conv_layer_extract(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def _dense_layer(self, n_in, n_out):\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(p=self.dense_dropout_rate),\n",
    "            nn.Linear(n_in, n_out),\n",
    "            nn.BatchNorm1d(n_out),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def l1_regularization(self):\n",
    "        l1_loss_example = 0\n",
    "        for param in self.parameters():\n",
    "            l1_loss_example += torch.sum(torch.abs(param))\n",
    "        return self.l1_strength * l1_loss_example\n",
    "\n",
    "    def forward(self, x):\n",
    "        # first pass over input\n",
    "        # print(x.size())\n",
    "        # print(\"Input shape:\", x.shape)\n",
    "        x = self.feature_extraction_layer(x)\n",
    "        # print(\"After feature extraction shape:\", x.shape)\n",
    "\n",
    "        # conv layers\n",
    "        for layer in self.conv_layers1:\n",
    "            x = layer(x)\n",
    "        # global max pool 1D\n",
    "        x = self.m(x)\n",
    "        # print(x.shape)\n",
    "        for layer in self.conv_layers2:\n",
    "            x = layer(x)\n",
    "        x = self.m(x)\n",
    "        \n",
    "        # x = torch.max(x, dim=-1).values\n",
    "        x = x.view(x.size(0), -1)  # Flattening the tensor to [batch_size, features]\n",
    "        # ic(x.shape)\n",
    "        # fully connected layers\n",
    "        for layer in self.dense_layers:\n",
    "            x = layer(x)\n",
    "        x_mic = self.prediction_layer(x)\n",
    "        x_bi = self.prediction_layer_bi(x)\n",
    "        \n",
    "        return x_mic, x_bi\n",
    "\n",
    "# def l1loss(layer): # https://stackoverflow.com/questions/50054049/lack-of-sparse-solution-with-l1-regularization-in-pytorch\n",
    "#     return torch.norm(layer.weight, p=1)\n",
    "\n",
    "# def l1loss(sequence):\n",
    "#     l1_regularization = 0\n",
    "#     for module in sequence.modules():\n",
    "#         if isinstance(module, nn.Conv1d):  # Check if the module is a Conv1d layer\n",
    "#             l1_regularization += torch.norm(module.weight, p=1)\n",
    "#     return l1_regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in [1e-7, 1e-4]:\n",
    "    print('***New input parameter***')\n",
    "    lr = lr\n",
    "    epoch = 600\n",
    "    conv_dropout_rate=0.5\n",
    "    dense_dropout_rate=0\n",
    "    weight_decay=0\n",
    "    ######################################\n",
    "\n",
    "    model = Model(\n",
    "    num_classes=6,\n",
    "    num_filters=64,\n",
    "    num_conv_layers=2,\n",
    "    # num_dense_neurons=256, # batch_size = 64\n",
    "    num_dense_neurons=128, # batch_size = 64\n",
    "    num_dense_layers=2,\n",
    "    return_logits=True,\n",
    "    conv_dropout_rate=conv_dropout_rate,\n",
    "    dense_dropout_rate=dense_dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "    patience_counter = 0\n",
    "    lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "    batch_size_train = 32\n",
    "    batch_size_val = 256\n",
    "    # lr = 0.0085\n",
    "    # lr = 0.00002\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size_train, shuffle=True ,num_workers=8, drop_last=True)\n",
    "    test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size_val, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "    criterion_mic = weighted_cross_entropy_loss_fn_mic\n",
    "    criterion_bi = weighted_cross_entropy_loss_fn_bi\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc; gc.collect()\n",
    "    # ic.enable()\n",
    "    ic.disable()\n",
    "\n",
    "    train_epoch_loss = []\n",
    "    test_epoch_loss = []\n",
    "\n",
    "    for e in tqdm(range(1, epoch+1)):\n",
    "        model.train()\n",
    "        train_batch_loss = []\n",
    "        test_batch_loss = []\n",
    "        # print(f'Epoch {e}')\n",
    "        for x_train, y_train_mic, y_train_bi in train_loader:\n",
    "            x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "            y_batch_mic = y_train_mic.to(device)\n",
    "            y_batch_bi = y_train_bi.to(device)\n",
    "            \n",
    "            x_batch = x_batch.float()\n",
    "            pred_mic, pred_bi = model(x_batch.float())\n",
    "\n",
    "            # break\n",
    "            # loss_train = loss_corn(pred, y_batch, 3, class_weights)\n",
    "            loss_train_mic = criterion_mic(pred_mic,y_batch_mic)\n",
    "            loss_train_bi = criterion_bi(pred_bi,y_batch_bi)\n",
    "            loss_train = loss_train_mic + loss_train_bi\n",
    "            # print(pred)\n",
    "            # print(y_batch)\n",
    "            # print(loss_train)\n",
    "            train_batch_loss.append(loss_train)        \n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()  # Update the learning rate\n",
    "            # break\n",
    "        train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "        \n",
    "        pred_mic_list = []\n",
    "        pred_bi_list = []\n",
    "        target_mic_list  = []\n",
    "        target_bi_list  = []\n",
    "        mse_list = []\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # print('>> test')\n",
    "            for x_test, y_test_mic, y_test_bi in test_loader:\n",
    "                x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "                x_batch = x_batch.float()\n",
    "                y_batch_mic = y_test_mic.to(device)\n",
    "                y_batch_bi = y_test_bi.to(device)\n",
    "                # print(x_batch.size())\n",
    "                # y_batch = torch.Tensor.float(y).to(device)\n",
    "                # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred_mic, pred_bi = model(x_batch.float())   \n",
    "        \n",
    "            # pred_mic_list.append(np.argmax(pred_mic.detach().cpu().numpy())) \n",
    "            # pred_bi_list.append(np.argmax(pred_bi.detach().cpu().numpy())) \n",
    "            # target_mic_list.append(y_test_mic.detach().cpu().numpy())\n",
    "            # target_bi_list.append(y_test_bi.detach().cpu().numpy())\n",
    "\n",
    "            # break\n",
    "            # loss_train = loss_corn(pred, y_batch, 3, class_weights)\n",
    "            loss_test_mic = criterion_mic(pred_mic,y_batch_mic)\n",
    "            loss_test_bi = criterion_bi(pred_bi,y_batch_bi)\n",
    "            loss_test = loss_test_mic + loss_test_bi\n",
    "            \n",
    "                # pred = pred.unsqueeze(0)\n",
    "                # print(pred[:10])\n",
    "                # print(y_batch[:10])\n",
    "\n",
    "                # loss_test = loss_corn(pred, y_batch, 3, class_weights)\n",
    "            test_batch_loss.append(loss_test)\n",
    "            test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "        if e%50 == 0:\n",
    "            print(f'Epoch {e}')\n",
    "            print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "            print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "        # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "\n",
    "\n",
    "    print('==='*10)\n",
    "    # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "    save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "                train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    x = np.arange(1, epoch+1, 1)\n",
    "    ax.plot(x, train_epoch_loss,label='Training')\n",
    "    ax.plot(x, test_epoch_loss,label='Validation')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Number of Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "    ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "    # ax_2 = ax.twinx()\n",
    "    # ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "    # ax_2.set_yscale(\"log\")\n",
    "    # ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "    ax.grid(axis=\"x\")\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "    print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "    # #%%\n",
    "\n",
    "    #%%\n",
    "    testing_dataset = Dataset(test_data, test_target, one_hot_dtype=torch.float, transform=False)\n",
    "    testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "    model.eval()  # For inference\n",
    "\n",
    "    ic.disable()\n",
    "    model.eval()\n",
    "    pred_mic_list = []\n",
    "    pred_bi_list = []\n",
    "    target_mic_list  = []\n",
    "    target_bi_list  = []\n",
    "    mse_list = []\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test_mic, y_test_bi in testing_loader1:\n",
    "            xtest1 = x_test.to(device).float()\n",
    "            # ytest1_mic = y_test_mic.to(device).float()\n",
    "            # ytest1_bi = y_test_bi.to(device).float()\n",
    "            pred_mic, pred_bi = model(xtest1)\n",
    "            pred_mic_list.append(np.argmax(pred_mic.detach().cpu().numpy())) \n",
    "            pred_bi_list.append(np.argmax(pred_bi.detach().cpu().numpy())) \n",
    "            target_mic_list.append(y_test_mic.detach().cpu().numpy())\n",
    "            target_bi_list.append(y_test_bi.detach().cpu().numpy())\n",
    "    target_list = np.array(target_list).flatten()\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "\n",
    "    def calculate_metrics(true_labels, predictions):\n",
    "        \"\"\"\n",
    "        Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "        Parameters:\n",
    "        - true_labels: List or array of true labels\n",
    "        - predictions: List or array of predicted labels\n",
    "\n",
    "        Returns:\n",
    "        - accuracy: Overall accuracy of predictions\n",
    "        - f1: Weighted average F1 score\n",
    "        - conf_matrix: Multiclass confusion matrix\n",
    "        - mae: Mean Absolute Error of predictions\n",
    "        \"\"\"\n",
    "        # Ensure inputs are numpy arrays for consistency\n",
    "        true_labels = np.array(true_labels).squeeze()\n",
    "        predictions = np.array(predictions).squeeze()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "        \n",
    "        if conf_matrix.shape[0] > 2:\n",
    "            pass\n",
    "        else:        \n",
    "            tn, fp, fn, tp = conf_matrix.ravel()\n",
    "            # Calculate sensitivity (recall)\n",
    "            sensitivity = tp / (tp + fn)\n",
    "            print(\"Sensitivity:\", sensitivity)\n",
    "            # Calculate specificity\n",
    "            specificity = tn / (tn + fp)\n",
    "            print(\"Specificity:\", specificity)\n",
    "\n",
    "        # Calculate MAE\n",
    "        mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "        return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "    accuracy, f1, conf_matrix, mae = calculate_metrics(target_bi_list, pred_bi_list)\n",
    "\n",
    "    print(\"======================\")\n",
    "    # print(\"Model's Named Parameters:\")\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     print(f\"Name: {name}\")\n",
    "    #     print(f\"Shape: {param.size()}\")\n",
    "    #     print(f\"Requires grad: {param.requires_grad}\")\n",
    "    #     print('-----')\n",
    "    print(\"Optimizer details:\")\n",
    "    print(optimizer)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"Learning rate:\", param_group['lr'])\n",
    "        print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "        \n",
    "    print(\"======================\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Mae: {mae}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"conf_matrix: {conf_matrix}\")\n",
    "    print(\"======================\")\n",
    "    doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_mic_list, target_mic_list)])\n",
    "    \n",
    "    \n",
    "    print('** EA terms:')   f\n",
    "    print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "    # Calculate AUC\n",
    "    cutoff = 4\n",
    "    test_target_bi = (np.squeeze(np.array(target_mic_list)) >= cutoff).astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "    test_predictions_bi = (np.squeeze(np.array(pred_mic_list)) >= cutoff).astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "    auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "    print(\"AUC:\", auc)\n",
    "\n",
    "    # Calculate confusion matrix components\n",
    "    tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "    # Calculate sensitivity (recall)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "    # Calculate specificity\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Autoencoder:\n\tsize mismatch for encoder.0.weight: copying a param with shape torch.Size([10000, 1473]) from checkpoint, the shape in current model is torch.Size([5000, 1473]).\n\tsize mismatch for encoder.0.bias: copying a param with shape torch.Size([10000]) from checkpoint, the shape in current model is torch.Size([5000]).\n\tsize mismatch for encoder.2.weight: copying a param with shape torch.Size([2000, 10000]) from checkpoint, the shape in current model is torch.Size([2000, 5000]).\n\tsize mismatch for decoder.4.weight: copying a param with shape torch.Size([10000, 2000]) from checkpoint, the shape in current model is torch.Size([5000, 2000]).\n\tsize mismatch for decoder.4.bias: copying a param with shape torch.Size([10000]) from checkpoint, the shape in current model is torch.Size([5000]).\n\tsize mismatch for decoder.6.weight: copying a param with shape torch.Size([1473, 10000]) from checkpoint, the shape in current model is torch.Size([1473, 5000]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32363/1016365165.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load saved parameters into the model instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoencoder.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m train_idx, validation_idx = train_test_split(np.arange(len(cryptic_drs)),\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1604\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1605\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Autoencoder:\n\tsize mismatch for encoder.0.weight: copying a param with shape torch.Size([10000, 1473]) from checkpoint, the shape in current model is torch.Size([5000, 1473]).\n\tsize mismatch for encoder.0.bias: copying a param with shape torch.Size([10000]) from checkpoint, the shape in current model is torch.Size([5000]).\n\tsize mismatch for encoder.2.weight: copying a param with shape torch.Size([2000, 10000]) from checkpoint, the shape in current model is torch.Size([2000, 5000]).\n\tsize mismatch for decoder.4.weight: copying a param with shape torch.Size([10000, 2000]) from checkpoint, the shape in current model is torch.Size([5000, 2000]).\n\tsize mismatch for decoder.4.bias: copying a param with shape torch.Size([10000]) from checkpoint, the shape in current model is torch.Size([5000]).\n\tsize mismatch for decoder.6.weight: copying a param with shape torch.Size([1473, 10000]) from checkpoint, the shape in current model is torch.Size([1473, 5000])."
     ]
    }
   ],
   "source": [
    "input_size = 1473  \n",
    "\n",
    "model = Autoencoder(input_size).to(device)  # Move model to GPU\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Load saved parameters into the model instance\n",
    "model.load_state_dict(torch.load('autoencoder.pth'))\n",
    "\n",
    "train_loader_cryptic = DataLoader(dataset=train_dataset_cryptic, batch_size=batch_size, num_workers=8, drop_last=True)\n",
    "test_loader_cryptic = DataLoader(dataset=test_dataset_cryptic, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True) \n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "compressed_data = []\n",
    "labels = []\n",
    "\n",
    "test_compressed_data = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in train_loader_cryptic:\n",
    "        data = data.to(device)  # Move data to GPU\n",
    "        compressed = model.encoder(data)\n",
    "        compressed_data.append(compressed.cpu().numpy())\n",
    "        labels.append(label.numpy())\n",
    "        \n",
    "    for data, label in test_loader_cryptic:\n",
    "        data = data.to(device)  # Move data to GPU\n",
    "        compressed = model.encoder(data)\n",
    "        test_compressed_data.append(compressed.cpu().numpy())\n",
    "        test_labels.append(label.numpy())\n",
    "\n",
    "compressed_data = np.vstack(compressed_data)\n",
    "compressed_data = compressed_data.squeeze(axis=1)\n",
    "labels = np.hstack(labels)\n",
    "\n",
    "test_compressed_data = np.vstack(test_compressed_data)\n",
    "test_compressed_data = test_compressed_data.squeeze(axis=1)\n",
    "test_labels = np.hstack(test_labels)\n",
    "\n",
    "    \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Split the compressed data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = compressed_data, test_compressed_data, labels, test_labels\n",
    "\n",
    "# Initialize and train the MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(500, 100, 50), max_iter=500, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained MLP\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Initialize and train the XGBoost classifier\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained XGBoost model\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb * 100:.2f}%\")\n",
    "print(\"\\nXGBoost Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 37.01%\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       206\n",
      "           1       0.40      0.91      0.55       715\n",
      "           2       0.00      0.00      0.00       528\n",
      "           3       0.00      0.00      0.00       271\n",
      "           4       0.26      0.43      0.33       244\n",
      "           5       0.00      0.00      0.00        84\n",
      "\n",
      "    accuracy                           0.37      2048\n",
      "   macro avg       0.11      0.22      0.15      2048\n",
      "weighted avg       0.17      0.37      0.23      2048\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 46.53%\n",
      "\n",
      "XGBoost Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.01      0.03       206\n",
      "           1       0.49      0.89      0.63       715\n",
      "           2       0.37      0.12      0.18       528\n",
      "           3       0.45      0.44      0.44       271\n",
      "           4       0.42      0.46      0.44       244\n",
      "           5       0.63      0.29      0.39        84\n",
      "\n",
      "    accuracy                           0.47      2048\n",
      "   macro avg       0.44      0.37      0.35      2048\n",
      "weighted avg       0.43      0.47      0.39      2048\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in train_loader_cryptic:\n",
    "        data = data.to(device)  # Move data to GPU\n",
    "        compressed = model.encoder(data)\n",
    "        compressed_data.append(compressed.cpu().numpy())\n",
    "        labels.append(label.numpy())\n",
    "        \n",
    "    for data, label in test_loader_cryptic:\n",
    "        data = data.to(device)  # Move data to GPU\n",
    "        compressed = model.encoder(data)\n",
    "        test_compressed_data.append(compressed.cpu().numpy())\n",
    "        test_labels.append(label.numpy())\n",
    "\n",
    "compressed_data = np.vstack(compressed_data)\n",
    "compressed_data = compressed_data.squeeze(axis=1)\n",
    "labels = np.hstack(labels)\n",
    "\n",
    "test_compressed_data = np.vstack(test_compressed_data)\n",
    "test_compressed_data = test_compressed_data.squeeze(axis=1)\n",
    "test_labels = np.hstack(test_labels)\n",
    "\n",
    "    \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Split the compressed data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = compressed_data, test_compressed_data, labels, test_labels\n",
    "\n",
    "# Initialize and train the MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(500, 100, 50), max_iter=500, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained MLP\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Initialize and train the XGBoost classifier\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained XGBoost model\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb * 100:.2f}%\")\n",
    "print(\"\\nXGBoost Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "# %%javascript\n",
    "\n",
    "# IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.48%\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.93      0.84      1891\n",
      "           1       0.24      0.07      0.11       564\n",
      "\n",
      "    accuracy                           0.73      2455\n",
      "   macro avg       0.51      0.50      0.48      2455\n",
      "weighted avg       0.65      0.73      0.68      2455\n",
      "\n",
      "XGBoost Accuracy: 96.82%\n",
      "\n",
      "XGBoost Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1891\n",
      "           1       0.94      0.92      0.93       564\n",
      "\n",
      "    accuracy                           0.97      2455\n",
      "   macro avg       0.96      0.95      0.95      2455\n",
      "weighted avg       0.97      0.97      0.97      2455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "    model.eval()\n",
    "\n",
    "    compressed_data = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data = data.to(device)  # Move data to GPU\n",
    "            compressed = model.encoder(data)\n",
    "            compressed_data.append(compressed.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "\n",
    "    compressed_data = np.vstack(compressed_data)\n",
    "    compressed_data = compressed_data.squeeze(axis=1)\n",
    "\n",
    "    labels = np.hstack(labels)\n",
    "    \n",
    "        \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    # Split the compressed data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(compressed_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and train the MLP classifier\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    # Predict using the trained MLP\n",
    "    y_pred = mlp.predict(X_test)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Initialize and train the XGBoost classifier\n",
    "    xgb_model = XGBClassifier()\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict using the trained XGBoost model\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "    print(f\"XGBoost Accuracy: {accuracy_xgb * 100:.2f}%\")\n",
    "    print(\"\\nXGBoost Classification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2455,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.66%\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.99      0.87      1879\n",
      "           1       0.55      0.03      0.06       576\n",
      "\n",
      "    accuracy                           0.77      2455\n",
      "   macro avg       0.66      0.51      0.46      2455\n",
      "weighted avg       0.72      0.77      0.68      2455\n",
      "\n",
      "XGBoost Accuracy: 97.27%\n",
      "\n",
      "XGBoost Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      1879\n",
      "           1       0.95      0.93      0.94       576\n",
      "\n",
      "    accuracy                           0.97      2455\n",
      "   macro avg       0.97      0.96      0.96      2455\n",
      "weighted avg       0.97      0.97      0.97      2455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compressed_data = compressed_data.squeeze(axis=1)\n",
    "\n",
    "labels = np.hstack(labels)\n",
    "\n",
    "    \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Split the compressed data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(compressed_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained MLP\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Initialize and train the XGBoost classifier\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained XGBoost model\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performan\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb * 100:.2f}%\")\n",
    "print(\"\\nXGBoost Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "# %%javascript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'autonencoder_model_state.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# running models using the encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(cryptic, gene_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "\n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        \n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "\n",
    "    aa = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table \n",
    "    for x in tqdm(variants['sample_id'].unique()):\n",
    "        aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "        # print('SNP')\n",
    "        \n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "\n",
    "    mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    mic_aa = mic_aa.sort_values([\"ENA_RUN\"])  ## 'sort' changed to 'sort_values'\n",
    "\n",
    "    return aa_array, mic_aa\n",
    "\n",
    "def data_split(aa_array, encoded_mic):\n",
    "    # Encode the target variable\n",
    "    \n",
    "    # Perform stratified train-test split\n",
    "    train_data, test_data, train_target, test_target = train_test_split(\n",
    "        aa_array,\n",
    "        encoded_mic,\n",
    "        test_size=0.1,  # 10% for testing\n",
    "        stratify=encoded_mic,  # Ensures the proportion of each class is preserved\n",
    "        random_state=42  # For reproducibility\n",
    "    )\n",
    "    return train_data, test_data, train_target, test_target\n",
    "\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "def data_prep_(cryptic, gene_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    \n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    variants = variants[variants['type'] != 'non_coding_transcript_exon_variant']\n",
    "\n",
    "    overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    # print(variants.shape)\n",
    "    # print(variants['sample_id'].unique().shape)\n",
    "\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        \n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "\n",
    "    aa = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table \n",
    "    for x in tqdm(overlap):\n",
    "    # for x in tqdm(variants['sample_id'].unique()):\n",
    "        if x in variants['sample_id'].tolist():\n",
    "            aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "        else:\n",
    "            aa.append([0]*len(all_snp))\n",
    "            \n",
    "        # print('SNP')\n",
    "        \n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "\n",
    "    mic_aa = cryptic[cryptic['ENA_RUN'].isin(overlap)]#.iloc[:,14:27]\n",
    "    # mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # print(mic_aa.shape)\n",
    "    # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(overlap)\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    mic_aa = mic_aa.sort_values([\"ENA_RUN\"])  ## 'sort' changed to 'sort_values'\n",
    "    # print(mic_aa.shape)\n",
    "\n",
    "    return aa_array, mic_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29397/2926147983.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_emb.loc[i, f'{x}'] = '16'\n",
      "/tmp/ipykernel_29397/2926147983.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_emb['EMB_MIC'] = df_emb['EMB_MIC'].astype('float')\n",
      "100%|██████████| 11362/11362 [01:01<00:00, 184.30it/s]\n",
      "/tmp/ipykernel_29397/3522971869.py:124: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
      "/tmp/ipykernel_29397/3522971869.py:125: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(overlap)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('../CRyPTIC_reuse_table_20231208.csv')\n",
    "gene_list = ['embB', 'embA', 'embC']\n",
    "dr_list = ['ethambutol']\n",
    "df_emb = df[df['EMB_MIC'].isin(['>8','8.0', '4.0', '2.0', '1.0', '0.5'])]\n",
    "# df_emb = df_emb[~df_emb['ENA_RUN'].isin(to_be_dropped)]\n",
    "for i, row in df_emb.iterrows():\n",
    "    x = 'EMB_MIC'\n",
    "    if row[x] == '>8' :\n",
    "        df_emb.loc[i, f'{x}'] = '16'\n",
    "    elif row[x] == '<=0.25':\n",
    "        df_emb.loc[i, f'{x}'] = '0.125'\n",
    "        \n",
    "df_emb['EMB_MIC'] = df_emb['EMB_MIC'].astype('float') \n",
    "# df_emb = df_emb[~df_emb['ENA_RUN'].isin(to_be_dropped)]\n",
    "\n",
    "# variants = pd.read_csv('variants_full.csv')\n",
    "# variants = variants[variants['type'] != 'synonymous_variant']\n",
    "# df_emb = df_emb[~df_emb['EMB_PHENOTYPE_QUALITY'].isin(['LOW','MEDIUM'])]  # remove low and med quality\n",
    "# df_emb = df_emb[~df_emb['EMB_PHENOTYPE_QUALITY'].isin(['MEDIUM'])]  # remove low and med quality\n",
    "# df_emb = df_emb[df_emb['ENA_RUN'].isin(samples)]\n",
    "cryptic = df_emb\n",
    "aa_array, drs = data_prep_(cryptic, gene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11362, 1710)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "aa_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(cryptic, gene_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "\n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        \n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "\n",
    "    aa = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table \n",
    "    for x in tqdm(variants['sample_id'].unique()):\n",
    "        aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "        # print('SNP')\n",
    "        \n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "\n",
    "    mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    mic_aa = mic_aa.sort_values([\"ENA_RUN\"])  ## 'sort' changed to 'sort_values'\n",
    "\n",
    "    return aa_array, mic_aa\n",
    "\n",
    "def data_split(aa_array, encoded_mic):\n",
    "    # Encode the target variable\n",
    "    \n",
    "    # Perform stratified train-test split\n",
    "    train_data, test_data, train_target, test_target = train_test_split(\n",
    "        aa_array,\n",
    "        encoded_mic,\n",
    "        test_size=0.1,  # 10% for testing\n",
    "        stratify=encoded_mic,  # Ensures the proportion of each class is preserved\n",
    "        random_state=42  # For reproducibility\n",
    "    )\n",
    "    return train_data, test_data, train_target, test_target\n",
    "\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "def data_prep_(cryptic, gene_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    \n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    variants = variants[variants['type'] != 'non_coding_transcript_exon_variant']\n",
    "\n",
    "    overlap = variants[~variants['sample_id'].isin(cryptic['ENA_RUN'])]\n",
    "\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    # print(variants.shape)\n",
    "    # print(variants['sample_id'].unique().shape)\n",
    "\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        \n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "\n",
    "    aa = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table \n",
    "    for x in tqdm(overlap):\n",
    "    # for x in tqdm(variants['sample_id'].unique()):\n",
    "        if x in variants['sample_id'].tolist():\n",
    "            aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "        else:\n",
    "            aa.append([0]*len(all_snp))\n",
    "            \n",
    "        # print('SNP')\n",
    "        \n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "\n",
    "    # mic_aa = cryptic[cryptic['ENA_RUN'].isin(overlap)]#.iloc[:,14:27]\n",
    "    # # mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # # print(mic_aa.shape)\n",
    "    # # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(overlap)\n",
    "    # # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    # mic_aa = mic_aa.sort_values([\"ENA_RUN\"])  ## 'sort' changed to 'sort_values'\n",
    "    # # print(mic_aa.shape)\n",
    "\n",
    "    return aa_array#, mic_aa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
