{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/nn_model_script_emb_test.py - starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f48cdcdef30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "print('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/nn_model_script_emb_test.py - starting')\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from array import array\n",
    "from cmath import nan\n",
    "import statistics\n",
    "from tkinter.ttk import Separator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torchviz import make_dot\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import variable\n",
    "from itertools import chain\n",
    "from sklearn import metrics as met\n",
    "import pickle\n",
    "from icecream import ic\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from importlib import reload\n",
    "# import util\n",
    "# import model_torch_simple\n",
    "# from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#%%\n",
    "seed = 42\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# train_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/aa_data_train_gene.csv', delimiter = ',')\n",
    "# train_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/mic_aa_train_hml.csv')\n",
    "# train_target = train_target[['EMB_MIC']]\n",
    "# # don't touch test data, split out validation data from training data during training\n",
    "# # test_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_EMB/aa_data_test_pca4k.csv', delimiter = ',')\n",
    "# test_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/aa_data_test_gene.csv', delimiter = ',')\n",
    "# test_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/mic_aa_test_hml.csv')\n",
    "# test_target = test_target[['EMB_MIC']]\n",
    "\n",
    "# all_data = np.concatenate((train_data, test_data), axis=0)\n",
    "# all_target = pd.concat((train_target, test_target), axis=0)\n",
    "\n",
    "# train_data, test_data, train_target, test_target = train_test_split(all_data, all_target, test_size=0.2, random_state=42, stratify=all_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(aa_array, encoded_mic):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Perform stratified train-test split\n",
    "    train_data, test_data, train_target, test_target = train_test_split(\n",
    "        aa_array,\n",
    "        encoded_mic,\n",
    "        test_size=0.1,  # 10% for testing\n",
    "        stratify=encoded_mic.iloc[:,0],  # Ensures the proportion of each class is preserved\n",
    "        random_state=42  # For reproducibility\n",
    "    )\n",
    "    return train_data, test_data, train_target, test_target\n",
    "\n",
    "# def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "#     _ = np.arange(target_min-1, target_max+2, 1)\n",
    "#     index = [i for i, x in enumerate(_) if x == target][0]\n",
    "#     return (_[index-1] <= pred <= _[index+1])\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min.values-1, target_max.values+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "# def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "#     # Convert Series to scalar if needed\n",
    "#     if isinstance(pred, pd.Series):\n",
    "#         pred = pred.iloc[0]  # or pred.item()\n",
    "#     if isinstance(target, pd.Series):\n",
    "#         target = target.iloc[0]  # or target.item()\n",
    "\n",
    "#     _ = np.arange(target_min - 1, target_max + 2, 1)\n",
    "#     # Now target is guaranteed to be a scalar\n",
    "#     index = [i for i, x in enumerate(_) if x == target][0]\n",
    "#     return (_[index - 1] <= pred <= _[index + 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_(cryptic, gene_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    \n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    variants = variants[variants['type'] != 'non_coding_transcript_exon_variant']\n",
    "\n",
    "    overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    # print(variants.shape)\n",
    "    # print(variants['sample_id'].unique().shape)\n",
    "\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        \n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "\n",
    "    aa = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table \n",
    "    print('Getting all snp data', len(all_snp))\n",
    "    for x in tqdm(overlap):\n",
    "    # for x in tqdm(variants['sample_id'].unique()):\n",
    "        if x in variants['sample_id'].tolist():\n",
    "            aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "        else:\n",
    "            aa.append([0]*len(all_snp))\n",
    "        # print('SNP')\n",
    "        \n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "\n",
    "    mic_aa = cryptic[cryptic['ENA_RUN'].isin(overlap)]#.iloc[:,14:27]\n",
    "    # mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # print(mic_aa.shape)\n",
    "    # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(overlap)\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    mic_aa = mic_aa.sort_values([\"ENA_RUN\"])  ## 'sort' changed to 'sort_values'\n",
    "    # print(mic_aa.shape)\n",
    "\n",
    "    return aa_array, mic_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4. -3. -2. -1.  0.  1.  2.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "drug = 'LZD'\n",
    "cutoff = 1  # this the cutoff for \n",
    "cutoff_mic = 2\n",
    "df = pd.read_csv('../CRyPTIC_reuse_table_20231208.csv')\n",
    "\n",
    "aa_array = np.load(f'./generated_data18122024/all_sample_snps_cryptic_{drug}.npy')\n",
    "drs = np.load(f'./generated_data18122024/all_sample_drs_cryptic_{drug}.npy')\n",
    "tbp = np.load(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC3/individual_models/generated_data18122024/all_sample_drs_cryptic_{drug}-tbp.npy')\n",
    "\n",
    "encoded_mic = pd.DataFrame(drs, columns=[f'{drug}_MIC'])\n",
    "encoded_mic[f'{drug}_MIC'] = encoded_mic[f'{drug}_MIC'].replace(0.03125, 0.0625)\n",
    "\n",
    "mic_series = np.log2(encoded_mic)\n",
    "\n",
    "print(np.unique(mic_series))\n",
    "mic_series +=  4\n",
    "cutoff += 4\n",
    "# ordinal_encoder = OrdinalEncoder()\n",
    "# encoded_mic['INH_MIC'] = ordinal_encoder.fit_transform(encoded_mic[['INH_MIC']])\n",
    "# mic_series = encoded_mic\n",
    "# sample_ids = mic_aa['ENA_RUN']\n",
    "mic_series_bi = encoded_mic[f'{drug}_MIC'].apply(lambda x: 1 if x >= cutoff_mic else 0)\n",
    "mic_series_all = pd.merge(mic_series, mic_series_bi, left_index=True, right_index=True)\n",
    "# train_data, test_data,  test_target_y, test_target = data_split(aa_array, mic_series_all)\n",
    "mic_series_all['tbp'] = tbp\n",
    "# Drop rows where 'LZD_MIC_x' is 9.0\n",
    "# mic_series_all = mic_series_all[mic_series_all['LZD_MIC_x'] != 9.0]\n",
    "\n",
    "# # Drop corresponding rows in aa_array\n",
    "# aa_array = aa_array[mic_series_all.index]\n",
    "\n",
    "# Split the data\n",
    "train_data, test_data, train_target, test_target = data_split(aa_array, mic_series_all)\n",
    "\n",
    "target_min, target_max = mic_series.min(), mic_series.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LZD_MIC_x\n",
       "3.0    5801\n",
       "2.0    3182\n",
       "4.0    1589\n",
       "1.0     827\n",
       "0.0     302\n",
       "6.0      79\n",
       "5.0      74\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mic_series_all['LZD_MIC_x'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[11685    16]\n",
      " [  106    47]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the absolute difference between the two columns\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(mic_series_all[f'{drug}_MIC_y'], mic_series_all['tbp'])\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LZD_MIC_x\n",
      "3.0    5221\n",
      "2.0    2864\n",
      "4.0    1430\n",
      "1.0     744\n",
      "0.0     272\n",
      "6.0      71\n",
      "5.0      66\n",
      "Name: count, dtype: int64\n",
      "Counter({3.0: 5221, 2.0: 4864, 1.0: 4744, 4.0: 4430, 0.0: 4272, 6.0: 3571, 5.0: 3566})\n",
      "(30668,)\n",
      "(30668, 174)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Assuming train_data is your feature array and train_target['EMB_MIC_x'] is your target array\n",
    "X = train_data\n",
    "y = train_target[f'{drug}_MIC_x']\n",
    "print(train_target[f'{drug}_MIC_x'].value_counts())\n",
    "\n",
    "target_counts = {\n",
    "3.0:5221,\n",
    "2.0:4864,\n",
    "4.0:4430,\n",
    "1.0:4744,\n",
    "0.0:4272,\n",
    "6.0:3571,\n",
    "5.0:3566,\n",
    "}\n",
    "# Initialize the RandomOverSampler\n",
    "ros= RandomOverSampler(sampling_strategy=target_counts, random_state=42)\n",
    "\n",
    "# ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Fit and resample the data\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Verify the new class distribution\n",
    "from collections import Counter\n",
    "print(Counter(y_resampled))\n",
    "print(y_resampled.shape)\n",
    "print(X_resampled.shape)\n",
    "\n",
    "# train_mic_series_bi = y_resampled.apply(lambda x: 1 if x >= 4 else 0)\n",
    "# train_target = pd.DataFrame({'EMB_MIC_x': y_resampled, 'EMB_MIC_y': train_mic_series_bi})\n",
    "# train_data = X_resampled\n",
    "train_mic_series_bi = y_resampled.apply(lambda x: 1 if x >= cutoff else 0)\n",
    "train_target = pd.DataFrame({f'{drug}_MIC_x': y_resampled, f'{drug}_MIC_y': train_mic_series_bi})\n",
    "train_data = X_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doubling Dilution Accuracy: 0.8903878583473862\n",
      "XGBoost Accuracy: 0.4856661045531197\n",
      "XGBoost Confusion Matrix:\n",
      " [[  0   0   0  30   0   0   0]\n",
      " [  0   0   2  81   0   0   0]\n",
      " [  1   0   1 316   0   0   0]\n",
      " [  4   0   2 574   0   0   0]\n",
      " [  0   0   1 158   0   0   0]\n",
      " [  0   0   0   6   0   0   2]\n",
      " [  0   0   0   7   0   0   1]]\n",
      "XGBoost ROC AUC Score: 0.4948006022100026\n",
      "binary:\n",
      "AUC: 0.59375\n",
      "Sensitivity: 0.1875\n",
      "Specificity: 1.0\n",
      "----------------------\n",
      "Doubling Dilution Accuracy: 0.8912310286677909\n",
      "Logistic Regression Accuracy: 0.48650927487352447\n",
      "Logistic Regression Confusion Matrix:\n",
      " [[  0   1   0  29   0   0   0]\n",
      " [  0   0   2  81   0   0   0]\n",
      " [  1   0   2 315   0   0   0]\n",
      " [  4   0   2 574   0   0   0]\n",
      " [  0   0   1 158   0   0   0]\n",
      " [  0   0   0   6   0   0   2]\n",
      " [  0   0   0   7   0   0   1]]\n",
      "Logistic Regression ROC AUC Score: 0.49313920726834637\n",
      "binary:\n",
      "AUC: 0.59375\n",
      "Sensitivity: 0.1875\n",
      "Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Prepare the training and testing data\n",
    "X_train, X_test, y_train, y_test = train_data, test_data, train_target[f'{drug}_MIC_x'], test_target[f'{drug}_MIC_x']\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "roc_auc_xgb = roc_auc_score(y_test, xgb_model.predict_proba(X_test), multi_class='ovr')\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(y_pred_xgb, y_test)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "\n",
    "print(\"XGBoost Accuracy:\", accuracy_xgb)\n",
    "print(\"XGBoost Confusion Matrix:\\n\", conf_matrix_xgb)\n",
    "print(\"XGBoost ROC AUC Score:\", roc_auc_xgb)\n",
    "\n",
    "cutoff = cutoff\n",
    "test_target_bi = (y_test >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(y_pred_xgb) >= cutoff).astype(int)\n",
    "\n",
    "print('binary:')\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)\n",
    "\n",
    "print('----------------------')\n",
    "# Multinomial Logistic Regression\n",
    "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "conf_matrix_log_reg = confusion_matrix(y_test, y_pred_log_reg)\n",
    "roc_auc_log_reg = roc_auc_score(y_test, log_reg.predict_proba(X_test), multi_class='ovr')\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(y_pred_log_reg, y_test)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_log_reg)\n",
    "print(\"Logistic Regression Confusion Matrix:\\n\", conf_matrix_log_reg)\n",
    "print(\"Logistic Regression ROC AUC Score:\", roc_auc_log_reg)\n",
    "\n",
    "cutoff = cutoff\n",
    "test_target_bi = (y_test >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(y_pred_log_reg) >= cutoff).astype(int)\n",
    "\n",
    "print('binary:')\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doubling Dilution Accuracy: 0.8946037099494097\n",
      "XGBoost Accuracy: 0.4873524451939292\n",
      "XGBoost Confusion Matrix:\n",
      " [[  0   0   0  30   0   0   0]\n",
      " [  0   0   2  81   0   0   0]\n",
      " [  1   0   1 316   0   0   0]\n",
      " [  4   0   2 574   0   0   0]\n",
      " [  0   0   1 158   0   0   0]\n",
      " [  0   0   0   4   0   2   2]\n",
      " [  0   0   0   4   0   3   1]]\n",
      "XGBoost ROC AUC Score: 0.5768149879892273\n",
      "binary:\n",
      "AUC: 0.75\n",
      "Sensitivity: 0.5\n",
      "Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "# test for binary assistance in baseline models\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min.values-1, target_max.values+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "train_data_with_bin_pred = np.column_stack((train_data, train_target[f'{drug}_MIC_y'].values))\n",
    "test_data_with_bin_pred = np.column_stack((test_data, test_target['tbp'].values))\n",
    "\n",
    "# Prepare the training and testing data\n",
    "X_train, X_test, y_train, y_test = train_data_with_bin_pred , test_data_with_bin_pred , train_target[f'{drug}_MIC_x'], test_target[f'{drug}_MIC_x']\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "roc_auc_xgb = roc_auc_score(y_test, xgb_model.predict_proba(X_test), multi_class='ovr')\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(y_pred_xgb, y_test)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "\n",
    "print(\"XGBoost Accuracy:\", accuracy_xgb)\n",
    "print(\"XGBoost Confusion Matrix:\\n\", conf_matrix_xgb)\n",
    "print(\"XGBoost ROC AUC Score:\", roc_auc_xgb)\n",
    "\n",
    "cutoff = cutoff\n",
    "test_target_bi = (y_test >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(y_pred_xgb) >= cutoff).astype(int)\n",
    "\n",
    "print('binary:')\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cornloss weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_counts = torch.from_numpy(train_target.values).flatten()\n",
    "# train_target_counts = torch.tensor([0,1,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(aa_array)\n",
    "\n",
    "aa_array = np.load(f'./generated_data18122024/all_sample_snps_50k_{drug}.npy')\n",
    "# Read the text file line by line into a list and convert to floats\n",
    "mic_series = np.load(f'./generated_data18122024/all_sample_drs_50k_{drug}.npy')\n",
    "\n",
    "# print(mic_series)\n",
    "mic_series_50k = mic_series\n",
    "\n",
    "aa_array_50k_pos = []\n",
    "mic_series_50k_pos = []\n",
    "for x, a in zip(mic_series_50k, aa_array):\n",
    "    if x == 1:\n",
    "        aa_array_50k_pos.append(a)\n",
    "        mic_series_50k_pos.append(x)\n",
    "aa_array_50k_pos = np.array(aa_array_50k_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Prepare your training and testing data\n",
    "# ------------------------------------------------\n",
    "\n",
    "# Assume you already have:\n",
    "#   train_data, test_data\n",
    "#   train_target, test_target\n",
    "#   aa_array_50k_pos, mic_series_50k_pos \n",
    "#   => as per your code snippet.\n",
    "\n",
    "train_target_y = train_target[f'{drug}_MIC_y'].values\n",
    "test_target_y  = test_target[f'{drug}_MIC_y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_data, test_data, train_target_y, test_target_y\n",
    "\n",
    "# Optionally concatenate the additional data you mentioned:\n",
    "X_train = np.concatenate((X_train, aa_array_50k_pos), axis=0)\n",
    "y_train = np.concatenate((y_train, mic_series_50k_pos), axis=0)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Define a function for checking doubling dilution \n",
    "# (if needed)\n",
    "# ------------------------------------------------\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Set up the XGBoost model & parameter grid\n",
    "# ------------------------------------------------\n",
    "# Create a base model\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Define a parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3], \n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    # 'reg_alpha': [0, 1, 10],       # optionally add\n",
    "    # 'reg_lambda': [0, 1, 10],     # optionally add\n",
    "}\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Run GridSearchCV\n",
    "# ------------------------------------------------\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',      # You can choose other metrics, e.g., 'roc_auc'\n",
    "    cv=3,                    # 3-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1               # Use all available CPU cores\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Evaluate on the test set\n",
    "# ------------------------------------------------\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Binarize predictions at cutoff=4 (per your example)\n",
    "cutoff = cutoff\n",
    "test_target_bi = y_test.astype(int)\n",
    "test_predictions_bi = y_pred.astype(int)\n",
    "\n",
    "# Compute AUC\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Compute confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_target_bi, test_predictions_bi))\n",
    "\n",
    "# Compute sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Compute specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9890387858347386\n",
      "AUC: 0.59375\n",
      "[[1170    0]\n",
      " [  13    3]]\n",
      "Sensitivity: 0.1875\n",
      "Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_data = np.column_stack((train_data, train_target['EMB_MIC_y'].values))\n",
    "# test_data = np.column_stack((test_data, test_target['EMB_MIC_y'].values))\n",
    "train_target_y = train_target[f'{drug}_MIC_y'].values\n",
    "test_target_y = test_target[f'{drug}_MIC_y'].values\n",
    "\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_data, test_data, train_target_y, test_target_y\n",
    "X_test = test_data\n",
    "y_test = test_target[f'{drug}_MIC_y'].values\n",
    "X_train = np.concatenate((X_train, aa_array_50k_pos), axis=0)\n",
    "y_train = np.concatenate((y_train, mic_series_50k_pos), axis=0)\n",
    "# # Create the XGBoost model\n",
    "model_bi = xgb.XGBClassifier(    \n",
    "    max_depth=3,\n",
    "    learning_rate=0.9,\n",
    "    n_estimators=4,\n",
    "    # gamma=0.1,\n",
    "    # min_child_weight=24,\n",
    "    # subsample=0.2,\n",
    "    # colsample_bytree=1,\n",
    "    # reg_alpha=15, reg_lambda=15,\n",
    "    random_state=42 \n",
    "    )\n",
    "# # Create the XGBoost model\n",
    "# model_bi = xgb.XGBClassifier(colsample_bytree= 0.5, gamma= 0.1, learning_rate= 0.01, max_depth= 4, min_child_weight= 1, n_estimators= 200, subsample= 0.8,random_state=42)\n",
    "model_bi = xgb.XGBClassifier()\n",
    "\n",
    "model_bi.fit(X_train, y_train)\n",
    "\n",
    "# model_bi = pickle.load(open(\"xgb_bi_mix1.pkl\", \"rb\"))\n",
    "# model_bi = pickle.load(open(\"xgb_bi_mix.pkl\", \"rb\"))\n",
    "# model_bi = pickle.load(open(\"xgb_bi_50kbalanced.pkl\", \"rb\"))\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model_bi.predict(X_test)\n",
    "\n",
    "# Evaluate the model_bi\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "#testing\n",
    "cutoff = cutoff\n",
    "test_target_bi = y_test.astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "test_predictions_bi = y_pred.astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "print(confusion_matrix(test_target_bi, test_predictions_bi))\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)  \n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9866220735785953\n",
      "AUC: 0.592478813559322\n",
      "[[1177    3]\n",
      " [  13    3]]\n",
      "Sensitivity: 0.1875\n",
      "Specificity: 0.997457627118644\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Define training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_data, test_data, train_target_y, test_target_y\n",
    "X_test = test_data\n",
    "y_test = test_target[f'{drug}_MIC_y'].values\n",
    "X_train = np.concatenate((X_train, aa_array_50k_pos), axis=0)\n",
    "y_train = np.concatenate((y_train, mic_series_50k_pos), axis=0)\n",
    "\n",
    "# Apply RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create the XGBoost model\n",
    "model_bi = xgb.XGBClassifier(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.9,\n",
    "    n_estimators=4,\n",
    "    random_state=42\n",
    ")\n",
    "model_bi = xgb.XGBClassifier()\n",
    "\n",
    "# Fit the model using the resampled dataset\n",
    "model_bi.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model_bi.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Compute AUC\n",
    "cutoff = cutoff\n",
    "test_target_bi = y_test.astype(int)\n",
    "test_predictions_bi = y_pred.astype(int)\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix and performance metrics\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "print(confusion_matrix(test_target_bi, test_predictions_bi))\n",
    "\n",
    "sensitivity = tp / (tp + fn)  # Recall\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_bi.save_model('/mnt/storageG1/lwang/Projects/tb_dr_MIC3/individual_models/saved_model1115/xgb_model_bi.json')  # or \"xgb_model.bin\"\n",
    "# loaded_model.load_model('/mnt/storageG1/lwang/Projects/tb_dr_MIC3/individual_models/saved_model1115/xgb_model.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running with xgb predictions\n",
    "# test_target[f'{drug}_MIC_y'] = test_predictions_bi\n",
    "# runninng with tbp predictio\n",
    "test_target[f'{drug}_MIC_y'] = test_target['tbp']\n",
    "\n",
    "# test_target[f'{drug}_MIC_y'] = 0\n",
    "# train_target[f'{drug}_MIC_y'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB without binary support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doubling Dilution Accuracy: 0.8903878583473862\n",
      "XGBoost Accuracy: 0.4856661045531197\n",
      "XGBoost Confusion Matrix:\n",
      " [[  0   0   0  30   0   0   0]\n",
      " [  0   0   2  81   0   0   0]\n",
      " [  1   0   1 316   0   0   0]\n",
      " [  4   0   2 574   0   0   0]\n",
      " [  0   0   1 158   0   0   0]\n",
      " [  0   0   0   6   0   0   2]\n",
      " [  0   0   0   7   0   0   1]]\n",
      "XGBoost ROC AUC Score: 0.4948006022100026\n",
      "Macro F1: 0.12046316612526657\n",
      "\n",
      "Class-specific AUC (One-vs-Rest):\n",
      "Class 0.0: AUC = 0.5030276816608996\n",
      "Class 1.0: AUC = 0.5031403947612754\n",
      "Class 2.0: AUC = 0.5028819957684839\n",
      "Class 3.0: AUC = 0.5060515534312051\n",
      "Class 4.0: AUC = 0.4987997036002768\n",
      "Class 5.0: AUC = 0.38285229202037346\n",
      "Class 6.0: AUC = 0.5668505942275043\n",
      "\n",
      "Class-specific performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        30\n",
      "         1.0       0.00      0.00      0.00        83\n",
      "         2.0       0.17      0.00      0.01       318\n",
      "         3.0       0.49      0.99      0.66       580\n",
      "         4.0       0.00      0.00      0.00       159\n",
      "         5.0       0.00      0.00      0.00         8\n",
      "         6.0       0.33      0.12      0.18         8\n",
      "\n",
      "    accuracy                           0.49      1186\n",
      "   macro avg       0.14      0.16      0.12      1186\n",
      "weighted avg       0.29      0.49      0.32      1186\n",
      "\n",
      "\n",
      "binary:\n",
      "AUC: 0.59375\n",
      "Sensitivity: 0.1875\n",
      "Specificity: 1.0\n",
      "\n",
      "95 percent confidence intervals (stratified bootstrap):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: mean=0.485650084317032, CI=(0.4814502529510961, 0.48988195615514335)\n",
      "AUC-OVR: mean=0.49497451130451464, CI=(0.46800960819657206, 0.5220737743551458)\n",
      "Doubling Dilution: mean=0.8904620573355819, CI=(0.8844856661045531, 0.8954468802698144)\n",
      "Macro F1: mean=0.11889984088817314, CI=(0.09322793578355472, 0.16558219033429142)\n",
      "Sensitivity: mean=0.19025, CI=(0.0, 0.375)\n",
      "Specificity: mean=1.0, CI=(1.0, 1.0)\n",
      "MIC mapping dictionary saved: {'0': '0.0625', '1': '0.125', '2': '0.25', '3': '0.5', '4': '1.0', '5': '2.0', '6': '4.0'}\n",
      "Prediction results saved to targets_pred/targets_pred_XGB-LZD.txt\n",
      "Saved trained model to saved_models/xgb_MIC_model_LZD.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, roc_auc_score, classification_report,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Add binary support features\n",
    "# ------------------------------\n",
    "train_data_with_bin_pred = train_data\n",
    "test_data_with_bin_pred = test_data\n",
    "\n",
    "X_train, X_test = train_data_with_bin_pred, test_data_with_bin_pred\n",
    "y_train, y_test = train_target[f'{drug}_MIC_x'], test_target[f'{drug}_MIC_x']\n",
    "\n",
    "# ------------------------------\n",
    "# Train model\n",
    "# ------------------------------\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# Predictions\n",
    "# ------------------------------\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_pred_probs = xgb_model.predict_proba(X_test)\n",
    "\n",
    "# ------------------------------\n",
    "# Metrics\n",
    "# ------------------------------\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "roc_auc_xgb = roc_auc_score(y_test, y_pred_probs, multi_class='ovr')\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([\n",
    "    is_within_doubling_dilution(pred, true, target_min, target_max)\n",
    "    for pred, true in zip(y_pred_xgb, y_test)\n",
    "])\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred_xgb, average=\"macro\")\n",
    "\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "print(\"XGBoost Accuracy:\", accuracy_xgb)\n",
    "print(\"XGBoost Confusion Matrix:\\n\", conf_matrix_xgb)\n",
    "print(\"XGBoost ROC AUC Score:\", roc_auc_xgb)\n",
    "print(\"Macro F1:\", macro_f1)\n",
    "\n",
    "# ------------------------------\n",
    "# Convert to numpy for safe indexing\n",
    "# ------------------------------\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_pred_np = np.array(y_pred_xgb)\n",
    "y_pred_probs_np = np.array(y_pred_probs)\n",
    "\n",
    "# ------------------------------\n",
    "# Class-specific AUC-OVR\n",
    "# ------------------------------\n",
    "print(\"\\nClass-specific AUC (One-vs-Rest):\")\n",
    "\n",
    "unique_classes = np.unique(y_test_np)\n",
    "class_to_index = {label: i for i, label in enumerate(xgb_model.classes_)}\n",
    "\n",
    "class_specific_auc = {}\n",
    "\n",
    "for cls in unique_classes:\n",
    "    y_true_binary = (y_test_np == cls).astype(int)\n",
    "    col = class_to_index[cls]\n",
    "    y_score_binary = y_pred_probs_np[:, col]\n",
    "\n",
    "    try:\n",
    "        auc_cls = roc_auc_score(y_true_binary, y_score_binary)\n",
    "    except:\n",
    "        auc_cls = np.nan\n",
    "\n",
    "    class_specific_auc[int(cls)] = auc_cls\n",
    "    print(f\"Class {cls}: AUC = {auc_cls}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Class-specific report\n",
    "# ------------------------------\n",
    "print(\"\\nClass-specific performance:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# ------------------------------\n",
    "# Binary metrics\n",
    "# ------------------------------\n",
    "test_target_bi = (y_test_np >= cutoff).astype(int)\n",
    "test_predictions_bi = (y_pred_np >= cutoff).astype(int)\n",
    "\n",
    "auc_bi = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"\\nbinary:\")\n",
    "print(\"AUC:\", auc_bi)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)\n",
    "\n",
    "# ------------------------------\n",
    "# STRATIFIED BOOTSTRAP (95 percent CI + MEAN)\n",
    "# ------------------------------\n",
    "def stratified_bootstrap(y_true, y_pred, y_proba, metric_fn, n_boot=2000, alpha=0.05):\n",
    "    classes = np.unique(y_true)\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(n_boot):\n",
    "        idx_list = []\n",
    "        for cls in classes:\n",
    "            cls_idx = np.where(y_true == cls)[0]\n",
    "            if len(cls_idx) > 0:\n",
    "                sampled = np.random.choice(cls_idx, size=len(cls_idx), replace=True)\n",
    "                idx_list.extend(sampled)\n",
    "\n",
    "        idx = np.array(idx_list)\n",
    "\n",
    "        m = metric_fn(y_true[idx], y_pred[idx], y_proba[idx])\n",
    "        if not np.isnan(m):\n",
    "            samples.append(m)\n",
    "\n",
    "    if len(samples) == 0:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "\n",
    "    samples = np.array(samples)\n",
    "\n",
    "    mean_val = np.mean(samples)\n",
    "    low = np.percentile(samples, 100 * (alpha / 2))\n",
    "    high = np.percentile(samples, 100 * (1 - alpha / 2))\n",
    "\n",
    "    return mean_val, low, high\n",
    "\n",
    "# Metric wrappers\n",
    "def metric_accuracy(y_t, y_p, y_s):\n",
    "    return accuracy_score(y_t, y_p)\n",
    "\n",
    "def metric_auc_mc(y_t, y_p, y_s):\n",
    "    try:\n",
    "        return roc_auc_score(y_t, y_s, multi_class='ovr')\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def metric_dd(y_t, y_p, y_s):\n",
    "    return np.mean([\n",
    "        is_within_doubling_dilution(p, t, target_min, target_max)\n",
    "        for p, t in zip(y_p, y_t)\n",
    "    ])\n",
    "\n",
    "def metric_f1(y_t, y_p, y_s):\n",
    "    try:\n",
    "        return f1_score(y_t, y_p, average=\"macro\")\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def metric_sens(y_t, y_p, y_s):\n",
    "    tn, fp, fn, tp = confusion_matrix((y_t >= cutoff).astype(int),\n",
    "                                      (y_p >= cutoff).astype(int)).ravel()\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "\n",
    "def metric_spec(y_t, y_p, y_s):\n",
    "    tn, fp, fn, tp = confusion_matrix((y_t >= cutoff).astype(int),\n",
    "                                      (y_p >= cutoff).astype(int)).ravel()\n",
    "    return tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "\n",
    "print(\"\\n95 percent confidence intervals (stratified bootstrap):\")\n",
    "\n",
    "acc_mean, acc_low, acc_high = stratified_bootstrap(y_test_np, y_pred_np, y_pred_probs_np, metric_accuracy)\n",
    "auc_mean, auc_low, auc_high = stratified_bootstrap(y_test_np, y_pred_np, y_pred_probs_np, metric_auc_mc)\n",
    "dd_mean, dd_low, dd_high = stratified_bootstrap(y_test_np, y_pred_np, y_pred_probs_np, metric_dd)\n",
    "f1_mean, f1_low, f1_high = stratified_bootstrap(y_test_np, y_pred_np, y_pred_probs_np, metric_f1)\n",
    "sens_mean, sens_low, sens_high = stratified_bootstrap(y_test_np, y_pred_np, y_pred_probs_np, metric_sens)\n",
    "spec_mean, spec_low, spec_high = stratified_bootstrap(y_test_np, y_pred_np, y_pred_probs_np, metric_spec)\n",
    "\n",
    "print(f\"Accuracy: mean={acc_mean}, CI=({acc_low}, {acc_high})\")\n",
    "print(f\"AUC-OVR: mean={auc_mean}, CI=({auc_low}, {auc_high})\")\n",
    "print(f\"Doubling Dilution: mean={dd_mean}, CI=({dd_low}, {dd_high})\")\n",
    "print(f\"Macro F1: mean={f1_mean}, CI=({f1_low}, {f1_high})\")\n",
    "print(f\"Sensitivity: mean={sens_mean}, CI=({sens_low}, {sens_high})\")\n",
    "print(f\"Specificity: mean={spec_mean}, CI=({spec_low}, {spec_high})\")\n",
    "\n",
    "# ------------------------------\n",
    "# Save MIC mapping\n",
    "# ------------------------------\n",
    "mic = np.sort(np.unique(encoded_mic))\n",
    "mic_log2 = np.sort(np.unique(y_test))\n",
    "mic_dict = {str(int(b)): str(a) for a, b in zip(mic, mic_log2)}\n",
    "\n",
    "with open(f'targets_pred/mic_dic-{drug}.json', 'w') as f:\n",
    "    json.dump(mic_dict, f)\n",
    "\n",
    "print(\"MIC mapping dictionary saved:\", mic_dict)\n",
    "\n",
    "# ------------------------------\n",
    "# Save predictions\n",
    "# ------------------------------\n",
    "os.makedirs(\"targets_pred\", exist_ok=True)\n",
    "file_path = f'targets_pred/targets_pred_XGB-{drug}.txt'\n",
    "with open(file_path, 'w') as file:\n",
    "    for pred, target in zip(y_pred_xgb, y_test):\n",
    "        file.write(f\"{target} {pred}\\n\")\n",
    "\n",
    "print(f\"Prediction results saved to {file_path}\")\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "model_path = f\"saved_models/xgb_MIC_model_{drug}.pkl\"\n",
    "\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "\n",
    "print(f\"Saved trained model to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgb with bin pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doubling Dilution Accuracy: 0.8946037099494097\n",
      "XGBoost Accuracy: 0.4873524451939292\n",
      "XGBoost Confusion Matrix:\n",
      " [[  0   0   0  30   0   0   0]\n",
      " [  0   0   2  81   0   0   0]\n",
      " [  1   0   1 316   0   0   0]\n",
      " [  4   0   2 574   0   0   0]\n",
      " [  0   0   1 158   0   0   0]\n",
      " [  0   0   0   4   0   2   2]\n",
      " [  0   0   0   4   0   3   1]]\n",
      "XGBoost ROC AUC Score: 0.5768149879892273\n",
      "Macro F1: 0.16468711879892953\n",
      "\n",
      "Class-specific AUC (One-vs-Rest):\n",
      "Class 0.0: AUC = 0.5215397923875432\n",
      "Class 1.0: AUC = 0.5049645545008684\n",
      "Class 2.0: AUC = 0.5063345940932673\n",
      "Class 3.0: AUC = 0.5091754865141687\n",
      "Class 4.0: AUC = 0.5007838670365539\n",
      "Class 5.0: AUC = 0.747453310696095\n",
      "Class 6.0: AUC = 0.747453310696095\n",
      "\n",
      "Class-specific performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        30\n",
      "         1.0       0.00      0.00      0.00        83\n",
      "         2.0       0.17      0.00      0.01       318\n",
      "         3.0       0.49      0.99      0.66       580\n",
      "         4.0       0.00      0.00      0.00       159\n",
      "         5.0       0.40      0.25      0.31         8\n",
      "         6.0       0.33      0.12      0.18         8\n",
      "\n",
      "    accuracy                           0.49      1186\n",
      "   macro avg       0.20      0.20      0.16      1186\n",
      "weighted avg       0.29      0.49      0.33      1186\n",
      "\n",
      "\n",
      "binary:\n",
      "AUC: 0.75\n",
      "Sensitivity: 0.5\n",
      "Specificity: 1.0\n",
      "\n",
      "95 percent confidence intervals (stratified bootstrap):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: mean=0.48721627318718386, CI=(0.4822934232715008, 0.4924114671163575)\n",
      "AUC-OVR: mean=0.5766294364781835, CI=(0.5405458890956831, 0.613283732454024)\n",
      "Doubling Dilution: mean=0.8947179595278244, CI=(0.8887015177065767, 0.9005059021922428)\n",
      "Macro F1: mean=0.16136521833915793, CI=(0.0950345239106547, 0.22754582440673538)\n",
      "Sensitivity: mean=0.50321875, CI=(0.25, 0.75)\n",
      "Specificity: mean=1.0, CI=(1.0, 1.0)\n",
      "MIC mapping dictionary saved: {'0': '0.0625', '1': '0.125', '2': '0.25', '3': '0.5', '4': '1.0', '5': '2.0', '6': '4.0'}\n",
      "Prediction results saved to targets_pred/targets_pred_XGB-LZD-bin.txt\n",
      "Saved trained model to saved_models/xgb_MIC_model_LZD_bin.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, roc_auc_score, classification_report,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Add binary support features\n",
    "# ------------------------------\n",
    "train_data_with_bin_pred = np.column_stack((train_data, train_target[f'{drug}_MIC_y'].values))\n",
    "test_data_with_bin_pred = np.column_stack((test_data, test_target['tbp'].values))\n",
    "\n",
    "X_train, X_test = train_data_with_bin_pred, test_data_with_bin_pred\n",
    "y_train, y_test = train_target[f'{drug}_MIC_x'], test_target[f'{drug}_MIC_x']\n",
    "\n",
    "# ------------------------------\n",
    "# Train model\n",
    "# ------------------------------\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# Predictions\n",
    "# ------------------------------\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_pred_probs = xgb_model.predict_proba(X_test)\n",
    "\n",
    "# ------------------------------\n",
    "# Metrics\n",
    "# ------------------------------\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "roc_auc_xgb = roc_auc_score(y_test, y_pred_probs, multi_class='ovr')\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([\n",
    "    is_within_doubling_dilution(pred, true, target_min, target_max)\n",
    "    for pred, true in zip(y_pred_xgb, y_test)\n",
    "])\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred_xgb, average=\"macro\")\n",
    "\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "print(\"XGBoost Accuracy:\", accuracy_xgb)\n",
    "print(\"XGBoost Confusion Matrix:\\n\", conf_matrix_xgb)\n",
    "print(\"XGBoost ROC AUC Score:\", roc_auc_xgb)\n",
    "print(\"Macro F1:\", macro_f1)\n",
    "\n",
    "# ------------------------------\n",
    "# Convert to numpy for safe indexing\n",
    "# ------------------------------\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_pred_np = np.array(y_pred_xgb)\n",
    "y_pred_probs_np = np.array(y_pred_probs)\n",
    "\n",
    "# ------------------------------\n",
    "# Class-specific AUC-OVR\n",
    "# ------------------------------\n",
    "print(\"\\nClass-specific AUC (One-vs-Rest):\")\n",
    "\n",
    "unique_classes = np.unique(y_test_np)\n",
    "class_to_index = {label: i for i, label in enumerate(xgb_model.classes_)}\n",
    "\n",
    "class_specific_auc = {}\n",
    "\n",
    "for cls in unique_classes:\n",
    "    y_true_binary = (y_test_np == cls).astype(int)\n",
    "    col = class_to_index[cls]\n",
    "    y_score_binary = y_pred_probs_np[:, col]\n",
    "\n",
    "    try:\n",
    "        auc_cls = roc_auc_score(y_true_binary, y_score_binary)\n",
    "    except:\n",
    "        auc_cls = np.nan\n",
    "\n",
    "    class_specific_auc[int(cls)] = auc_cls\n",
    "    print(f\"Class {cls}: AUC = {auc_cls}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Class-specific report\n",
    "# ------------------------------\n",
    "print(\"\\nClass-specific performance:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# ------------------------------\n",
    "# Binary metrics\n",
    "# ------------------------------\n",
    "test_target_bi = (y_test_np >= cutoff).astype(int)\n",
    "test_predictions_bi = (y_pred_np >= cutoff).astype(int)\n",
    "\n",
    "auc_bi = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"\\nbinary:\")\n",
    "print(\"AUC:\", auc_bi)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)\n",
    "\n",
    "# ------------------------------\n",
    "# STRATIFIED BOOTSTRAP (95 percent CI + MEAN)\n",
    "# ------------------------------\n",
    "def stratified_bootstrap(y_true, y_pred, y_proba, metric_fn, n_boot=2000, alpha=0.05):\n",
    "    classes = np.unique(y_true)\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(n_boot):\n",
    "        idx_list = []\n",
    "        for cls in classes:\n",
    "            cls_idx = np.where(y_true == cls)[0]\n",
    "            if len(cls_idx) > 0:\n",
    "                sampled = np.random.choice(cls_idx, size=len(cls_idx), replace=True)\n",
    "                idx_list.extend(sampled)\n",
    "\n",
    "        idx = np.array(idx_list)\n",
    "\n",
    "        m = metric_fn(y_true[idx], y_pred[idx], y_proba[idx])\n",
    "        if not np.isnan(m):\n",
    "            samples.append(m)\n",
    "\n",
    "    if len(samples) == 0:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "\n",
    "    samples = np.array(samples)\n",
    "\n",
    "    mean_val = np.mean(samples)\n",
    "    low = np.percentile(samples, 100 * (alpha / 2))\n",
    "    high = np.percentile(samples, 100 * (1 - alpha / 2))\n",
    "\n",
    "    return mean_val, low, high\n",
    "\n",
    "# Metric wrappers\n",
    "def metric_accuracy(y_t, y_p, y_s):\n",
    "    return accuracy_score(y_t, y_p)\n",
    "\n",
    "def metric_auc_mc(y_t, y_p, y_s):\n",
    "    try:\n",
    "        return roc_auc_score(y_t, y_s, multi_class='ovr')\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def metric_dd(y_t, y_p, y_s):\n",
    "    return np.mean([\n",
    "        is_within_doubling_dilution(p, t, target_min, target_max)\n",
    "        for p, t in zip(y_p, y_t)\n",
    "    ])\n",
    "\n",
    "def metric_f1(y_t, y_p, y_s):\n",
    "    try:\n",
    "        return f1_score(y_t, y_p, average=\"macro\")\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def metric_sens(y_t, y_p, y_s):\n",
    "    tn, fp, fn, tp = confusion_matrix((y_t >= cutoff).astype(int),\n",
    "                                      (y_p >= cutoff).astype(int)).ravel()\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "\n",
    "def metric_spec(y_t, y_p, y_s):\n",
    "    tn, fp, fn, tp = confusion_matrix((y_t >= cutoff).astype(int),\n",
    "                                      (y_p >= cutoff).astype(int)).ravel()\n",
    "    return tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "\n",
    "print(\"\\n95 percent confidence intervals (stratified bootstrap):\")\n",
    "\n",
    "acc_mean, acc_low, acc_high = stratified_bootstrap(y_test_np, y_pred_np, y_pred_probs_np, metric_accuracy)\n",
    "auc_mean, auc_low, auc_high = stratified_bootstrap(y_test_np, y_pred_np, y_pred_probs_np, metric_auc_mc)\n",
    "dd_mean, dd_low, dd_high = stratified_bootstrap(y_test_np, y_pred_np, y_pred_probs_np, metric_dd)\n",
    "f1_mean, f1_low, f1_high = stratified_bootstrap(y_test_np, y_pred_np, y_pred_probs_np, metric_f1)\n",
    "sens_mean, sens_low, sens_high = stratified_bootstrap(y_test_np, y_pred_np, y_pred_probs_np, metric_sens)\n",
    "spec_mean, spec_low, spec_high = stratified_bootstrap(y_test_np, y_pred_np, y_pred_probs_np, metric_spec)\n",
    "\n",
    "print(f\"Accuracy: mean={acc_mean}, CI=({acc_low}, {acc_high})\")\n",
    "print(f\"AUC-OVR: mean={auc_mean}, CI=({auc_low}, {auc_high})\")\n",
    "print(f\"Doubling Dilution: mean={dd_mean}, CI=({dd_low}, {dd_high})\")\n",
    "print(f\"Macro F1: mean={f1_mean}, CI=({f1_low}, {f1_high})\")\n",
    "print(f\"Sensitivity: mean={sens_mean}, CI=({sens_low}, {sens_high})\")\n",
    "print(f\"Specificity: mean={spec_mean}, CI=({spec_low}, {spec_high})\")\n",
    "\n",
    "# ------------------------------\n",
    "# Save MIC mapping\n",
    "# ------------------------------\n",
    "mic = np.sort(np.unique(encoded_mic))\n",
    "mic_log2 = np.sort(np.unique(y_test))\n",
    "mic_dict = {str(int(b)): str(a) for a, b in zip(mic, mic_log2)}\n",
    "\n",
    "with open(f'targets_pred/mic_dic-{drug}-bin.json', 'w') as f:\n",
    "    json.dump(mic_dict, f)\n",
    "\n",
    "print(\"MIC mapping dictionary saved:\", mic_dict)\n",
    "\n",
    "# ------------------------------\n",
    "# Save predictions\n",
    "# ------------------------------\n",
    "os.makedirs(\"targets_pred\", exist_ok=True)\n",
    "file_path = f'targets_pred/targets_pred_XGB-{drug}-bin.txt'\n",
    "with open(file_path, 'w') as file:\n",
    "    for pred, target in zip(y_pred_xgb, y_test):\n",
    "        file.write(f\"{target} {pred}\\n\")\n",
    "\n",
    "print(f\"Prediction results saved to {file_path}\")\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "model_path = f\"saved_models/xgb_MIC_model_{drug}_bin.pkl\"\n",
    "\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "\n",
    "print(f\"Saved trained model to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from collections import Counter\n",
    "\n",
    "N_samples = train_data.shape[0]\n",
    "DRUGS = train_target.columns\n",
    "# LOCI = train_data.columns\n",
    "assert set(DRUGS) == set(train_target.columns)\n",
    "N_drugs = len(DRUGS)\n",
    "#%%\n",
    "\n",
    "def my_padding(seq_tuple):\n",
    "    list_x_ = list(seq_tuple)\n",
    "    max_len = len(max(list_x_, key=len))\n",
    "    for i, x in enumerate(list_x_):\n",
    "        list_x_[i] = x + \"N\"*(max_len-len(x))\n",
    "    return list_x_\n",
    "\n",
    "#! faster than my_padding try to incorporate\n",
    "def collate_padded_batch(batch):\n",
    "    # get max length of seqs in batch\n",
    "    max_len = max([x[0].shape[1] for x in batch])\n",
    "    return torch.utils.data.default_collate(\n",
    "        [(F.pad(x[0], (0, max_len - x[0].shape[1])), x[1]) for x in batch] #how does F.pad work\n",
    "    )\n",
    "\n",
    "# Julian's code - implement this, might be faster\n",
    "class Dataset(torch.utils.data.Dataset): #? what's the difference between using inheritance and not?\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_df,\n",
    "        res_df,\n",
    "        # target_loci=LOCI,\n",
    "        target_mic,\n",
    "        target_res,\n",
    "        one_hot_dtype=torch.int8,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        # self.seq_df = seq_df[target_loci]\n",
    "        self.seq_df = seq_df\n",
    "        self.res_df = res_df[target_res]\n",
    "        self.mic_df = res_df[target_mic]\n",
    "        # if not self.seq_df.index.equals(self.res_df.index):\n",
    "        #     raise ValueError(\n",
    "        #         \"Indices of sequence and resistance dataframes don't match up\"\n",
    "        #     )\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        numerical index --> get `index`-th sample\n",
    "        string index --> get sample with name `index`\n",
    "        \"\"\"\n",
    "        index = int(index)\n",
    "        if isinstance(index, int):\n",
    "            seqs_comb = self.seq_df[index]\n",
    "            res = self.res_df.iloc[index]\n",
    "            mic = self.mic_df.iloc[index]\n",
    "        elif isinstance(index, str):\n",
    "            seqs_comb = self.seq_df[int(index)]\n",
    "            res = self.res_df.loc[index]\n",
    "            mic = self.mic_df.loc[index]\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Index needs to be an integer or a sample name present in the dataset\"\n",
    "            )\n",
    "\n",
    "        if self.transform:\n",
    "            res = np.log(res)\n",
    "            \n",
    "            # self.res_mean = self.res_df.mean()\n",
    "            # self.res_std = self.res_df.std()\n",
    "            # res = (res - self.res_mean) / self.res_std\n",
    "            # res = self.transform(res)\n",
    "        return torch.unsqueeze(torch.tensor(seqs_comb).float(), 0), torch.tensor(mic).long().flatten().squeeze(), torch.tensor(res).long().flatten().squeeze()\n",
    "    def __len__(self):\n",
    "        return self.res_df.shape[0]\n",
    "\n",
    "training_dataset = Dataset(train_data, train_target, f'{drug}_MIC_x',f'{drug}_MIC_y', one_hot_dtype=torch.float, transform=False)\n",
    "# train_dataset, val_dataset = random_split(training_dataset, [int(len(training_dataset)*0.9), len(training_dataset)-int(len(training_dataset)*0.9)])\n",
    "\n",
    "# test_data = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/snps_crypticTest_emb.npy')\n",
    "# train_data = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/drs_crypticTest_emb.npy')\n",
    "testing_dataset = Dataset(test_data, test_target, f'{drug}_MIC_x',f'{drug}_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "\n",
    "train_idx, validation_idx = train_test_split(np.arange(len(train_data)),\n",
    "                                             test_size=0.1,\n",
    "                                             random_state=42,\n",
    "                                             shuffle=True,\n",
    "                                             stratify=train_target)\n",
    "\n",
    "# Subset dataset for train and val\n",
    "train_dataset = Subset(training_dataset, train_idx)\n",
    "val_dataset = Subset(training_dataset, validation_idx)\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# # device = 'cpu'\n",
    "\n",
    "y_true = train_target\n",
    "# y_true = pd.concat([train_target, test_target])\n",
    "\n",
    "column_weight_maps = {}\n",
    "\n",
    "for column in y_true.columns:\n",
    "    column_values = y_true[column].dropna().values\n",
    "    values, counts = np.unique(column_values, return_counts=True)\n",
    "    frequency = counts / len(column_values)\n",
    "    \n",
    "    # Calculate weights as the inverse of frequencies\n",
    "    weights_inverse = 1/frequency\n",
    "    # weights_inverse = 1 - frequency\n",
    "    \n",
    "    # Normalize weights to ensure they sum up to 1\n",
    "    weights_normalized = weights_inverse / np.sum(weights_inverse)\n",
    "    \n",
    "    # Map each MIC value to its corresponding weight\n",
    "    weight_map = {value: weight for value, weight in zip(values, weights_normalized)}\n",
    "    \n",
    "    column_weight_maps[column] = weight_map\n",
    "\n",
    "def get_weighted_masked_cross_entropy_loss(column_weight_maps):\n",
    "    \"\"\"\n",
    "    Creates a loss function that computes a weighted cross entropy loss, taking into account class imbalances.\n",
    "    :param column_weight_maps: Dictionary mapping column names to their corresponding class weight maps.\n",
    "    \"\"\"\n",
    "    def weighted_masked_cross_entropy_loss(y_pred, y_true):\n",
    "        # weighted_losses = torch.Tensor().to(device)\n",
    "        weighted_losses = []\n",
    "        col_weight_map = column_weight_maps\n",
    "        # print(col_weight_map)\n",
    "        mean_weight = np.mean(list(col_weight_map.values())) # just in case if a number is not recognised and the loss doesn't go crazy\n",
    "\n",
    "        # print(y_pred.size())\n",
    "        # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        weights_col = [col_weight_map.get(y.item(), mean_weight) for y in y_true]\n",
    "        # print(weights_col)\n",
    "        # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        loss_fn = F.cross_entropy\n",
    "        col_loss = loss_fn(y_pred, y_true, reduction = 'none').to(device)\n",
    "        \n",
    "        # loss_fn = nn.CrossEntropyLoss(reduction = 'none')\n",
    "        # col_loss = loss_fn(y_pred, y_true)\n",
    "        # print(y_true.dtype)\n",
    "        # print(col_loss)\n",
    "        weights_col = torch.Tensor(weights_col).to(device)\n",
    "        # print(weights_col)\n",
    "        # print(col_loss)\n",
    "        weighted_col_loss = weights_col * col_loss\n",
    "        # print(weighted_col_loss)\n",
    "        weighted_losses.append(weighted_col_loss.mean())\n",
    "\n",
    "        total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        \n",
    "        # for i, column in enumerate(column_weight_maps.keys()):\n",
    "        #     col_weight_map = column_weight_maps[column]\n",
    "        #     print(y_pred.size())\n",
    "        #     # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        #     weights_col = torch.tensor([col_weight_map[y.item()] for y in y_true[:, i]], dtype=torch.float32, device=y_true.device)\n",
    "        #     print(weights_col)\n",
    "        #     # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        #     loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        #     col_loss = loss_fn(y_pred[:, i,], y_true[:, i])\n",
    "            \n",
    "        #     weighted_col_loss = weights_col * col_loss\n",
    "        #     weighted_losses.append(weighted_col_loss.mean())\n",
    "        \n",
    "        # total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        return total_weighted_loss\n",
    "\n",
    "    return weighted_masked_cross_entropy_loss\n",
    "\n",
    "# Also assuming `columns` is a list of your target column names corresponding to y_true and y_pred\n",
    "weighted_cross_entropy_loss_fn = get_weighted_masked_cross_entropy_loss(column_weight_maps[f'{drug}_MIC_x'])\n",
    "# loss = weighted_cross_entropy_loss_fn(y_true_tensor, y_pred_logits, columns)\n",
    "\n",
    "def save_to_file(file_path, appendix, epoch, lr, cnndr, fcdr, l2, train_loss, test_loss, optimizer, model):\n",
    "    train_loss = [float(arr) for arr in train_loss]\n",
    "    test_loss = [float(arr) for arr in test_loss]\n",
    "    with open(file_path, \"a\") as f:\n",
    "        f.write(f\"#>> {appendix}, Epoch: {epoch}, LR: {lr}, fcDR: {fcdr}\\n\")\n",
    "        f.write(f\"Train_Loss= {train_loss}\\n\")\n",
    "        f.write(f\"Test_Loss= {test_loss}\\n\")\n",
    "        f.write(f\"lossGraph(Train_Loss, Test_Loss, '{appendix}-Epoch-{epoch}-LR-{lr}-fcDR-{fcdr}')\\n\")\n",
    "\n",
    "    torch.save({\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model': model.state_dict(),\n",
    "    }, f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/seq-{appendix}-{epoch}-{lr}-{cnndr}-{fcdr}-{l2}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_data = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/snps_crypticTest_emb.npy')\n",
    "# test_target = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/drs_crypticTest_emb.npy')\n",
    "# testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "# testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        num_classes=6,\n",
    "        num_filters=64,\n",
    "        filter_length=25,\n",
    "        num_conv_layers=2,\n",
    "        filter_scaling_factor=1,  # New parameter\n",
    "        num_dense_neurons=256,\n",
    "        num_dense_layers=2,\n",
    "        conv_dropout_rate=0.0,\n",
    "        dense_dropout_rate=0.2,\n",
    "        l1_strength = 0.1,\n",
    "        return_logits=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_length = filter_length\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.num_dense_layers = num_dense_layers\n",
    "        self.conv_dropout_rate = conv_dropout_rate\n",
    "        self.dense_dropout_rate = dense_dropout_rate\n",
    "        self.return_logits = return_logits\n",
    "\n",
    "        \n",
    "        # now define the actual model\n",
    "        # self.feature_extraction_layer = self._conv_layer(\n",
    "            # in_channels, num_filters, filter_length\n",
    "        # )\n",
    "        self.feature_extraction_layer = self._conv_layer_extract(\n",
    "            in_channels, num_filters, filter_length\n",
    "        )\n",
    "        #dynamic filter scaling from deepram\n",
    "        current_num_filters1 = num_filters\n",
    "        self.conv_layers1 = nn.ModuleList()\n",
    "        for i in range(num_conv_layers):\n",
    "            layer = self._conv_layer(current_num_filters1, int(current_num_filters1 * filter_scaling_factor), 3)\n",
    "            self.conv_layers1.append(layer)\n",
    "            current_num_filters1 = int(current_num_filters1 * filter_scaling_factor)\n",
    "            \n",
    "        current_num_filters2 = 32\n",
    "        self.conv_layers2 = nn.ModuleList()\n",
    "        for i in range(num_conv_layers):\n",
    "            layer = self._conv_layer(current_num_filters1, int(current_num_filters2 * filter_scaling_factor), 3)\n",
    "            self.conv_layers2.append(layer)\n",
    "            current_num_filters1 = current_num_filters2\n",
    "            \n",
    "        # self.dense_layers = nn.ModuleList(\n",
    "        #     self._dense_layer(input_dim, num_dense_neurons)\n",
    "        #     for input_dim in [31200]\n",
    "        #     + [num_dense_neurons] * (num_dense_layers - 1)\n",
    "        # )\n",
    "        \n",
    "        self.dense_layers = nn.ModuleList(\n",
    "            self._dense_layer(input_dim, num_dense_neurons)\n",
    "            for input_dim in [current_num_filters2]\n",
    "            + [num_dense_neurons] * (num_dense_layers - 1) \n",
    "        )\n",
    "        \n",
    "        # self.prediction_layer = (\n",
    "        #     nn.Linear(num_dense_neurons, num_classes)\n",
    "        #     if return_logits\n",
    "        #     else nn.Sequential(nn.Linear(num_dense_neurons, num_classes), nn.ReLU()) #difference between sequential and nn.moduleList?\n",
    "        # )\n",
    "  \n",
    "        \n",
    "        dense_output_size = num_dense_neurons  # Assuming dense layer output is num_dense_neurons\n",
    "        additional_input_size = 1  # Assuming additional input is a single value\n",
    "        total_input_size = dense_output_size + additional_input_size  # Total input size for the prediction layer\n",
    "        # total_input_size = dense_output_size  # Total input size for the prediction layer\n",
    "\n",
    "        self.prediction_layer = (\n",
    "            nn.Linear(total_input_size, num_classes)  # If logits are returned directly\n",
    "            if return_logits\n",
    "            else nn.Sequential(\n",
    "                nn.Linear(total_input_size, int(total_input_size * 0.7)),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dense_dropout_rate),  # Dropout layer after the first ReLU activation\n",
    "                nn.Linear(int(total_input_size * 0.7), int(total_input_size * 0.5)),  # Optional additional layer with reduced size\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dense_dropout_rate),  # Dropout layer after the second ReLU activation\n",
    "                nn.Linear(int(total_input_size * 0.5), num_classes)  # Final layer to match the number of classes\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.m = nn.MaxPool1d(3, stride=1)\n",
    "        \n",
    "        self.apply(self.init_weights)    \n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _conv_layer(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(p=self.conv_dropout_rate),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def _conv_layer_extract(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def _dense_layer(self, n_in, n_out):\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(p=self.dense_dropout_rate),\n",
    "            nn.Linear(n_in, n_out),\n",
    "            nn.BatchNorm1d(n_out),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def l1_regularization(self):\n",
    "        l1_loss_example = 0\n",
    "        for param in self.parameters():\n",
    "            l1_loss_example += torch.sum(torch.abs(param))\n",
    "        return self.l1_strength * l1_loss_example\n",
    "\n",
    "    def forward(self, x, additional_input):\n",
    "        # Feature extraction\n",
    "        x = self.feature_extraction_layer(x)\n",
    "\n",
    "        # Convolutional layers\n",
    "        for layer in self.conv_layers1:\n",
    "            x = layer(x)\n",
    "        x = self.m(x)\n",
    "        for layer in self.conv_layers2:\n",
    "            x = layer(x)\n",
    "        x = self.m(x)\n",
    "        \n",
    "        # Flatten the tensor to [batch_size, features]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Dense layers\n",
    "        for layer in self.dense_layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        # # Concatenate additional input value\n",
    "        additional_input = additional_input.unsqueeze(1)\n",
    "        x = torch.cat((x, additional_input), dim=1)  # Concatenate along the feature dimension\n",
    "\n",
    "        # Prediction layer\n",
    "        x = self.prediction_layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# def l1loss(layer): # https://stackoverflow.com/questions/50054049/lack-of-sparse-solution-with-l1-regularization-in-pytorch\n",
    "#     return torch.norm(layer.weight, p=1)\n",
    "\n",
    "# def l1loss(sequence):\n",
    "#     l1_regularization = 0\n",
    "#     for module in sequence.modules():\n",
    "#         if isinstance(module, nn.Conv1d):  # Check if the module is a Conv1d layer\n",
    "#             l1_regularization += torch.norm(module.weight, p=1)\n",
    "#     return l1_regularization\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        num_classes=6,\n",
    "        num_filters=64,\n",
    "        filter_length=25,\n",
    "        num_conv_layers=2,\n",
    "        filter_scaling_factor=1,\n",
    "        num_dense_neurons=256,\n",
    "        num_dense_layers=2,\n",
    "        conv_dropout_rate=0.0,\n",
    "        dense_dropout_rate=0.2,\n",
    "        l1_strength=0.1,\n",
    "        return_logits=False,\n",
    "        # example_input_len=5000   # <-- YOU CAN SET THIS, OR DETECT FROM DATA\n",
    "        example_input_len = train_dataset[0][0].shape[-1]\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_length = filter_length\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.num_dense_layers = num_dense_layers\n",
    "        self.conv_dropout_rate = conv_dropout_rate\n",
    "        self.dense_dropout_rate = dense_dropout_rate\n",
    "        self.return_logits = return_logits\n",
    "        self.l1_strength = l1_strength\n",
    "\n",
    "        # ------------------------------\n",
    "        # FEATURE EXTRACTOR\n",
    "        # ------------------------------\n",
    "        self.feature_extraction_layer = self._conv_layer_extract(\n",
    "            in_channels, num_filters, filter_length\n",
    "        )\n",
    "\n",
    "        # Conv block 1\n",
    "        current_num_filters1 = num_filters\n",
    "        self.conv_layers1 = nn.ModuleList()\n",
    "        for i in range(num_conv_layers):\n",
    "            layer = self._conv_layer(current_num_filters1,\n",
    "                                     int(current_num_filters1 * filter_scaling_factor),\n",
    "                                     3)\n",
    "            self.conv_layers1.append(layer)\n",
    "            current_num_filters1 = int(current_num_filters1 * filter_scaling_factor)\n",
    "\n",
    "        # Conv block 2\n",
    "        current_num_filters2 = 32\n",
    "        self.conv_layers2 = nn.ModuleList()\n",
    "        for i in range(num_conv_layers):\n",
    "            layer = self._conv_layer(current_num_filters1,\n",
    "                                     int(current_num_filters2 * filter_scaling_factor),\n",
    "                                     3)\n",
    "            self.conv_layers2.append(layer)\n",
    "            current_num_filters1 = current_num_filters2\n",
    "\n",
    "        self.m = nn.MaxPool1d(3, stride=1)\n",
    "\n",
    "        # ------------------------------\n",
    "        # DYNAMICALLY COMPUTE FLATTENED FEATURE SIZE\n",
    "        # ------------------------------\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, example_input_len)\n",
    "            dummy_out = self._forward_conv_only(dummy)\n",
    "            self.flat_dim = dummy_out.view(1, -1).shape[1]\n",
    "\n",
    "        # ------------------------------\n",
    "        # DENSE LAYERS (NOW DYNAMIC)\n",
    "        # ------------------------------\n",
    "        self.dense_layers = nn.ModuleList(\n",
    "            [self._dense_layer(self.flat_dim, num_dense_neurons)] +\n",
    "            [self._dense_layer(num_dense_neurons, num_dense_neurons)\n",
    "             for _ in range(num_dense_layers - 1)]\n",
    "        )\n",
    "\n",
    "        # ------------------------------\n",
    "        # PREDICTION LAYER\n",
    "        # ------------------------------\n",
    "        total_input_size = num_dense_neurons + 1  # +1 for additional_input\n",
    "\n",
    "        self.prediction_layer = (\n",
    "            nn.Linear(total_input_size, num_classes)\n",
    "            if return_logits\n",
    "            else nn.Sequential(\n",
    "                nn.Linear(total_input_size, int(total_input_size * 0.7)),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dense_dropout_rate),\n",
    "                nn.Linear(int(total_input_size * 0.7), int(total_input_size * 0.5)),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dense_dropout_rate),\n",
    "                nn.Linear(int(total_input_size * 0.5), num_classes)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    # -------------------------\n",
    "    # HELPER FUNCTIONS\n",
    "    # -------------------------\n",
    "    def _forward_conv_only(self, x):\n",
    "        x = self.feature_extraction_layer(x)\n",
    "        for layer in self.conv_layers1:\n",
    "            x = layer(x)\n",
    "        x = self.m(x)\n",
    "\n",
    "        for layer in self.conv_layers2:\n",
    "            x = layer(x)\n",
    "        x = self.m(x)\n",
    "\n",
    "        return x   # not flattened yet\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _conv_layer(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(p=self.conv_dropout_rate),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def _conv_layer_extract(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def _dense_layer(self, n_in, n_out):\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(p=self.dense_dropout_rate),\n",
    "            nn.Linear(n_in, n_out),\n",
    "            nn.BatchNorm1d(n_out),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # FORWARD PASS\n",
    "    # -------------------------\n",
    "    def forward(self, x, additional_input):\n",
    "\n",
    "        x = self._forward_conv_only(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        for layer in self.dense_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        additional_input = additional_input.unsqueeze(1)\n",
    "        x = torch.cat((x, additional_input), dim=1)\n",
    "\n",
    "        x = self.prediction_layer(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 50/500 [06:15<56:02,  7.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Training loss: 0.26282408833503723\n",
      "Validation loss: 0.2644728124141693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 100/500 [12:29<49:55,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100\n",
      "Training loss: 0.26013070344924927\n",
      "Validation loss: 0.2602468729019165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 150/500 [18:46<43:51,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150\n",
      "Training loss: 0.2597578763961792\n",
      "Validation loss: 0.25947245955467224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 200/500 [25:02<37:45,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200\n",
      "Training loss: 0.25910624861717224\n",
      "Validation loss: 0.258876770734787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 250/500 [31:17<31:04,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250\n",
      "Training loss: 0.2590838074684143\n",
      "Validation loss: 0.2586255669593811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 300/500 [37:31<24:59,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300\n",
      "Training loss: 0.25920748710632324\n",
      "Validation loss: 0.2586715519428253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 350/500 [43:48<18:50,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350\n",
      "Training loss: 0.25903040170669556\n",
      "Validation loss: 0.25868135690689087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 400/500 [50:02<12:24,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400\n",
      "Training loss: 0.2590715289115906\n",
      "Validation loss: 0.25883719325065613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 450/500 [56:16<06:14,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 450\n",
      "Training loss: 0.25889813899993896\n",
      "Validation loss: 0.25893348455429077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 500/500 [1:02:31<00:00,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500\n",
      "Training loss: 0.2591494619846344\n",
      "Validation loss: 0.2591931223869324\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_23833/449602202.py:170: UserWarning:\n",
      "\n",
      "Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_0.0001_weighted_balanced.png-LZD\n",
      "======================\n",
      "Optimizer details:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "Learning rate: 0.0001\n",
      "Weight decay: 0.0001\n",
      "======================\n",
      "Accuracy: 0.4881956155143339\n",
      "Mae: 0.657672849915683\n",
      "F1 Score: 0.32261274036478504\n",
      "conf_matrix: [[  0   0   0  29   0   1   0]\n",
      " [  0   0   0  82   0   1   0]\n",
      " [  0   0   0 317   0   0   1]\n",
      " [  0   0   0 578   0   1   1]\n",
      " [  0   0   0 159   0   0   0]\n",
      " [  0   0   0   6   0   0   2]\n",
      " [  0   0   0   7   0   0   1]]\n",
      "AUC-ovr: 0.489945513373243\n",
      "Entropy: 1.9287286855419843\n",
      "======================\n",
      "Doubling Dilution Accuracy: 0.8912310286677909\n",
      "AUC: 0.591613247863248\n",
      "Sensitivity: 0.1875\n",
      "Specificity: 0.9957264957264957\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input parameter\n",
    "lr = 1e-4\n",
    "epoch = 500\n",
    "conv_dropout_rate=0.4\n",
    "dense_dropout_rate=0.7\n",
    "weight_decay=1e-4\n",
    "######################################\n",
    "\n",
    "model = Model(\n",
    "num_classes=len(np.unique(encoded_mic[f'{drug}_MIC'])),\n",
    "num_filters=64,\n",
    "num_conv_layers=2,\n",
    "# num_dense_neurons=256, # batch_size = 64\n",
    "num_dense_neurons=128, # batch_size = 64\n",
    "num_dense_layers=2,\n",
    "return_logits=False,\n",
    "conv_dropout_rate=conv_dropout_rate,\n",
    "dense_dropout_rate=dense_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# model = Model( #! way too memory intensive\n",
    "# num_classes=13,\n",
    "# num_filters=128,\n",
    "# num_conv_layers=2,\n",
    "# num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "# num_dense_layers=2,\n",
    "# return_logits=True,\n",
    "# conv_dropout_rate=0,\n",
    "# dense_dropout_rate=0\n",
    "# ).to(device)\n",
    "## early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "patience_counter = 0\n",
    "lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "batch_size = 64\n",
    "# lr = 0.0085\n",
    "# lr = 0.00002\n",
    "lr = lr\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "# test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = masked_weighted_MAE\n",
    "# criterion = masked_weighted_MSE\n",
    "criterion = weighted_cross_entropy_loss_fn\n",
    "# criterion = masked_MAE\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "# scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "#%%\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "# ic.enable()\n",
    "ic.disable()\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    # print(f'Epoch {e}')\n",
    "    for x_train, y_train, y_train_res in train_loader:\n",
    "        x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "        y_batch = y_train.to(device).long()  # Convert to torch.long\n",
    "        y_batch_res = y_train_res.to(device)\n",
    "        \n",
    "        x_batch = x_batch.float()\n",
    "        pred = model(x_batch.float(),y_batch_res.float())\n",
    "        \n",
    "        pred = pred.float()  # Convert predictions to float if necessary\n",
    "        y_batch = y_batch.long()  # Ensure targets are long integers\n",
    "        # break\n",
    "        # loss_train = loss_corn(pred, y_batch, 3, class_weights)\n",
    "        # print(pred, y_batch)\n",
    "        loss_train = criterion(pred,y_batch)\n",
    "        # print(pred)\n",
    "        # print(y_batch)\n",
    "        # print(loss_train)\n",
    "        train_batch_loss.append(loss_train)        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update the learning rate\n",
    "        # break\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # print('>> test')\n",
    "        for x_test, y_test, y_test_res in test_loader:\n",
    "            x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            y_batch = y_test.to(device)\n",
    "            y_batch_res = y_test_res.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float(), y_batch_res.float())\n",
    "            loss_test = criterion(pred,y_batch)\n",
    "            # pred = pred.unsqueeze(0)\n",
    "            # print(pred[:10])\n",
    "            # print(y_batch[:10])\n",
    "\n",
    "            # loss_test = loss_corn(pred, y_batch, 3, class_weights)\n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "    if e%50 == 0:\n",
    "        print(f'Epoch {e}')\n",
    "        print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "        print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    # #! implementing early stopping\n",
    "    # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "    # print(f'Current val loss: {current_val_loss}')\n",
    "    # print(f'Best val loss: {best_val_loss}')\n",
    "    # if current_val_loss < best_val_loss:\n",
    "    #     best_val_loss = current_val_loss\n",
    "    #     patience_counter = 0  # reset patience counter\n",
    "    #     # Save the best model\n",
    "    #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         torch.save({\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #         'model': model.state_dict(),\n",
    "    #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "    #         break  # Early stopping\n",
    "    \n",
    "print('==='*10)\n",
    "# torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "save_to_file('trials3.txt', 'aa-training_weighted_balance-MXF' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "             train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, epoch+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-{drug}.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-{drug}')\n",
    "\n",
    "#%%\n",
    "# testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "# testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "# testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Ensure the model is on the same device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "pred_prob = [] \n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test, y_test_res in testing_loader1:\n",
    "        # Move input and target data to the correct device\n",
    "        x_test = x_test.to(device).float()\n",
    "        y_test = y_test.to(device).float()\n",
    "        y_test_res = y_test_res.to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(x_test, y_test_res)\n",
    "        pred_prob.append(F.softmax(pred, dim=1).detach().cpu().numpy().tolist()[0])\n",
    "        # Append predictions and targets to lists\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "        \n",
    "# Flatten the target list\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")   \n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print('AUC-ovr:', roc_auc_score(target_list, np.array(pred_prob), multi_class='ovr'))\n",
    "# Calculate entropy for the predicted probabilities\n",
    "pred_prob_array = np.array(pred_prob).squeeze()\n",
    "entropy_values = np.mean(entropy(pred_prob_array.T))\n",
    "\n",
    "print(f\"Entropy: {entropy_values}\")\n",
    "print(\"======================\")\n",
    "\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min.values-1, target_max.values+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "# Calculate AUC\n",
    "cutoff = cutoff\n",
    "test_target_bi = (target_list >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cnn with bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target[f'{drug}_MIC_y'] = test_target['tbp']\n",
    "# runninng with tbp empty predictions\n",
    "# test_target[f'{drug}_MIC_y'] = 0\n",
    "\n",
    "training_dataset = Dataset(train_data, train_target, f'{drug}_MIC_x',f'{drug}_MIC_y', one_hot_dtype=torch.float, transform=False)\n",
    "# train_dataset, val_dataset = random_split(training_dataset, [int(len(training_dataset)*0.9), len(training_dataset)-int(len(training_dataset)*0.9)])\n",
    "\n",
    "# test_data = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/snps_crypticTest_emb.npy')\n",
    "# train_data = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/drs_crypticTest_emb.npy')\n",
    "testing_dataset = Dataset(test_data, test_target, f'{drug}_MIC_x',f'{drug}_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "\n",
    "train_idx, validation_idx = train_test_split(np.arange(len(train_data)),\n",
    "                                             test_size=0.1,\n",
    "                                             random_state=42,\n",
    "                                             shuffle=True,\n",
    "                                             stratify=train_target)\n",
    "\n",
    "# Subset dataset for train and val\n",
    "train_dataset = Subset(training_dataset, train_idx)\n",
    "val_dataset = Subset(training_dataset, validation_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-03 12:29:18,894] A new study created in memory with name: no-name-cd0d4b8b-01f6-4baf-bffd-faebde92ce14\n",
      "[I 2025-12-03 12:37:21,146] Trial 0 finished with value: 0.18832941881070533 and parameters: {'lr': 5.6115164153345e-05, 'weight_decay': 0.0007114476009343421, 'conv_dropout_rate': 0.43919636508684307, 'dense_dropout_rate': 0.4789267873576293, 'num_filters': 47, 'num_conv_layers': 1, 'num_dense_neurons': 75, 'num_dense_layers': 3}. Best is trial 0 with value: 0.18832941881070533.\n",
      "[I 2025-12-03 12:45:25,796] Trial 1 finished with value: 0.1877781351407369 and parameters: {'lr': 0.00015930522616241006, 'weight_decay': 0.000133112160807369, 'conv_dropout_rate': 0.012350696577481468, 'dense_dropout_rate': 0.7759278817295955, 'num_filters': 112, 'num_conv_layers': 1, 'num_dense_neurons': 99, 'num_dense_layers': 1}. Best is trial 1 with value: 0.1877781351407369.\n",
      "[I 2025-12-03 12:54:06,096] Trial 2 finished with value: 0.18600352574139833 and parameters: {'lr': 4.059611610484306e-05, 'weight_decay': 3.752055855124284e-05, 'conv_dropout_rate': 0.25916701118526947, 'dense_dropout_rate': 0.23298331215843354, 'num_filters': 91, 'num_conv_layers': 1, 'num_dense_neurons': 120, 'num_dense_layers': 2}. Best is trial 2 with value: 0.18600352574139833.\n",
      "[I 2025-12-03 13:02:15,013] Trial 3 finished with value: 0.18442923513551554 and parameters: {'lr': 8.168455894760161e-05, 'weight_decay': 0.0002267398652378039, 'conv_dropout_rate': 0.11980426929501584, 'dense_dropout_rate': 0.4113875507308893, 'num_filters': 89, 'num_conv_layers': 1, 'num_dense_neurons': 181, 'num_dense_layers': 1}. Best is trial 3 with value: 0.18442923513551554.\n",
      "[I 2025-12-03 13:10:50,181] Trial 4 finished with value: 0.24466547556221485 and parameters: {'lr': 1.3492834268013232e-05, 'weight_decay': 0.0007025166339242157, 'conv_dropout_rate': 0.5793792198447356, 'dense_dropout_rate': 0.646717878493169, 'num_filters': 61, 'num_conv_layers': 1, 'num_dense_neurons': 196, 'num_dense_layers': 2}. Best is trial 3 with value: 0.18442923513551554.\n",
      "[I 2025-12-03 13:15:55,839] Trial 5 finished with value: 0.26594203275938827 and parameters: {'lr': 1.7541893487450798e-05, 'weight_decay': 3.058656666978529e-05, 'conv_dropout_rate': 0.020633112669131037, 'dense_dropout_rate': 0.7274563216630257, 'num_filters': 57, 'num_conv_layers': 2, 'num_dense_neurons': 124, 'num_dense_layers': 2}. Best is trial 3 with value: 0.18442923513551554.\n",
      "[I 2025-12-03 13:25:12,888] Trial 6 finished with value: 0.18962369735042253 and parameters: {'lr': 0.00012399967836846095, 'weight_decay': 3.5856126103453987e-06, 'conv_dropout_rate': 0.5817507766587351, 'dense_dropout_rate': 0.6201062586888917, 'num_filters': 123, 'num_conv_layers': 3, 'num_dense_neurons': 179, 'num_dense_layers': 3}. Best is trial 3 with value: 0.18442923513551554.\n",
      "[I 2025-12-03 13:34:34,288] Trial 7 finished with value: 0.1890881760045886 and parameters: {'lr': 1.5030900645056805e-05, 'weight_decay': 3.87211803217458e-06, 'conv_dropout_rate': 0.02713637334632284, 'dense_dropout_rate': 0.2602642646106115, 'num_filters': 69, 'num_conv_layers': 1, 'num_dense_neurons': 223, 'num_dense_layers': 2}. Best is trial 3 with value: 0.18442923513551554.\n",
      "[I 2025-12-03 13:44:20,914] Trial 8 finished with value: 0.18799779253701368 and parameters: {'lr': 3.6464395589807184e-05, 'weight_decay': 4.247058562261871e-05, 'conv_dropout_rate': 0.08455453498485759, 'dense_dropout_rate': 0.6417575846032317, 'num_filters': 39, 'num_conv_layers': 3, 'num_dense_neurons': 213, 'num_dense_layers': 1}. Best is trial 3 with value: 0.18442923513551554.\n",
      "[I 2025-12-03 13:52:59,040] Trial 9 finished with value: 0.23253064323216677 and parameters: {'lr': 1.0257563974185649e-05, 'weight_decay': 0.0002795015916508333, 'conv_dropout_rate': 0.4241144063085703, 'dense_dropout_rate': 0.5832057344327899, 'num_filters': 106, 'num_conv_layers': 1, 'num_dense_neurons': 133, 'num_dense_layers': 1}. Best is trial 3 with value: 0.18442923513551554.\n",
      "[I 2025-12-03 14:03:39,834] Trial 10 finished with value: 0.18356981097410122 and parameters: {'lr': 0.000698704649008624, 'weight_decay': 1.0422971466648463e-06, 'conv_dropout_rate': 0.18461652687015728, 'dense_dropout_rate': 0.014369500491855536, 'num_filters': 86, 'num_conv_layers': 2, 'num_dense_neurons': 249, 'num_dense_layers': 1}. Best is trial 10 with value: 0.18356981097410122.\n",
      "[I 2025-12-03 14:11:54,390] Trial 11 finished with value: 0.1833361495907108 and parameters: {'lr': 0.0006294532478500545, 'weight_decay': 1.047999311322588e-06, 'conv_dropout_rate': 0.1941720189778417, 'dense_dropout_rate': 0.013748920215166998, 'num_filters': 85, 'num_conv_layers': 2, 'num_dense_neurons': 254, 'num_dense_layers': 1}. Best is trial 11 with value: 0.1833361495907108.\n",
      "[I 2025-12-03 14:22:35,567] Trial 12 finished with value: 0.18359723966568708 and parameters: {'lr': 0.0008142120841588115, 'weight_decay': 1.055555333664687e-06, 'conv_dropout_rate': 0.23317916206492492, 'dense_dropout_rate': 0.0029125962464887894, 'num_filters': 81, 'num_conv_layers': 2, 'num_dense_neurons': 256, 'num_dense_layers': 1}. Best is trial 11 with value: 0.1833361495907108.\n",
      "[I 2025-12-03 14:35:25,293] Trial 13 finished with value: 0.1836295205478867 and parameters: {'lr': 0.0009434443885389003, 'weight_decay': 1.2611236054600694e-06, 'conv_dropout_rate': 0.1770347633092498, 'dense_dropout_rate': 0.00811397674992932, 'num_filters': 101, 'num_conv_layers': 2, 'num_dense_neurons': 256, 'num_dense_layers': 1}. Best is trial 11 with value: 0.1833361495907108.\n",
      "[I 2025-12-03 14:46:01,496] Trial 14 finished with value: 0.1834533056244254 and parameters: {'lr': 0.00033756948175707506, 'weight_decay': 5.3047485033023735e-06, 'conv_dropout_rate': 0.3167106057439115, 'dense_dropout_rate': 0.1276247186454773, 'num_filters': 74, 'num_conv_layers': 2, 'num_dense_neurons': 231, 'num_dense_layers': 1}. Best is trial 11 with value: 0.1833361495907108.\n",
      "[I 2025-12-03 14:56:58,844] Trial 15 finished with value: 0.18391224897156158 and parameters: {'lr': 0.0003271691468805144, 'weight_decay': 6.499021027885739e-06, 'conv_dropout_rate': 0.35739885895693446, 'dense_dropout_rate': 0.16630099450997554, 'num_filters': 68, 'num_conv_layers': 3, 'num_dense_neurons': 227, 'num_dense_layers': 2}. Best is trial 11 with value: 0.1833361495907108.\n",
      "[I 2025-12-03 15:07:17,496] Trial 16 finished with value: 0.18385004779944816 and parameters: {'lr': 0.00031628893488130195, 'weight_decay': 1.077739227038727e-05, 'conv_dropout_rate': 0.33372066831485653, 'dense_dropout_rate': 0.1313307138356483, 'num_filters': 67, 'num_conv_layers': 2, 'num_dense_neurons': 233, 'num_dense_layers': 1}. Best is trial 11 with value: 0.1833361495907108.\n",
      "[I 2025-12-03 15:15:38,283] Trial 17 finished with value: 0.18383950336525837 and parameters: {'lr': 0.0003568142237842736, 'weight_decay': 2.8358787286555118e-06, 'conv_dropout_rate': 0.41582077073203905, 'dense_dropout_rate': 0.13258094210339438, 'num_filters': 77, 'num_conv_layers': 2, 'num_dense_neurons': 204, 'num_dense_layers': 3}. Best is trial 11 with value: 0.1833361495907108.\n",
      "[I 2025-12-03 15:24:23,459] Trial 18 finished with value: 0.18424003664404154 and parameters: {'lr': 0.00020326739957561538, 'weight_decay': 1.531599328597221e-05, 'conv_dropout_rate': 0.2925450983852529, 'dense_dropout_rate': 0.3114310456058059, 'num_filters': 95, 'num_conv_layers': 2, 'num_dense_neurons': 174, 'num_dense_layers': 2}. Best is trial 11 with value: 0.1833361495907108.\n",
      "[I 2025-12-03 15:32:35,911] Trial 19 finished with value: 0.18363136891275644 and parameters: {'lr': 0.0005175712433987563, 'weight_decay': 2.1792597383800013e-06, 'conv_dropout_rate': 0.19227637584261392, 'dense_dropout_rate': 0.11402514625554351, 'num_filters': 50, 'num_conv_layers': 3, 'num_dense_neurons': 156, 'num_dense_layers': 1}. Best is trial 11 with value: 0.1833361495907108.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best hyperparameters:\n",
      "{'lr': 0.0006294532478500545, 'weight_decay': 1.047999311322588e-06, 'conv_dropout_rate': 0.1941720189778417, 'dense_dropout_rate': 0.013748920215166998, 'num_filters': 85, 'num_conv_layers': 2, 'num_dense_neurons': 254, 'num_dense_layers': 1}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # ------------------------\n",
    "    # Hyperparameters to tune\n",
    "    # ------------------------\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    conv_dropout_rate = trial.suggest_float(\"conv_dropout_rate\", 0.0, 0.6)\n",
    "    dense_dropout_rate = trial.suggest_float(\"dense_dropout_rate\", 0.0, 0.8)\n",
    "    num_filters = trial.suggest_int(\"num_filters\", 32, 128)\n",
    "    num_conv_layers = trial.suggest_int(\"num_conv_layers\", 1, 3)\n",
    "    num_dense_neurons = trial.suggest_int(\"num_dense_neurons\", 64, 256)\n",
    "    num_dense_layers = trial.suggest_int(\"num_dense_layers\", 1, 3)\n",
    "\n",
    "    # ------------------------\n",
    "    # Model\n",
    "    # ------------------------\n",
    "    model = Model(\n",
    "        num_classes=len(np.unique(encoded_mic[f'{drug}_MIC'])),\n",
    "        num_filters=num_filters,\n",
    "        num_conv_layers=num_conv_layers,\n",
    "        num_dense_neurons=num_dense_neurons,\n",
    "        num_dense_layers=num_dense_layers,\n",
    "        conv_dropout_rate=conv_dropout_rate,\n",
    "        dense_dropout_rate=dense_dropout_rate,\n",
    "        return_logits=False\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = weighted_cross_entropy_loss_fn\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=8)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=8)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    patience = 8\n",
    "\n",
    "    # ------------------------\n",
    "    # Training\n",
    "    # ------------------------\n",
    "    for epoch in range(1, 30):   # small number of epochs for tuning\n",
    "        model.train()\n",
    "        for x_train, y_train, y_train_res in train_loader:\n",
    "            x_train = torch.squeeze(x_train, 0).float().to(device)\n",
    "            y_train = y_train.long().to(device)\n",
    "            y_train_res = y_train_res.float().to(device)\n",
    "\n",
    "            pred = model(x_train, y_train_res)\n",
    "            loss = criterion(pred, y_train)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # ------------------------\n",
    "        # Validation\n",
    "        # ------------------------\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val, y_val_res in val_loader:\n",
    "                x_val = torch.squeeze(x_val, 0).float().to(device)\n",
    "                y_val = y_val.long().to(device)\n",
    "                y_val_res = y_val_res.float().to(device)\n",
    "                pred = model(x_val, y_val_res)\n",
    "                loss = criterion(pred, y_val)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(val_losses)\n",
    "\n",
    "        # Early stopping inside Optuna\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# OPTUNA STUDY\n",
    "# ------------------------\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "print(study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.0006294532478500545,\n",
       " 'weight_decay': 1.047999311322588e-06,\n",
       " 'conv_dropout_rate': 0.1941720189778417,\n",
       " 'dense_dropout_rate': 0.013748920215166998,\n",
       " 'num_filters': 85,\n",
       " 'num_conv_layers': 2,\n",
       " 'num_dense_neurons': 254,\n",
       " 'num_dense_layers': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 50/100 [05:44<05:44,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered\n",
      "Final best validation loss: 0.2575665405020118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters:\n",
    "# best_params = study.best_params\n",
    "\n",
    "best_params = {'lr': 0.0006294532478500545,\n",
    " 'weight_decay': 1.047999311322588e-06,\n",
    " 'conv_dropout_rate': 0.1941720189778417,\n",
    " 'dense_dropout_rate': 0.013748920215166998,\n",
    " 'num_filters': 85,\n",
    " 'num_conv_layers': 2,\n",
    " 'num_dense_neurons': 254,\n",
    " 'num_dense_layers': 1}\n",
    "\n",
    "model = Model(\n",
    "    num_classes=len(np.unique(encoded_mic[f'{drug}_MIC'])),\n",
    "    num_filters=best_params[\"num_filters\"],\n",
    "    num_conv_layers=best_params[\"num_conv_layers\"],\n",
    "    num_dense_neurons=best_params[\"num_dense_neurons\"],\n",
    "    num_dense_layers=best_params[\"num_dense_layers\"],\n",
    "    conv_dropout_rate=best_params[\"conv_dropout_rate\"],\n",
    "    dense_dropout_rate=best_params[\"dense_dropout_rate\"],\n",
    "    return_logits=False\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=best_params[\"lr\"],\n",
    "    weight_decay=best_params[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "criterion = weighted_cross_entropy_loss_fn\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=8)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in tqdm(range(1, 100 + 1)):\n",
    "    model.train()\n",
    "    for x_train, y_train, y_train_res in train_loader:\n",
    "        x_train = torch.squeeze(x_train, 0).float().to(device)\n",
    "        y_train = y_train.long().to(device)\n",
    "        y_train_res = y_train_res.float().to(device)\n",
    "\n",
    "        pred = model(x_train, y_train_res)\n",
    "        loss = criterion(pred, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # ------------------------\n",
    "    # Validation\n",
    "    # ------------------------\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val, y_val_res in val_loader:\n",
    "            x_val = torch.squeeze(x_val, 0).float().to(device)\n",
    "            y_val = y_val.long().to(device)\n",
    "            y_val_res = y_val_res.float().to(device)\n",
    "            pred = model(x_val, y_val_res)\n",
    "            loss = criterion(pred, y_val)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "\n",
    "        # SAVE MODEL HERE\n",
    "        torch.save(model.state_dict(),\n",
    "                   f\"saved_models/cnn_MIC_best_{drug}-bin.pt\")\n",
    "\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "print(\"Final best validation loss:\", best_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.load_state_dict(torch.load(f\"saved_models/cnn_MIC_best_{drug}-bin.pt\"))\n",
    "model.eval() \n",
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "Evaluation Metrics\n",
      "======================\n",
      "Accuracy: 0.03204047217537943\n",
      "Macro-F1: 0.035287061193932434\n",
      "MAE: 2.6441821247892072\n",
      "Confusion matrix:\n",
      " [[ 28   1   0   0   0   1   0]\n",
      " [ 81   0   1   1   0   0   0]\n",
      " [314   0   1   1   2   0   0]\n",
      " [567   0   1   8   3   0   1]\n",
      " [157   0   1   1   0   0   0]\n",
      " [  6   0   0   0   0   0   2]\n",
      " [  7   0   0   0   0   0   1]]\n",
      "AUC-OVR: 0.4916748092490065\n",
      "\n",
      "Class-specific AUC (OVR):\n",
      "Class 0: 0.47246251441753173\n",
      "Class 1: 0.5063572513080427\n",
      "Class 2: 0.50347614700171\n",
      "Class 3: 0.5101300216228519\n",
      "Class 4: 0.49381173718408017\n",
      "Class 5: 0.3867784380305603\n",
      "Class 6: 0.5687075551782683\n",
      "Entropy: 1.9126557\n",
      "Doubling Dilution Accuracy: 0.1087689713322091\n",
      "Binary AUC: 0.5928952991452991\n",
      "Sensitivity: 0.1875\n",
      "Specificity: 0.9982905982905983\n",
      "\n",
      "======================\n",
      "95 percent Confidence Intervals\n",
      "======================\n",
      "Accuracy: mean=0.03192537942664418, CI=(0.026981450252951095, 0.03794266441821248)\n",
      "AUC-OVR: mean=0.4920309061422752, CI=(0.4638474965328585, 0.5183022961841531)\n",
      "Doubling Dilution: mean=0.10871416526138279, CI=(0.10202360876897133, 0.1163575042158516)\n",
      "Macro-F1: mean=0.03421742372340902, CI=(0.009114160527480692, 0.07650720330622302)\n",
      "Sensitivity: mean=0.18846875, CI=(0.0, 0.375)\n",
      "Specificity: mean=0.998268376068376, CI=(0.9957264957264957, 1.0)\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, confusion_matrix,\n",
    "    roc_auc_score, classification_report, mean_absolute_error\n",
    ")\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load best model\n",
    "# ------------------------------------------------------------\n",
    "model.load_state_dict(torch.load(f\"saved_models/cnn_MIC_best_{drug}-bin.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Get predictions on test set\n",
    "# ------------------------------------------------------------\n",
    "pred_list = []\n",
    "pred_prob_list = []\n",
    "target_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test, y_test_res in testing_loader1:\n",
    "        x_test = x_test.to(device).float()\n",
    "        y_test = y_test.to(device).long()\n",
    "        y_test_res = y_test_res.to(device).float()\n",
    "\n",
    "        logits = model(x_test, y_test_res)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        pred_list.append(torch.argmax(probs, dim=1).cpu().numpy()[0])\n",
    "        pred_prob_list.append(probs.cpu().numpy()[0])\n",
    "        target_list.append(y_test.cpu().numpy()[0])\n",
    "\n",
    "pred_list = np.array(pred_list)\n",
    "target_list = np.array(target_list)\n",
    "pred_prob_array = np.array(pred_prob_list)\n",
    "\n",
    "print(\"======================\")\n",
    "print(\"Evaluation Metrics\")\n",
    "print(\"======================\")\n",
    "\n",
    "# -----------------------------\n",
    "# Basic metrics\n",
    "# -----------------------------\n",
    "accuracy = accuracy_score(target_list, pred_list)\n",
    "macro_f1 = f1_score(target_list, pred_list, average='macro')\n",
    "mae = mean_absolute_error(target_list, pred_list)\n",
    "conf = confusion_matrix(target_list, pred_list)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Macro-F1:\", macro_f1)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"Confusion matrix:\\n\", conf)\n",
    "\n",
    "# -----------------------------\n",
    "# Global AUC (multiclass OVR)\n",
    "# -----------------------------\n",
    "auc_ovr = roc_auc_score(target_list, pred_prob_array, multi_class='ovr')\n",
    "print(\"AUC-OVR:\", auc_ovr)\n",
    "\n",
    "# -----------------------------\n",
    "# Class-specific AUC\n",
    "# -----------------------------\n",
    "print(\"\\nClass-specific AUC (OVR):\")\n",
    "unique_classes = np.unique(target_list)\n",
    "class_to_index = {label: i for i, label in enumerate(unique_classes)}\n",
    "\n",
    "class_specific_auc = {}\n",
    "\n",
    "for c in unique_classes:\n",
    "    y_true_bin = (target_list == c).astype(int)\n",
    "    y_score_bin = pred_prob_array[:, class_to_index[c]]\n",
    "\n",
    "    try:\n",
    "        auc_cls = roc_auc_score(y_true_bin, y_score_bin)\n",
    "    except:\n",
    "        auc_cls = np.nan\n",
    "\n",
    "    class_specific_auc[c] = auc_cls\n",
    "    print(f\"Class {c}: {auc_cls}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Entropy (model uncertainty)\n",
    "# -----------------------------\n",
    "entropy_values = np.mean(entropy(pred_prob_array.T))\n",
    "print(\"Entropy:\", entropy_values)\n",
    "\n",
    "# -----------------------------\n",
    "# Doubling dilution accuracy\n",
    "# -----------------------------\n",
    "def is_within_doubling_dilution(pred, true):\n",
    "    # pred and true are the MIC class indices (log2-scaled)\n",
    "    return abs(int(pred) - int(true)) <= 1\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([\n",
    "    is_within_doubling_dilution(p, t)\n",
    "    for p, t in zip(pred_list, target_list)\n",
    "])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "# -----------------------------\n",
    "# Binary metrics (R/S)\n",
    "# -----------------------------\n",
    "test_target_bi = (target_list >= cutoff).astype(int)\n",
    "test_pred_bi = (pred_list >= cutoff).astype(int)\n",
    "\n",
    "auc_binary = roc_auc_score(test_target_bi, test_pred_bi)\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_pred_bi).ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"Binary AUC:\", auc_binary)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                 BOOTSTRAP CONFIDENCE INTERVALS\n",
    "# ============================================================\n",
    "\n",
    "def stratified_bootstrap(y_true, y_pred, y_proba, metric_fn, n_boot=2000, alpha=0.05):\n",
    "    classes = np.unique(y_true)\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(n_boot):\n",
    "        idx_list = []\n",
    "        for cls in classes:\n",
    "            cls_idx = np.where(y_true == cls)[0]\n",
    "            sampled = np.random.choice(cls_idx, size=len(cls_idx), replace=True)\n",
    "            idx_list.extend(sampled)\n",
    "\n",
    "        idx = np.array(idx_list)\n",
    "\n",
    "        m = metric_fn(y_true[idx], y_pred[idx], y_proba[idx])\n",
    "        if not np.isnan(m):\n",
    "            samples.append(m)\n",
    "\n",
    "    if len(samples) == 0:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "\n",
    "    samples = np.array(samples)\n",
    "    mean_val = np.mean(samples)\n",
    "    low = np.percentile(samples, 100 * (alpha / 2))\n",
    "    high = np.percentile(samples, 100 * (1 - alpha / 2))\n",
    "\n",
    "    return mean_val, low, high\n",
    "\n",
    "\n",
    "# Metric wrappers\n",
    "def metric_acc(y, p, s): return accuracy_score(y, p)\n",
    "def metric_auc(y, p, s): \n",
    "    try: return roc_auc_score(y, s, multi_class='ovr')\n",
    "    except: return np.nan\n",
    "\n",
    "def metric_dd(y, p, s):\n",
    "    return np.mean([abs(int(p_i) - int(y_i)) <= 1 for p_i, y_i in zip(p, y)])\n",
    "\n",
    "def metric_f1_macro(y, p, s): return f1_score(y, p, average='macro')\n",
    "\n",
    "def metric_sens(y, p, s):\n",
    "    yb = (y >= cutoff).astype(int)\n",
    "    pb = (p >= cutoff).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(yb, pb).ravel()\n",
    "    return tp / (tp + fn) if tp + fn > 0 else np.nan\n",
    "\n",
    "def metric_spec(y, p, s):\n",
    "    yb = (y >= cutoff).astype(int)\n",
    "    pb = (p >= cutoff).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(yb, pb).ravel()\n",
    "    return tn / (tn + fp) if tn + fp > 0 else np.nan\n",
    "\n",
    "\n",
    "print(\"\\n======================\")\n",
    "print(\"95 percent Confidence Intervals\")\n",
    "print(\"======================\")\n",
    "\n",
    "acc_mean, acc_low, acc_high = stratified_bootstrap(target_list, pred_list, pred_prob_array, metric_acc)\n",
    "auc_mean, auc_low, auc_high = stratified_bootstrap(target_list, pred_list, pred_prob_array, metric_auc)\n",
    "dd_mean, dd_low, dd_high = stratified_bootstrap(target_list, pred_list, pred_prob_array, metric_dd)\n",
    "f1_mean, f1_low, f1_high = stratified_bootstrap(target_list, pred_list, pred_prob_array, metric_f1_macro)\n",
    "sens_mean, sens_low, sens_high = stratified_bootstrap(target_list, pred_list, pred_prob_array, metric_sens)\n",
    "spec_mean, spec_low, spec_high = stratified_bootstrap(target_list, pred_list, pred_prob_array, metric_spec)\n",
    "\n",
    "print(f\"Accuracy: mean={acc_mean}, CI=({acc_low}, {acc_high})\")\n",
    "print(f\"AUC-OVR: mean={auc_mean}, CI=({auc_low}, {auc_high})\")\n",
    "print(f\"Doubling Dilution: mean={dd_mean}, CI=({dd_low}, {dd_high})\")\n",
    "print(f\"Macro-F1: mean={f1_mean}, CI=({f1_low}, {f1_high})\")\n",
    "print(f\"Sensitivity: mean={sens_mean}, CI=({sens_low}, {sens_high})\")\n",
    "print(f\"Specificity: mean={spec_mean}, CI=({spec_low}, {spec_high})\")\n",
    "print(\"======================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "is_within_doubling_dilution() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53392/2168517170.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Populate the dictionary using the correctness function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_within_doubling_dilution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmic_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# Increment correct count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: is_within_doubling_dilution() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# Initialize dictionary to store MIC values with correct and incorrect counts\n",
    "mic_dict = {mic: [0, 0] for mic in np.unique(target_list)}  # {MIC: [Correct count, Incorrect count]}\n",
    "\n",
    "# Populate the dictionary using the correctness function\n",
    "for pred, true in zip(pred_list, target_list):\n",
    "    if is_within_doubling_dilution(pred, true, target_min, target_max):\n",
    "        mic_dict[true][0] += 1  # Increment correct count\n",
    "    else:\n",
    "        mic_dict[true][1] += 1  # Increment incorrect count\n",
    "\n",
    "# Convert dictionary to DataFrame for plotting\n",
    "mic_df = pd.DataFrame.from_dict(mic_dict, orient=\"index\", columns=[\"Correct\", \"Erroneous\"])\n",
    "\n",
    "# Display the dictionary and DataFrame for verification\n",
    "# mic_dict, mic_df\n",
    "\n",
    "# Plot stacked bar chart using the dictionary-based DataFrame\n",
    "# Reverse log2 transformation of the current MIC values\n",
    "mic_df.index = np.sort(np.unique(encoded_mic))\n",
    "# mic_df.index =  np.array([0.0625, 0.125, 0.25, 0.5, 1, 2])\n",
    "# mic_df.index =  np.array(['<=0.03','<=0.06', '0.06', '0.12', '0.25', '0.5', '1', '2', '4', '8'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "mic_df.plot(kind='bar', stacked=True, ax=ax, color=[\"blue\", \"grey\"])\n",
    "\n",
    "for i, (correct, erroneous) in enumerate(zip(mic_df[\"Correct\"], mic_df[\"Erroneous\"])):\n",
    "    total = correct + erroneous\n",
    "    if total > 0:\n",
    "        percent_correct = correct / total * 100\n",
    "        ax.text(i, total + 8, f\"{percent_correct:.1f}%\", ha='center', fontsize=10)\n",
    "\n",
    "ax.set_xlabel(\"MIC Value\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(f\"Stacked Barplot of Correct vs. Erroneous MIC Predictions-{drug}\")\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=[handles[1], handles[0]], labels=[\"Erroneous\", \"Correct\"], title=\"Prediction\")\n",
    "plt.xticks(rotation=0)\n",
    "ax.axvline(x=cutoff, color='red', linestyle='--', linewidth=1.4)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_53392/2372630904.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmic\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_mic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmic_log2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdict_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmic_log2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_list' is not defined"
     ]
    }
   ],
   "source": [
    "mic= np.sort(np.unique(encoded_mic))\n",
    "mic_log2 = np.sort(np.unique(target_list))\n",
    "import json\n",
    "dict_ = {}\n",
    "for a, b in zip(mic, mic_log2):\n",
    "    dict_[str(int(b))] = str(a)\n",
    "    \n",
    "with open(f'targets_pred/mic_dic-{drug}.json', 'w') as f:\n",
    "    json.dump(dict_, f)\n",
    "\n",
    "print(dict_)\n",
    "\n",
    "# Define the file path\n",
    "file_path = f'targets_pred/targets_pred-{drug}.txt'\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path, 'w') as file:\n",
    "    # Iterate over the lists and write each pair to the file\n",
    "    for pred, target in zip(pred_list, target_list):\n",
    "        file.write(f\"{target} {pred}\\n\")\n",
    "\n",
    "print(f\"Data saved to {file_path}\")\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import plotly.graph_objects as go\n",
    "# target_list_ = df_sorted['target_list']\n",
    "# pred_list_ = df_sorted['pred_list_']\n",
    "# Create a list of transitions\n",
    "pred_list_ = [str(i) for i in pred_list]\n",
    "\n",
    "transitions = [(source, target) for source, target in zip(target_list, pred_list_)]\n",
    "\n",
    "# Count the occurrences of each transition\n",
    "transition_counts = Counter(transitions)\n",
    "\n",
    "# Extract unique source and target values\n",
    "unique_sources = sorted(set(target_list))\n",
    "unique_targets = sorted(set(pred_list_))\n",
    "\n",
    "# Create a mapping from value to index\n",
    "value_to_index = {value: index for index, value in enumerate(unique_sources + unique_targets)}\n",
    "\n",
    "# Prepare the source, target, and value lists for the Sankey diagram\n",
    "source_indices = [value_to_index[source] for source, target in transition_counts.keys()]\n",
    "target_indices = [value_to_index[target] for source, target in transition_counts.keys()]\n",
    "values = list(transition_counts.values())\n",
    "\n",
    "# Prepare the labels\n",
    "labels = [str(value) for value in unique_sources + unique_targets]\n",
    "labels = [x.replace('.0', '') for x in labels]\n",
    "labels = [dict_[label] for label in labels]\n",
    "\n",
    "# Determine the color for each link based on correct or erroneous prediction\n",
    "link_colors = []\n",
    "for (source, target) in transition_counts.keys():\n",
    "    if abs(float(source) - int(target)) <= 1:  # Check if the prediction is correct\n",
    "        link_colors.append('rgba(0, 255, 0, 0.6)')  # Green for correct predictions\n",
    "    else:\n",
    "        link_colors.append('rgba(255, 0, 0, 0.6)')  # Red for erroneous predictions\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=labels,\n",
    "        color=\"blue\"\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source_indices,  \n",
    "        target=target_indices,  \n",
    "        value=values,\n",
    "        color=link_colors\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram of Target to Prediction Transitions\", font_size=10, width=800)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min.values-1, target_max.values+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate AUC\n",
    "cutoff = cutoff\n",
    "test_target_bi = (target_list >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cnn without bin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_target[f'{drug}_MIC_y'] = test_target['tbp']\n",
    "test_target[f'{drug}_MIC_y'] = 0\n",
    "# train_target[f'{drug}_MIC_y'] = train_target['tbp']\n",
    "train_target[f'{drug}_MIC_y'] = 0\n",
    "# runninng with tbp empty predictions\n",
    "# test_target[f'{drug}_MIC_y'] = 0\n",
    "\n",
    "training_dataset = Dataset(train_data, train_target, f'{drug}_MIC_x',f'{drug}_MIC_y', one_hot_dtype=torch.float, transform=False)\n",
    "# train_dataset, val_dataset = random_split(training_dataset, [int(len(training_dataset)*0.9), len(training_dataset)-int(len(training_dataset)*0.9)])\n",
    "\n",
    "# test_data = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/snps_crypticTest_emb.npy')\n",
    "# train_data = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/drs_crypticTest_emb.npy')\n",
    "testing_dataset = Dataset(test_data, test_target, f'{drug}_MIC_x',f'{drug}_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "\n",
    "train_idx, validation_idx = train_test_split(np.arange(len(train_data)),\n",
    "                                             test_size=0.1,\n",
    "                                             random_state=42,\n",
    "                                             shuffle=True,\n",
    "                                             stratify=train_target)\n",
    "\n",
    "# Subset dataset for train and val\n",
    "train_dataset = Subset(training_dataset, train_idx)\n",
    "val_dataset = Subset(training_dataset, validation_idx)\n",
    "\n",
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-04 12:05:39,630] A new study created in memory with name: no-name-c859889f-5a10-4e43-b706-120ecc6ebd51\n",
      "[I 2025-12-04 12:10:06,222] Trial 0 finished with value: 0.26357716508209705 and parameters: {'lr': 5.6115164153345e-05, 'weight_decay': 0.0007114476009343421, 'conv_dropout_rate': 0.43919636508684307, 'dense_dropout_rate': 0.4789267873576293, 'num_filters': 47, 'num_conv_layers': 1, 'num_dense_neurons': 75, 'num_dense_layers': 3}. Best is trial 0 with value: 0.26357716508209705.\n",
      "[I 2025-12-04 12:14:08,399] Trial 1 finished with value: 0.26011549557248753 and parameters: {'lr': 0.00015930522616241006, 'weight_decay': 0.000133112160807369, 'conv_dropout_rate': 0.012350696577481468, 'dense_dropout_rate': 0.7759278817295955, 'num_filters': 112, 'num_conv_layers': 1, 'num_dense_neurons': 99, 'num_dense_layers': 1}. Best is trial 1 with value: 0.26011549557248753.\n",
      "[I 2025-12-04 12:18:33,678] Trial 2 finished with value: 0.25885121896862984 and parameters: {'lr': 4.059611610484306e-05, 'weight_decay': 3.752055855124284e-05, 'conv_dropout_rate': 0.25916701118526947, 'dense_dropout_rate': 0.23298331215843354, 'num_filters': 91, 'num_conv_layers': 1, 'num_dense_neurons': 120, 'num_dense_layers': 2}. Best is trial 2 with value: 0.25885121896862984.\n",
      "[I 2025-12-04 12:22:48,647] Trial 3 finished with value: 0.2577209500595927 and parameters: {'lr': 8.168455894760161e-05, 'weight_decay': 0.0002267398652378039, 'conv_dropout_rate': 0.11980426929501584, 'dense_dropout_rate': 0.4113875507308893, 'num_filters': 89, 'num_conv_layers': 1, 'num_dense_neurons': 181, 'num_dense_layers': 1}. Best is trial 3 with value: 0.2577209500595927.\n",
      "[I 2025-12-04 12:26:03,226] Trial 4 finished with value: 0.2694702371954918 and parameters: {'lr': 1.3492834268013232e-05, 'weight_decay': 0.0007025166339242157, 'conv_dropout_rate': 0.5793792198447356, 'dense_dropout_rate': 0.646717878493169, 'num_filters': 61, 'num_conv_layers': 1, 'num_dense_neurons': 196, 'num_dense_layers': 2}. Best is trial 3 with value: 0.2577209500595927.\n",
      "[I 2025-12-04 12:28:02,264] Trial 5 finished with value: 0.2689624906828006 and parameters: {'lr': 1.7541893487450798e-05, 'weight_decay': 3.058656666978529e-05, 'conv_dropout_rate': 0.020633112669131037, 'dense_dropout_rate': 0.7274563216630257, 'num_filters': 57, 'num_conv_layers': 2, 'num_dense_neurons': 124, 'num_dense_layers': 2}. Best is trial 3 with value: 0.2577209500595927.\n",
      "[I 2025-12-04 12:29:50,169] Trial 6 finished with value: 0.27285083569586277 and parameters: {'lr': 0.00012399967836846095, 'weight_decay': 3.5856126103453987e-06, 'conv_dropout_rate': 0.5817507766587351, 'dense_dropout_rate': 0.6201062586888917, 'num_filters': 123, 'num_conv_layers': 3, 'num_dense_neurons': 179, 'num_dense_layers': 3}. Best is trial 3 with value: 0.2577209500595927.\n",
      "[I 2025-12-04 12:34:26,896] Trial 7 finished with value: 0.25921395731468994 and parameters: {'lr': 1.5030900645056805e-05, 'weight_decay': 3.87211803217458e-06, 'conv_dropout_rate': 0.02713637334632284, 'dense_dropout_rate': 0.2602642646106115, 'num_filters': 69, 'num_conv_layers': 1, 'num_dense_neurons': 223, 'num_dense_layers': 2}. Best is trial 3 with value: 0.2577209500595927.\n",
      "[I 2025-12-04 12:39:58,209] Trial 8 finished with value: 0.26048967987298965 and parameters: {'lr': 3.6464395589807184e-05, 'weight_decay': 4.247058562261871e-05, 'conv_dropout_rate': 0.08455453498485759, 'dense_dropout_rate': 0.6417575846032317, 'num_filters': 39, 'num_conv_layers': 3, 'num_dense_neurons': 213, 'num_dense_layers': 1}. Best is trial 3 with value: 0.2577209500595927.\n",
      "[I 2025-12-04 12:44:41,978] Trial 9 finished with value: 0.2614885304744045 and parameters: {'lr': 1.0257563974185649e-05, 'weight_decay': 0.0002795015916508333, 'conv_dropout_rate': 0.4241144063085703, 'dense_dropout_rate': 0.5832057344327899, 'num_filters': 106, 'num_conv_layers': 1, 'num_dense_neurons': 133, 'num_dense_layers': 1}. Best is trial 3 with value: 0.2577209500595927.\n",
      "[I 2025-12-04 12:48:35,242] Trial 10 finished with value: 0.25774448706458014 and parameters: {'lr': 0.000698704649008624, 'weight_decay': 1.0422971466648463e-06, 'conv_dropout_rate': 0.18461652687015728, 'dense_dropout_rate': 0.014369500491855536, 'num_filters': 86, 'num_conv_layers': 2, 'num_dense_neurons': 249, 'num_dense_layers': 1}. Best is trial 3 with value: 0.2577209500595927.\n",
      "[I 2025-12-04 12:52:34,004] Trial 11 finished with value: 0.2576585713153084 and parameters: {'lr': 0.0006294532478500545, 'weight_decay': 1.047999311322588e-06, 'conv_dropout_rate': 0.1941720189778417, 'dense_dropout_rate': 0.013748920215166998, 'num_filters': 85, 'num_conv_layers': 2, 'num_dense_neurons': 254, 'num_dense_layers': 1}. Best is trial 11 with value: 0.2576585713153084.\n",
      "[I 2025-12-04 12:56:44,112] Trial 12 finished with value: 0.25723433556656045 and parameters: {'lr': 0.0003865043768360942, 'weight_decay': 1.0215234551846988e-05, 'conv_dropout_rate': 0.16019464376722162, 'dense_dropout_rate': 0.005253927096353793, 'num_filters': 95, 'num_conv_layers': 2, 'num_dense_neurons': 256, 'num_dense_layers': 1}. Best is trial 12 with value: 0.25723433556656045.\n",
      "[I 2025-12-04 13:00:25,942] Trial 13 finished with value: 0.25722003479798633 and parameters: {'lr': 0.0008723705388150743, 'weight_decay': 7.68600701244574e-06, 'conv_dropout_rate': 0.2533385119913028, 'dense_dropout_rate': 0.007844603383318747, 'num_filters': 76, 'num_conv_layers': 2, 'num_dense_neurons': 256, 'num_dense_layers': 1}. Best is trial 13 with value: 0.25722003479798633.\n",
      "[I 2025-12-04 13:03:23,081] Trial 14 finished with value: 0.25742184308667976 and parameters: {'lr': 0.000339899904095981, 'weight_decay': 9.811581235264329e-06, 'conv_dropout_rate': 0.2982512361665114, 'dense_dropout_rate': 0.13987026788542675, 'num_filters': 102, 'num_conv_layers': 2, 'num_dense_neurons': 230, 'num_dense_layers': 1}. Best is trial 13 with value: 0.25722003479798633.\n",
      "[I 2025-12-04 13:08:19,295] Trial 15 finished with value: 0.25765446728716296 and parameters: {'lr': 0.0003103487374222189, 'weight_decay': 1.170139079758927e-05, 'conv_dropout_rate': 0.407887687330878, 'dense_dropout_rate': 0.1345348330324592, 'num_filters': 71, 'num_conv_layers': 3, 'num_dense_neurons': 256, 'num_dense_layers': 2}. Best is trial 13 with value: 0.25722003479798633.\n",
      "[I 2025-12-04 13:12:37,910] Trial 16 finished with value: 0.2574535779034098 and parameters: {'lr': 0.00033702735211480774, 'weight_decay': 9.369002911019009e-06, 'conv_dropout_rate': 0.21902469376260056, 'dense_dropout_rate': 0.1339623894928902, 'num_filters': 128, 'num_conv_layers': 2, 'num_dense_neurons': 209, 'num_dense_layers': 1}. Best is trial 13 with value: 0.25722003479798633.\n",
      "[I 2025-12-04 13:16:34,260] Trial 17 finished with value: 0.2582717640325427 and parameters: {'lr': 0.0009598706275355609, 'weight_decay': 3.7124647849161366e-06, 'conv_dropout_rate': 0.33750771421520476, 'dense_dropout_rate': 0.30389682450725514, 'num_filters': 75, 'num_conv_layers': 2, 'num_dense_neurons': 160, 'num_dense_layers': 3}. Best is trial 13 with value: 0.25722003479798633.\n",
      "[I 2025-12-04 13:20:00,817] Trial 18 finished with value: 0.25757481064647436 and parameters: {'lr': 0.00021134453568162815, 'weight_decay': 1.944076280451722e-05, 'conv_dropout_rate': 0.12547900245920743, 'dense_dropout_rate': 0.0685224459290451, 'num_filters': 95, 'num_conv_layers': 2, 'num_dense_neurons': 236, 'num_dense_layers': 2}. Best is trial 13 with value: 0.25722003479798633.\n",
      "[I 2025-12-04 13:22:31,288] Trial 19 finished with value: 0.2584832984333237 and parameters: {'lr': 0.0005603553956608588, 'weight_decay': 7.34232270900806e-05, 'conv_dropout_rate': 0.34572222835911814, 'dense_dropout_rate': 0.3461964889443931, 'num_filters': 60, 'num_conv_layers': 3, 'num_dense_neurons': 168, 'num_dense_layers': 1}. Best is trial 13 with value: 0.25722003479798633.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best hyperparameters:\n",
      "{'lr': 0.0008723705388150743, 'weight_decay': 7.68600701244574e-06, 'conv_dropout_rate': 0.2533385119913028, 'dense_dropout_rate': 0.007844603383318747, 'num_filters': 76, 'num_conv_layers': 2, 'num_dense_neurons': 256, 'num_dense_layers': 1}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # ------------------------\n",
    "    # Hyperparameters to tune\n",
    "    # ------------------------\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    conv_dropout_rate = trial.suggest_float(\"conv_dropout_rate\", 0.0, 0.6)\n",
    "    dense_dropout_rate = trial.suggest_float(\"dense_dropout_rate\", 0.0, 0.8)\n",
    "    num_filters = trial.suggest_int(\"num_filters\", 32, 128)\n",
    "    num_conv_layers = trial.suggest_int(\"num_conv_layers\", 1, 3)\n",
    "    num_dense_neurons = trial.suggest_int(\"num_dense_neurons\", 64, 256)\n",
    "    num_dense_layers = trial.suggest_int(\"num_dense_layers\", 1, 3)\n",
    "\n",
    "    # ------------------------\n",
    "    # Model\n",
    "    # ------------------------\n",
    "    model = Model(\n",
    "        num_classes=len(np.unique(encoded_mic[f'{drug}_MIC'])),\n",
    "        num_filters=num_filters,\n",
    "        num_conv_layers=num_conv_layers,\n",
    "        num_dense_neurons=num_dense_neurons,\n",
    "        num_dense_layers=num_dense_layers,\n",
    "        conv_dropout_rate=conv_dropout_rate,\n",
    "        dense_dropout_rate=dense_dropout_rate,\n",
    "        return_logits=False\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = weighted_cross_entropy_loss_fn\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=8)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=8)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    patience = 8\n",
    "\n",
    "    # ------------------------\n",
    "    # Training\n",
    "    # ------------------------\n",
    "    for epoch in range(1, 30):   # small number of epochs for tuning\n",
    "        model.train()\n",
    "        for x_train, y_train, y_train_res in train_loader:\n",
    "            x_train = torch.squeeze(x_train, 0).float().to(device)\n",
    "            y_train = y_train.long().to(device)\n",
    "            y_train_res = y_train_res.float().to(device)\n",
    "\n",
    "            pred = model(x_train, y_train_res)\n",
    "            loss = criterion(pred, y_train)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # ------------------------\n",
    "        # Validation\n",
    "        # ------------------------\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val, y_val_res in val_loader:\n",
    "                x_val = torch.squeeze(x_val, 0).float().to(device)\n",
    "                y_val = y_val.long().to(device)\n",
    "                y_val_res = y_val_res.float().to(device)\n",
    "                pred = model(x_val, y_val_res)\n",
    "                loss = criterion(pred, y_val)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(val_losses)\n",
    "\n",
    "        # Early stopping inside Optuna\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# OPTUNA STUDY\n",
    "# ------------------------\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "print(study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 25/100 [02:57<08:51,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered\n",
      "Final best validation loss: 0.2574367970228195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters:\n",
    "best_params = study.best_params\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    num_classes=len(np.unique(encoded_mic[f'{drug}_MIC'])),\n",
    "    num_filters=best_params[\"num_filters\"],\n",
    "    num_conv_layers=best_params[\"num_conv_layers\"],\n",
    "    num_dense_neurons=best_params[\"num_dense_neurons\"],\n",
    "    num_dense_layers=best_params[\"num_dense_layers\"],\n",
    "    conv_dropout_rate=best_params[\"conv_dropout_rate\"],\n",
    "    dense_dropout_rate=best_params[\"dense_dropout_rate\"],\n",
    "    return_logits=False\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=best_params[\"lr\"],\n",
    "    weight_decay=best_params[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "criterion = weighted_cross_entropy_loss_fn\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=8)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in tqdm(range(1, 100 + 1)):\n",
    "    model.train()\n",
    "    for x_train, y_train, y_train_res in train_loader:\n",
    "        x_train = torch.squeeze(x_train, 0).float().to(device)\n",
    "        y_train = y_train.long().to(device)\n",
    "        y_train_res = y_train_res.float().to(device)\n",
    "\n",
    "        pred = model(x_train, y_train_res)\n",
    "        loss = criterion(pred, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # ------------------------\n",
    "    # Validation\n",
    "    # ------------------------\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val, y_val_res in val_loader:\n",
    "            x_val = torch.squeeze(x_val, 0).float().to(device)\n",
    "            y_val = y_val.long().to(device)\n",
    "            y_val_res = y_val_res.float().to(device)\n",
    "            pred = model(x_val, y_val_res)\n",
    "            loss = criterion(pred, y_val)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "\n",
    "        # SAVE MODEL HERE\n",
    "        torch.save(model.state_dict(),\n",
    "                   f\"saved_models/cnn_MIC_best_{drug}.pt\")\n",
    "\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "print(\"Final best validation loss:\", best_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "Evaluation Metrics\n",
      "======================\n",
      "Accuracy: 0.2715008431703204\n",
      "Macro-F1: 0.08744877081040069\n",
      "MAE: 0.9258010118043845\n",
      "Confusion matrix:\n",
      " [[  0   1  28   0   0   1   0]\n",
      " [  0   0  82   1   0   0   0]\n",
      " [  0   0 315   1   1   0   1]\n",
      " [  2   2 568   6   2   0   0]\n",
      " [  0   0 158   1   0   0   0]\n",
      " [  0   0   6   0   0   0   2]\n",
      " [  0   0   7   0   0   0   1]]\n",
      "AUC-OVR: 0.49394357649991555\n",
      "\n",
      "Class-specific AUC (OVR):\n",
      "Class 0: 0.47231833910034604\n",
      "Class 1: 0.5136538902664147\n",
      "Class 2: 0.5011792452830188\n",
      "Class 3: 0.5102694321156254\n",
      "Class 4: 0.5048042475795044\n",
      "Class 5: 0.38592954159592524\n",
      "Class 6: 0.5694503395585738\n",
      "Entropy: 1.9205514\n",
      "Doubling Dilution Accuracy: 0.8254637436762225\n",
      "Binary AUC: 0.5928952991452991\n",
      "Sensitivity: 0.1875\n",
      "Specificity: 0.9982905982905983\n",
      "\n",
      "======================\n",
      "95 percent Confidence Intervals\n",
      "======================\n",
      "Accuracy: mean=0.2714835581787521, CI=(0.26644182124789206, 0.2765598650927487)\n",
      "AUC-OVR: mean=0.4940059335727236, CI=(0.4672261935243535, 0.5224131981774177)\n",
      "Doubling Dilution: mean=0.8255130691399664, CI=(0.8204047217537943, 0.8313659359190556)\n",
      "Macro-F1: mean=0.08576939023453548, CI=(0.061678710187904114, 0.1297348442438314)\n",
      "Sensitivity: mean=0.1849375, CI=(0.0, 0.375)\n",
      "Specificity: mean=0.9982760683760684, CI=(0.9957264957264957, 1.0)\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, confusion_matrix,\n",
    "    roc_auc_score, classification_report, mean_absolute_error\n",
    ")\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load best model\n",
    "# ------------------------------------------------------------\n",
    "model.load_state_dict(torch.load(f\"saved_models/cnn_MIC_best_{drug}.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Get predictions on test set\n",
    "# ------------------------------------------------------------\n",
    "pred_list = []\n",
    "pred_prob_list = []\n",
    "target_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test, y_test_res in testing_loader1:\n",
    "        x_test = x_test.to(device).float()\n",
    "        y_test = y_test.to(device).long()\n",
    "        y_test_res = y_test_res.to(device).float()\n",
    "\n",
    "        logits = model(x_test, y_test_res)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        pred_list.append(torch.argmax(probs, dim=1).cpu().numpy()[0])\n",
    "        pred_prob_list.append(probs.cpu().numpy()[0])\n",
    "        target_list.append(y_test.cpu().numpy()[0])\n",
    "\n",
    "pred_list = np.array(pred_list)\n",
    "target_list = np.array(target_list)\n",
    "pred_prob_array = np.array(pred_prob_list)\n",
    "\n",
    "print(\"======================\")\n",
    "print(\"Evaluation Metrics\")\n",
    "print(\"======================\")\n",
    "\n",
    "# -----------------------------\n",
    "# Basic metrics\n",
    "# -----------------------------\n",
    "accuracy = accuracy_score(target_list, pred_list)\n",
    "macro_f1 = f1_score(target_list, pred_list, average='macro')\n",
    "mae = mean_absolute_error(target_list, pred_list)\n",
    "conf = confusion_matrix(target_list, pred_list)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Macro-F1:\", macro_f1)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"Confusion matrix:\\n\", conf)\n",
    "\n",
    "# -----------------------------\n",
    "# Global AUC (multiclass OVR)\n",
    "# -----------------------------\n",
    "auc_ovr = roc_auc_score(target_list, pred_prob_array, multi_class='ovr')\n",
    "print(\"AUC-OVR:\", auc_ovr)\n",
    "\n",
    "# -----------------------------\n",
    "# Class-specific AUC\n",
    "# -----------------------------\n",
    "print(\"\\nClass-specific AUC (OVR):\")\n",
    "unique_classes = np.unique(target_list)\n",
    "class_to_index = {label: i for i, label in enumerate(unique_classes)}\n",
    "\n",
    "class_specific_auc = {}\n",
    "\n",
    "for c in unique_classes:\n",
    "    y_true_bin = (target_list == c).astype(int)\n",
    "    y_score_bin = pred_prob_array[:, class_to_index[c]]\n",
    "\n",
    "    try:\n",
    "        auc_cls = roc_auc_score(y_true_bin, y_score_bin)\n",
    "    except:\n",
    "        auc_cls = np.nan\n",
    "\n",
    "    class_specific_auc[c] = auc_cls\n",
    "    print(f\"Class {c}: {auc_cls}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Entropy (model uncertainty)\n",
    "# -----------------------------\n",
    "entropy_values = np.mean(entropy(pred_prob_array.T))\n",
    "print(\"Entropy:\", entropy_values)\n",
    "\n",
    "# -----------------------------\n",
    "# Doubling dilution accuracy\n",
    "# -----------------------------\n",
    "def is_within_doubling_dilution(pred, true):\n",
    "    # pred and true are the MIC class indices (log2-scaled)\n",
    "    return abs(int(pred) - int(true)) <= 1\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([\n",
    "    is_within_doubling_dilution(p, t)\n",
    "    for p, t in zip(pred_list, target_list)\n",
    "])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "# -----------------------------\n",
    "# Binary metrics (R/S)\n",
    "# -----------------------------\n",
    "test_target_bi = (target_list >= cutoff).astype(int)\n",
    "test_pred_bi = (pred_list >= cutoff).astype(int)\n",
    "\n",
    "auc_binary = roc_auc_score(test_target_bi, test_pred_bi)\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_pred_bi).ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(\"Binary AUC:\", auc_binary)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                 BOOTSTRAP CONFIDENCE INTERVALS\n",
    "# ============================================================\n",
    "\n",
    "def stratified_bootstrap(y_true, y_pred, y_proba, metric_fn, n_boot=2000, alpha=0.05):\n",
    "    classes = np.unique(y_true)\n",
    "    samples = []\n",
    "\n",
    "    for _ in range(n_boot):\n",
    "        idx_list = []\n",
    "        for cls in classes:\n",
    "            cls_idx = np.where(y_true == cls)[0]\n",
    "            sampled = np.random.choice(cls_idx, size=len(cls_idx), replace=True)\n",
    "            idx_list.extend(sampled)\n",
    "\n",
    "        idx = np.array(idx_list)\n",
    "\n",
    "        m = metric_fn(y_true[idx], y_pred[idx], y_proba[idx])\n",
    "        if not np.isnan(m):\n",
    "            samples.append(m)\n",
    "\n",
    "    if len(samples) == 0:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "\n",
    "    samples = np.array(samples)\n",
    "    mean_val = np.mean(samples)\n",
    "    low = np.percentile(samples, 100 * (alpha / 2))\n",
    "    high = np.percentile(samples, 100 * (1 - alpha / 2))\n",
    "\n",
    "    return mean_val, low, high\n",
    "\n",
    "\n",
    "# Metric wrappers\n",
    "def metric_acc(y, p, s): return accuracy_score(y, p)\n",
    "def metric_auc(y, p, s): \n",
    "    try: return roc_auc_score(y, s, multi_class='ovr')\n",
    "    except: return np.nan\n",
    "\n",
    "def metric_dd(y, p, s):\n",
    "    return np.mean([abs(int(p_i) - int(y_i)) <= 1 for p_i, y_i in zip(p, y)])\n",
    "\n",
    "def metric_f1_macro(y, p, s): return f1_score(y, p, average='macro')\n",
    "\n",
    "def metric_sens(y, p, s):\n",
    "    yb = (y >= cutoff).astype(int)\n",
    "    pb = (p >= cutoff).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(yb, pb).ravel()\n",
    "    return tp / (tp + fn) if tp + fn > 0 else np.nan\n",
    "\n",
    "def metric_spec(y, p, s):\n",
    "    yb = (y >= cutoff).astype(int)\n",
    "    pb = (p >= cutoff).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(yb, pb).ravel()\n",
    "    return tn / (tn + fp) if tn + fp > 0 else np.nan\n",
    "\n",
    "\n",
    "print(\"\\n======================\")\n",
    "print(\"95 percent Confidence Intervals\")\n",
    "print(\"======================\")\n",
    "\n",
    "acc_mean, acc_low, acc_high = stratified_bootstrap(target_list, pred_list, pred_prob_array, metric_acc)\n",
    "auc_mean, auc_low, auc_high = stratified_bootstrap(target_list, pred_list, pred_prob_array, metric_auc)\n",
    "dd_mean, dd_low, dd_high = stratified_bootstrap(target_list, pred_list, pred_prob_array, metric_dd)\n",
    "f1_mean, f1_low, f1_high = stratified_bootstrap(target_list, pred_list, pred_prob_array, metric_f1_macro)\n",
    "sens_mean, sens_low, sens_high = stratified_bootstrap(target_list, pred_list, pred_prob_array, metric_sens)\n",
    "spec_mean, spec_low, spec_high = stratified_bootstrap(target_list, pred_list, pred_prob_array, metric_spec)\n",
    "\n",
    "print(f\"Accuracy: mean={acc_mean}, CI=({acc_low}, {acc_high})\")\n",
    "print(f\"AUC-OVR: mean={auc_mean}, CI=({auc_low}, {auc_high})\")\n",
    "print(f\"Doubling Dilution: mean={dd_mean}, CI=({dd_low}, {dd_high})\")\n",
    "print(f\"Macro-F1: mean={f1_mean}, CI=({f1_low}, {f1_high})\")\n",
    "print(f\"Sensitivity: mean={sens_mean}, CI=({sens_low}, {sens_high})\")\n",
    "print(f\"Specificity: mean={spec_mean}, CI=({spec_low}, {spec_high})\")\n",
    "print(\"======================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './saved_model1115/emb_resFeed_working17122024.pth'\n",
    "# torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved_model1115/resFeed1.pth'\n",
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "Optimizer details:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "Learning rate: 0.0001\n",
      "Weight decay: 0.0001\n",
      "======================\n",
      "Accuracy: 0.5224274406332454\n",
      "Mae: 0.503957783641161\n",
      "F1 Score: 0.48083488925470447\n",
      "conf_matrix: [[ 18  86   8   0   0   0]\n",
      " [ 48 330  19   0   0   0]\n",
      " [ 15 230  51   0   0   0]\n",
      " [  0   0   0 104  44   1]\n",
      " [  0   0   0  52  65  20]\n",
      " [  0   0   0   6  14  26]]\n",
      "======================\n",
      "Doubling Dilution Accuracy: 0.9736147757255936\n",
      "AUC: 0.8179451489844314\n",
      "Sensitivity: 0.6830601092896175\n",
      "Specificity: 0.9528301886792453\n"
     ]
    }
   ],
   "source": [
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "# Ensure the model is on the same device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test, y_test_res in testing_loader1:\n",
    "        # Move input and target data to the correct device\n",
    "        x_test = x_test.to(device).float()\n",
    "        y_test = y_test.to(device).float()\n",
    "        y_test_res = y_test_res.to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(x_test, y_test_res)\n",
    "        \n",
    "        # Append predictions and targets to lists\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "        \n",
    "# Flatten the target list\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "\n",
    "# Calculate AUC\n",
    "cutoff = cutoff\n",
    "test_target_bi = (target_list >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypterparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 50/400 [02:56<20:30,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Training loss: 0.1510869264602661\n",
      "Validation loss: 0.13646160066127777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 100/400 [05:52<17:28,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100\n",
      "Training loss: 0.12380748987197876\n",
      "Validation loss: 0.11622773110866547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 114/400 [06:41<16:51,  3.54s/it]"
     ]
    }
   ],
   "source": [
    "#input parameter\n",
    "for _ in [1e-4, 5e-4,1e-3, 5e-3,1e-2]:\n",
    "    lr = 1e-4\n",
    "    epoch = 400\n",
    "    conv_dropout_rate=0.4\n",
    "    dense_dropout_rate=0.7\n",
    "    weight_decay= _ #1e-4\n",
    "    ######################################\n",
    "\n",
    "    model = Model(\n",
    "    num_classes=6,\n",
    "    num_filters=64,\n",
    "    num_conv_layers=2,\n",
    "    # num_dense_neurons=256, # batch_size = 64\n",
    "    num_dense_neurons=128, # batch_size = 64\n",
    "    num_dense_layers=2,\n",
    "    return_logits=False,\n",
    "    conv_dropout_rate=conv_dropout_rate,\n",
    "    dense_dropout_rate=dense_dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "    # model = Model( #! way too memory intensive\n",
    "    # num_classes=13,\n",
    "    # num_filters=128,\n",
    "    # num_conv_layers=2,\n",
    "    # num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "    # num_dense_layers=2,\n",
    "    # return_logits=True,\n",
    "    # conv_dropout_rate=0,\n",
    "    # dense_dropout_rate=0\n",
    "    # ).to(device)\n",
    "    ## early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "    patience_counter = 0\n",
    "    lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "    batch_size = 64\n",
    "    # lr = 0.0085\n",
    "    # lr = 0.00002\n",
    "    lr = lr\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "    test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "    # train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "    # test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "    # criterion = nn.MSELoss()\n",
    "    # criterion = masked_weighted_MAE\n",
    "    # criterion = masked_weighted_MSE\n",
    "    criterion = weighted_cross_entropy_loss_fn\n",
    "    # criterion = masked_MAE\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "    # scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "    #%%\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc; gc.collect()\n",
    "    # ic.enable()\n",
    "    ic.disable()\n",
    "\n",
    "    train_epoch_loss = []\n",
    "    test_epoch_loss = []\n",
    "\n",
    "    for e in tqdm(range(1, epoch+1)):\n",
    "        model.train()\n",
    "        train_batch_loss = []\n",
    "        test_batch_loss = []\n",
    "        # print(f'Epoch {e}')\n",
    "        for x_train, y_train, y_train_res in train_loader:\n",
    "            x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "            y_batch = y_train.to(device)\n",
    "            y_batch_res = y_train_res.to(device)\n",
    "            \n",
    "            x_batch = x_batch.float()\n",
    "            pred = model(x_batch.float(),y_batch_res.float())\n",
    "\n",
    "            # break\n",
    "            # loss_train = loss_corn(pred, y_batch, 3, class_weights)\n",
    "            # print(pred, y_batch)\n",
    "            loss_train = criterion(pred,y_batch)\n",
    "            # print(pred)\n",
    "            # print(y_batch)\n",
    "            # print(loss_train)\n",
    "            train_batch_loss.append(loss_train)        \n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()  # Update the learning rate\n",
    "            # break\n",
    "        train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # print('>> test')\n",
    "            for x_test, y_test, y_test_res in test_loader:\n",
    "                x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "                x_batch = x_batch.float()\n",
    "                y_batch = y_test.to(device)\n",
    "                y_batch_res = y_test_res.to(device)\n",
    "                # print(x_batch.size())\n",
    "                # y_batch = torch.Tensor.float(y).to(device)\n",
    "                # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "                pred = model(x_batch.float(), y_batch_res.float())\n",
    "                loss_test = criterion(pred,y_batch)\n",
    "                # pred = pred.unsqueeze(0)\n",
    "                # print(pred[:10])\n",
    "                # print(y_batch[:10])\n",
    "\n",
    "                # loss_test = loss_corn(pred, y_batch, 3, class_weights)\n",
    "                test_batch_loss.append(loss_test)\n",
    "            test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "        if e%50 == 0:\n",
    "            print(f'Epoch {e}')\n",
    "            print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "            print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "        # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "        # print(train_batch_loss)\n",
    "        # print(test_batch_loss)\n",
    "        # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "        # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "        # #! implementing early stopping\n",
    "        # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "        # print(f'Current val loss: {current_val_loss}')\n",
    "        # print(f'Best val loss: {best_val_loss}')\n",
    "        # if current_val_loss < best_val_loss:\n",
    "        #     best_val_loss = current_val_loss\n",
    "        #     patience_counter = 0  # reset patience counter\n",
    "        #     # Save the best model\n",
    "        #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     if patience_counter >= patience:\n",
    "        #         print(\"Early stopping triggered\")\n",
    "        #         torch.save({\n",
    "        #         'optimizer': optimizer.state_dict(),\n",
    "        #         'model': model.state_dict(),\n",
    "        #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "        #         break  # Early stopping\n",
    "            \n",
    "    print('==='*10)\n",
    "    # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "    save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "                train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    x = np.arange(1, epoch+1, 1)\n",
    "    ax.plot(x, train_epoch_loss,label='Training')\n",
    "    ax.plot(x, test_epoch_loss,label='Validation')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Number of Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "    ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "    # ax_2 = ax.twinx()\n",
    "    # ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "    # ax_2.set_yscale(\"log\")\n",
    "    # ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "    ax.grid(axis=\"x\")\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "    print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "    #%%\n",
    "    # testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "    testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Ensure the model is on the same device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    ic.disable()\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    target_list  = []\n",
    "    mse_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test, y_test_res in testing_loader1:\n",
    "            # Move input and target data to the correct device\n",
    "            x_test = x_test.to(device).float()\n",
    "            y_test = y_test.to(device).float()\n",
    "            y_test_res = y_test_res.to(device).float()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(x_test, y_test_res)\n",
    "            \n",
    "            # Append predictions and targets to lists\n",
    "            pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "            target_list.append(y_test.detach().cpu().numpy())\n",
    "            \n",
    "    # Flatten the target list\n",
    "    target_list = np.array(target_list).flatten()\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "    from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "    def calculate_metrics(true_labels, predictions):\n",
    "        \"\"\"\n",
    "        Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "        Parameters:\n",
    "        - true_labels: List or array of true labels\n",
    "        - predictions: List or array of predicted labels\n",
    "\n",
    "        Returns:\n",
    "        - accuracy: Overall accuracy of predictions\n",
    "        - f1: Weighted average F1 score\n",
    "        - conf_matrix: Multiclass confusion matrix\n",
    "        - mae: Mean Absolute Error of predictions\n",
    "        \"\"\"\n",
    "        # Ensure inputs are numpy arrays for consistency\n",
    "        true_labels = np.array(true_labels)\n",
    "        predictions = np.array(predictions)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "        # Calculate MAE\n",
    "        mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "        return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "    # Example usage\n",
    "    # true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "    # predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "    accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "    print(\"======================\")\n",
    "    # print(\"Model's Named Parameters:\")\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     print(f\"Name: {name}\")\n",
    "    #     print(f\"Shape: {param.size()}\")\n",
    "    #     print(f\"Requires grad: {param.requires_grad}\")\n",
    "    #     print('-----')\n",
    "    print(\"Optimizer details:\")\n",
    "    print(optimizer)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"Learning rate:\", param_group['lr'])\n",
    "        print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "        \n",
    "    print(\"======================\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Mae: {mae}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"conf_matrix: {conf_matrix}\")\n",
    "    print(\"======================\")\n",
    "    doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "    print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "\n",
    "    # Calculate AUC\n",
    "    cutoff = cutoff\n",
    "    test_target_bi = (target_list >= cutoff).astype(int)\n",
    "    test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "    auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "    print(\"AUC:\", auc)\n",
    "\n",
    "    # Calculate confusion matrix components\n",
    "    tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "    # Calculate sensitivity (recall)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "    # Calculate specificity\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One cycle lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 50/400 [02:28<17:21,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Training loss: 0.11537401378154755\n",
      "Validation loss: 0.14441072940826416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 100/400 [04:58<14:56,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100\n",
      "Training loss: 0.11210744082927704\n",
      "Validation loss: 0.1536770612001419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 150/400 [07:28<12:28,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150\n",
      "Training loss: 0.11056072264909744\n",
      "Validation loss: 0.16507422924041748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 200/400 [09:57<09:53,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200\n",
      "Training loss: 0.1097438856959343\n",
      "Validation loss: 0.17116251587867737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 250/400 [12:32<07:39,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250\n",
      "Training loss: 0.10918270796537399\n",
      "Validation loss: 0.1748015433549881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 300/400 [15:02<04:53,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300\n",
      "Training loss: 0.10868095606565475\n",
      "Validation loss: 0.17704497277736664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 350/400 [17:41<02:47,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350\n",
      "Training loss: 0.10827459394931793\n",
      "Validation loss: 0.18684180080890656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 400/400 [20:27<00:00,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400\n",
      "Training loss: 0.10818441957235336\n",
      "Validation loss: 0.17728930711746216\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_1e-07_weighted_balanced.png-emb\n",
      "======================\n",
      "Optimizer details:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    base_momentum: 0.85\n",
      "    betas: (0.9499999993756804, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    initial_lr: 0.0004\n",
      "    lr: 4.006243171376002e-08\n",
      "    max_lr: 0.01\n",
      "    max_momentum: 0.95\n",
      "    maximize: False\n",
      "    min_lr: 4e-08\n",
      "    weight_decay: 0\n",
      ")\n",
      "Learning rate: 4.006243171376002e-08\n",
      "Weight decay: 0\n",
      "======================\n",
      "Accuracy: 0.47568523430592397\n",
      "Mae: 0.6578249336870027\n",
      "F1 Score: 0.44584107920478183\n",
      "conf_matrix: [[ 34  66   7   3   1   1]\n",
      " [ 63 285  32   9   5   0]\n",
      " [ 40 188  41  19   3   2]\n",
      " [  4  20   7  75  42   1]\n",
      " [  2   8   0  34  77  16]\n",
      " [  0   3   0   5  12  26]]\n",
      "======================\n",
      "Doubling Dilution Accuracy: 0.8992042440318302\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#input parameter\n",
    "lr = 1e-7\n",
    "epoch = 400\n",
    "conv_dropout_rate=0\n",
    "dense_dropout_rate=0\n",
    "# weight_decay=1e-8\n",
    "weight_decay=0\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "patience_counter = 0\n",
    "lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "batch_size = 128\n",
    "# lr = 0.0085\n",
    "# lr = 0.00002\n",
    "lr = lr\n",
    "######################################\n",
    "\n",
    "model = Model(\n",
    "num_classes=6,\n",
    "num_filters=64,\n",
    "num_conv_layers=2,\n",
    "# num_dense_neurons=256, # batch_size = 64\n",
    "num_dense_neurons=128, # batch_size = 64\n",
    "num_dense_layers=2,\n",
    "return_logits=False,\n",
    "conv_dropout_rate=conv_dropout_rate,\n",
    "dense_dropout_rate=dense_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "criterion = weighted_cross_entropy_loss_fn\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=epoch)\n",
    "# model = Model( #! way too memory intensive\n",
    "# num_classes=13,\n",
    "# num_filters=128,\n",
    "# num_conv_layers=2,\n",
    "# num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "# num_dense_layers=2,\n",
    "# return_logits=True,\n",
    "# conv_dropout_rate=0,\n",
    "# dense_dropout_rate=0\n",
    "# ).to(device)\n",
    "## early stopping\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "# test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = masked_weighted_MAE\n",
    "# criterion = masked_weighted_MSE\n",
    "# criterion = weighted_cross_entropy_loss_fn\n",
    "\n",
    "\n",
    "# criterion = masked_MAE\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "#%%\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "# ic.enable()\n",
    "ic.disable()\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    # print(f'Epoch {e}')\n",
    "    for x_train, y_train in train_loader:\n",
    "        x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "        y_batch = y_train.to(device)\n",
    "        x_batch = x_batch.float()\n",
    "        pred = model(x_batch.float())\n",
    "\n",
    "        # break\n",
    "        loss_train = criterion(pred,y_batch)\n",
    "\n",
    "        train_batch_loss.append(loss_train)        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update the learning rate\n",
    "\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # print('>> test')\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            y_batch = y_test.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float())\n",
    "\n",
    "            # pred = pred.unsqueeze(0)\n",
    "            # print(pred[:10])\n",
    "            # print(y_batch[:10])\n",
    "\n",
    "            loss_test = criterion(pred,y_batch)\n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "    if e % 50 == 0:\n",
    "        print(f'Epoch {e}')\n",
    "        print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "        print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    # #! implementing early stopping\n",
    "    # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "    # print(f'Current val loss: {current_val_loss}')\n",
    "    # print(f'Best val loss: {best_val_loss}')\n",
    "    # if current_val_loss < best_val_loss:\n",
    "    #     best_val_loss = current_val_loss\n",
    "    #     patience_counter = 0  # reset patience counter\n",
    "    #     # Save the best model\n",
    "    #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         torch.save({\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #         'model': model.state_dict(),\n",
    "    #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "    #         break  # Early stopping\n",
    "        \n",
    "print('==='*10)\n",
    "# torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "             train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, epoch+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "#%%\n",
    "\n",
    "model.eval()  # For inference\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in testing_loader1:\n",
    "        xtest1 = x_test.to(device).float()\n",
    "        ytest1 = y_test.to(device).float()\n",
    "        pred = model(xtest1)\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclical lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input parameter\n",
    "lr = 1e-7\n",
    "epoch = 250\n",
    "conv_dropout_rate=0.05\n",
    "dense_dropout_rate=0.5\n",
    "weight_decay=1e-8\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-9, max_lr=0.001)\n",
    "######################################\n",
    "\n",
    "model = Model(\n",
    "num_classes=3,\n",
    "num_filters=64,\n",
    "num_conv_layers=2,\n",
    "# num_dense_neurons=256, # batch_size = 64\n",
    "num_dense_neurons=128, # batch_size = 64\n",
    "num_dense_layers=2,\n",
    "return_logits=False,\n",
    "conv_dropout_rate=conv_dropout_rate,\n",
    "dense_dropout_rate=dense_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# model = Model( #! way too memory intensive\n",
    "# num_classes=13,\n",
    "# num_filters=128,\n",
    "# num_conv_layers=2,\n",
    "# num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "# num_dense_layers=2,\n",
    "# return_logits=True,\n",
    "# conv_dropout_rate=0,\n",
    "# dense_dropout_rate=0\n",
    "# ).to(device)\n",
    "## early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "patience_counter = 0\n",
    "lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "batch_size = 128\n",
    "# lr = 0.0085\n",
    "# lr = 0.00002\n",
    "lr = lr\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "# test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = masked_weighted_MAE\n",
    "# criterion = masked_weighted_MSE\n",
    "# criterion = weighted_cross_entropy_loss_fn\n",
    "# criterion = masked_MAE\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "# scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "#%%\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "# ic.enable()\n",
    "ic.disable()\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    # print(f'Epoch {e}')\n",
    "    for x_train, y_train in train_loader:\n",
    "        x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "        y_batch = y_train.to(device)\n",
    "        x_batch = x_batch.float()\n",
    "        pred = model(x_batch.float())\n",
    "\n",
    "        # break\n",
    "        loss_train = criterion(pred,y_batch)\n",
    "\n",
    "        train_batch_loss.append(loss_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update the learning rate\n",
    "\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # print('>> test')\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            y_batch = y_test.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float())\n",
    "            loss_test = loss_corn(pred, y_batch, 3, class_weights)\n",
    "\n",
    "            # pred = pred.unsqueeze(0)\n",
    "            # print(pred[:10])\n",
    "            # print(y_batch[:10])\n",
    "\n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "\n",
    "    print(f'Epoch {e}')\n",
    "    print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "    print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    # #! implementing early stopping\n",
    "    # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "    # print(f'Current val loss: {current_val_loss}')\n",
    "    # print(f'Best val loss: {best_val_loss}')\n",
    "    # if current_val_loss < best_val_loss:\n",
    "    #     best_val_loss = current_val_loss\n",
    "    #     patience_counter = 0  # reset patience counter\n",
    "    #     # Save the best model\n",
    "    #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         torch.save({\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #         'model': model.state_dict(),\n",
    "    #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "    #         break  # Early stopping\n",
    "        \n",
    "print('==='*10)\n",
    "# torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "             train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, epoch+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "# ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "#%%\n",
    "testing_dataset = Dataset(test_data, test_target, one_hot_dtype=torch.float, transform=False)\n",
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, collate_fn=collate_padded_batch, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "model.eval()  # For inference\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in testing_loader1:\n",
    "        xtest1 = x_test.to(device).float()\n",
    "        ytest1 = y_test.to(device).float()\n",
    "        pred = model(xtest1)\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_corn(logits, y_train, num_classes):\n",
    "    sets = []\n",
    "    for i in range(num_classes-1):\n",
    "        label_mask = y_train > i-1\n",
    "        label_tensor = (y_train[label_mask] > i).to(torch.int64)\n",
    "        sets.append((label_mask, label_tensor))\n",
    "\n",
    "    num_examples = 0\n",
    "    losses = 0.\n",
    "    for task_index, s in enumerate(sets):\n",
    "        train_examples = s[0]\n",
    "        train_labels = s[1]\n",
    "\n",
    "        if len(train_labels) < 1:\n",
    "            continue\n",
    "\n",
    "        num_examples += len(train_labels)\n",
    "        pred = logits[train_examples, task_index]\n",
    "\n",
    "        loss = -torch.sum(F.logsigmoid(pred)*train_labels\n",
    "                          + (F.logsigmoid(pred) - pred)*(1-train_labels)\n",
    "                          )\n",
    "        losses += loss\n",
    "    return losses/num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred  = torch.tensor([[0.0000, 0.5111],\n",
    "        [0.1329, 1.1051]], device='cuda:0')\n",
    "target = torch.tensor([0, 0], device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7275, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "out = loss_corn(pred, target, 3)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost with snps and fed in res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "#     # Ensure target_min and target_max are scalars\n",
    "#     target_min = target_min.item() if isinstance(target_min, np.ndarray) or isinstance(target_min, pd.Series) else target_min\n",
    "#     target_max = target_max.item() if isinstance(target_max, np.ndarray) or isinstance(target_max, pd.Series) else target_max\n",
    "\n",
    "#     # Create a range based on the scalar values of target_min and target_max\n",
    "#     dilution_range = np.arange(target_min - 1, target_max + 2, 1)\n",
    "    \n",
    "#     # Find the index of the target value\n",
    "#     index = np.where(dilution_range == target)[0][0]  # Use np.where to find the index\n",
    "    \n",
    "#     # Check if prediction is within the acceptable range\n",
    "#     return dilution_range[index - 1] <= pred <= dilution_range[index + 1]\n",
    "\n",
    "# Example usage\n",
    "target_min, target_max = cryptic_drs.min().values, cryptic_drs.max()\n",
    "\n",
    "# Load the data\n",
    "cryptic_drs = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_sample_drs_cryptic_emb.npy')\n",
    "cryptic_snps = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_snps_cryptic_emb.npy')\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "cryptic_drs = pd.DataFrame(cryptic_drs)\n",
    "\n",
    "# Combine the features and target variable\n",
    "data = cryptic_snps\n",
    "target = cryptic_drs\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the XGBoost model\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "target_min, target_max = cryptic_drs.min().values, cryptic_drs.max().values\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(y_pred, y_test)])\n",
    "print(f\"Doubling Dilution Accuracy: {doubling_dilution_accuracy * 100:.2f}%\")\n",
    "\n",
    "#testing\n",
    "cutoff = cutoff\n",
    "test_target_bi = (np.squeeze(np.array(y_test)) >= cutoff).astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.squeeze(np.array(y_pred)) >= cutoff).astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
