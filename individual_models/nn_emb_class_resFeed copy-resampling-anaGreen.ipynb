{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/nn_model_script_emb_test.py - starting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fca98033f30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "print('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/nn_model_script_emb_test.py - starting')\n",
    "\n",
    "from array import array\n",
    "from cmath import nan\n",
    "from pyexpat import model\n",
    "import statistics\n",
    "from tkinter.ttk import Separator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torchviz import make_dot\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import variable\n",
    "from itertools import chain\n",
    "from sklearn import metrics as met\n",
    "import pickle\n",
    "from icecream import ic\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from importlib import reload\n",
    "# import util\n",
    "# import model_torch_simple\n",
    "# from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#%%\n",
    "seed = 42\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# train_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/aa_data_train_gene.csv', delimiter = ',')\n",
    "# train_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/mic_aa_train_hml.csv')\n",
    "# train_target = train_target[['EMB_MIC']]\n",
    "# # don't touch test data, split out validation data from training data during training\n",
    "# # test_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_EMB/aa_data_test_pca4k.csv', delimiter = ',')\n",
    "# test_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/aa_data_test_gene.csv', delimiter = ',')\n",
    "# test_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/mic_aa_test_hml.csv')\n",
    "# test_target = test_target[['EMB_MIC']]\n",
    "\n",
    "# all_data = np.concatenate((train_data, test_data), axis=0)\n",
    "# all_target = pd.concat((train_target, test_target), axis=0)\n",
    "\n",
    "# train_data, test_data, train_target, test_target = train_test_split(all_data, all_target, test_size=0.2, random_state=42, stratify=all_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def data_split(aa_array, encoded_mic):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Perform stratified train-test split\n",
    "    train_data, test_data, train_target, test_target = train_test_split(\n",
    "        aa_array,\n",
    "        encoded_mic,\n",
    "        test_size=0.1,  # 10% for testing\n",
    "        stratify=encoded_mic.iloc[:,0],  # Ensures the proportion of each class is preserved\n",
    "        random_state=42  # For reproducibility\n",
    "    )\n",
    "    return train_data, test_data, train_target, test_target\n",
    "\n",
    "# def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "#     _ = np.arange(target_min-1, target_max+2, 1)\n",
    "#     index = [i for i, x in enumerate(_) if x == target][0]\n",
    "#     return (_[index-1] <= pred <= _[index+1])\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min.values-1, target_max.values+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "# def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "#     # Convert Series to scalar if needed\n",
    "#     if isinstance(pred, pd.Series):\n",
    "#         pred = pred.iloc[0]  # or pred.item()\n",
    "#     if isinstance(target, pd.Series):\n",
    "#         target = target.iloc[0]  # or target.item()\n",
    "\n",
    "#     _ = np.arange(target_min - 1, target_max + 2, 1)\n",
    "#     # Now target is guaranteed to be a scalar\n",
    "#     index = [i for i, x in enumerate(_) if x == target][0]\n",
    "#     return (_[index - 1] <= pred <= _[index + 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_(cryptic, gene_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    \n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    variants = variants[variants['type'] != 'non_coding_transcript_exon_variant']\n",
    "\n",
    "    overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    # print(variants.shape)\n",
    "    # print(variants['sample_id'].unique().shape)\n",
    "\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        \n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "\n",
    "    aa = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table \n",
    "    print('Getting all snp data', len(all_snp))\n",
    "    for x in tqdm(overlap):\n",
    "    # for x in tqdm(variants['sample_id'].unique()):\n",
    "        if x in variants['sample_id'].tolist():\n",
    "            aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "        else:\n",
    "            aa.append([0]*len(all_snp))\n",
    "        # print('SNP')\n",
    "        \n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "\n",
    "    mic_aa = cryptic[cryptic['ENA_RUN'].isin(overlap)]#.iloc[:,14:27]\n",
    "    # mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # print(mic_aa.shape)\n",
    "    # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(overlap)\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    mic_aa = mic_aa.sort_values([\"ENA_RUN\"])  ## 'sort' changed to 'sort_values'\n",
    "    # print(mic_aa.shape)\n",
    "\n",
    "    return aa_array, mic_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug = 'EMB'\n",
    "df = pd.read_csv('../CRyPTIC_reuse_table_20231208.csv')\n",
    "\n",
    "aa_array = np.load(f'./generated_data18122024/all_sample_snps_cryptic_{drug}.npy')\n",
    "drs = np.load(f'./generated_data18122024/all_sample_drs_cryptic_{drug}.npy')\n",
    "\n",
    "encoded_mic = pd.DataFrame(drs, columns=['EMB_MIC'])\n",
    "mic_series = np.log2(encoded_mic)\n",
    "mic_series += 1\n",
    "# sample_ids = mic_aa['ENA_RUN']\n",
    "\n",
    "mic_series_bi = encoded_mic['EMB_MIC'].apply(lambda x: 1 if x >= 8 else 0)\n",
    "mic_series_all = pd.merge(mic_series, mic_series_bi, left_index=True, right_index=True)\n",
    "# train_data, test_data,  test_target_y, test_target = data_split(aa_array, mic_series_all)\n",
    "train_data, test_data, train_target, test_target = data_split(aa_array, mic_series_all)\n",
    "\n",
    "target_min, target_max = mic_series.min(), mic_series.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = np.load('./train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(train_data, train_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1.0: 3571, 2.0: 2661, 3.0: 2000, 4.0: 2000, 0.0: 2000, 5.0: 2000})\n",
      "(14232,)\n",
      "(14232, 1710)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Assuming train_data is your feature array and train_target['EMB_MIC_x'] is your target array\n",
    "X = train_data\n",
    "y = train_target['EMB_MIC_x']\n",
    "\n",
    "target_counts = {\n",
    "    1.0: 3571,  # Keep the majority class as is\n",
    "    2.0: 2661,  # Bring other classes closer\n",
    "    3.0: 2000,\n",
    "    4.0: 2000,\n",
    "    0.0: 2000,\n",
    "    5.0: 2000\n",
    "}\n",
    "# Initialize the RandomOverSampler\n",
    "ros= RandomOverSampler(sampling_strategy=target_counts, random_state=42)\n",
    "# ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Fit and resample the data\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Verify the new class distribution\n",
    "from collections import Counter\n",
    "print(Counter(y_resampled))\n",
    "print(y_resampled.shape)\n",
    "print(X_resampled.shape)\n",
    "\n",
    "# train_mic_series_bi = y_resampled.apply(lambda x: 1 if x >= 4 else 0)\n",
    "# train_target = pd.DataFrame({'EMB_MIC_x': y_resampled, 'EMB_MIC_y': train_mic_series_bi})\n",
    "# train_data = X_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mic_series_bi = y_resampled.apply(lambda x: 1 if x >= 4 else 0)\n",
    "train_target = pd.DataFrame({'EMB_MIC_x': y_resampled, 'EMB_MIC_y': train_mic_series_bi})\n",
    "train_data = X_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14232, 1710)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cornloss weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_counts = torch.from_numpy(train_target.values).flatten()\n",
    "# train_target_counts = torch.tensor([0,1,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Read the CSV file into a NumPy array\n",
    "# aa_array = np.loadtxt('aa_array_50k.csv', delimiter=',').astype(float)  # Ensure all elements are floats\n",
    "# # print(aa_array)\n",
    "\n",
    "\n",
    "# # Read the text file line by line into a list and convert to floats\n",
    "# with open('dr_target_50k.txt', 'r') as f:\n",
    "#     mic_series = [float(line.strip()) for line in f]  # Convert each line to float\n",
    "\n",
    "# # print(mic_series)\n",
    "# mic_series_50k = mic_series\n",
    "\n",
    "# aa_array_50k_pos = []\n",
    "# mic_series_50k_pos = []\n",
    "# for x, a in zip(mic_series_50k, aa_array):\n",
    "#     if x == 1:\n",
    "#         aa_array_50k_pos.append(a)\n",
    "#         mic_series_50k_pos.append(x)\n",
    "# aa_array_50k_pos = np.array(aa_array_50k_pos)\n",
    "import numpy as np\n",
    "\n",
    "drug = 'EMB'\n",
    "df = pd.read_csv('../CRyPTIC_reuse_table_20231208.csv')\n",
    "\n",
    "aa_array = np.load(f'./generated_data18122024/all_sample_snps_50k_{drug}_checks.npy')\n",
    "drs = np.load(f'./generated_data18122024/all_sample_drs_50k_{drug}_checks.npy')\n",
    "aa_array_50k_pos = aa_array\n",
    "mic_series_50k_pos = drs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgb training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8504837291116975\n",
      "AUC: 0.8888201532804068\n",
      "[[794 160]\n",
      " [ 10 173]]\n",
      "Sensitivity: 0.9453551912568307\n",
      "Specificity: 0.8322851153039832\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# train_data = np.column_stack((train_data, train_target['EMB_MIC_y'].values))\n",
    "# test_data = np.column_stack((test_data, test_target['EMB_MIC_y'].values))\n",
    "train_target_y = train_target['EMB_MIC_y'].values\n",
    "test_target_y = test_target['EMB_MIC_y'].values\n",
    "\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_data, test_data, train_target_y, test_target_y\n",
    "X_test = test_data\n",
    "y_test = test_target['EMB_MIC_y'].values\n",
    "X_train = np.concatenate((X_train, aa_array_50k_pos), axis=0)\n",
    "y_train = np.concatenate((y_train, mic_series_50k_pos), axis=0)\n",
    "# # Create the XGBoost model\n",
    "# model_bi = xgb.XGBClassifier(    \n",
    "#     max_depth=3,\n",
    "#     learning_rate=0.9,\n",
    "#     n_estimators=4,\n",
    "#     # gamma=0.1,\n",
    "#     # min_child_weight=24,\n",
    "#     # subsample=0.2,\n",
    "#     # colsample_bytree=1,\n",
    "#     # reg_alpha=15, reg_lambda=15,\n",
    "#     random_state=42 \n",
    "#     )\n",
    "# # Create the XGBoost model\n",
    "# model_bi = xgb.XGBClassifier(colsample_bytree= 0.5, gamma= 0.1, learning_rate= 0.01, max_depth= 4, min_child_weight= 1, n_estimators= 200, subsample= 0.8,random_state=42)\n",
    "model_bi = xgb.XGBClassifier(colsample_bytree= 0.5, gamma= 5, learning_rate= 0.3, max_depth= 7, min_child_weight= 1, n_estimators= 200, subsample= 0.8,random_state=42)\n",
    "\n",
    "model_bi.fit(X_train, y_train)\n",
    "\n",
    "# model_bi = pickle.load(open(\"xgb_bi_mix1.pkl\", \"rb\"))\n",
    "# model_bi = pickle.load(open(\"xgb_bi_mix.pkl\", \"rb\"))\n",
    "# model_bi = pickle.load(open(\"xgb_bi_50kbalanced.pkl\", \"rb\"))\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model_bi.predict(X_test)\n",
    "\n",
    "# Evaluate the model_bi\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "#testing\n",
    "cutoff = 4\n",
    "test_target_bi = y_test.astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "test_predictions_bi = y_pred.astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "print(confusion_matrix(test_target_bi, test_predictions_bi))\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)  \n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target['EMB_MIC_y'] = test_predictions_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14232"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from collections import Counter\n",
    "\n",
    "N_samples = train_data.shape[0]\n",
    "DRUGS = train_target.columns\n",
    "# LOCI = train_data.columns\n",
    "assert set(DRUGS) == set(train_target.columns)\n",
    "N_drugs = len(DRUGS)\n",
    "#%%\n",
    "\n",
    "def my_padding(seq_tuple):\n",
    "    list_x_ = list(seq_tuple)\n",
    "    max_len = len(max(list_x_, key=len))\n",
    "    for i, x in enumerate(list_x_):\n",
    "        list_x_[i] = x + \"N\"*(max_len-len(x))\n",
    "    return list_x_\n",
    "\n",
    "#! faster than my_padding try to incorporate\n",
    "def collate_padded_batch(batch):\n",
    "    # get max length of seqs in batch\n",
    "    max_len = max([x[0].shape[1] for x in batch])\n",
    "    return torch.utils.data.default_collate(\n",
    "        [(F.pad(x[0], (0, max_len - x[0].shape[1])), x[1]) for x in batch] #how does F.pad work\n",
    "    )\n",
    "\n",
    "# Julian's code - implement this, might be faster\n",
    "class Dataset(torch.utils.data.Dataset): #? what's the difference between using inheritance and not?\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_df,\n",
    "        res_df,\n",
    "        # target_loci=LOCI,\n",
    "        target_mic,\n",
    "        target_res,\n",
    "        one_hot_dtype=torch.int8,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        # self.seq_df = seq_df[target_loci]\n",
    "        self.seq_df = seq_df\n",
    "        self.res_df = res_df[target_res]\n",
    "        self.mic_df = res_df[target_mic]\n",
    "        # if not self.seq_df.index.equals(self.res_df.index):\n",
    "        #     raise ValueError(\n",
    "        #         \"Indices of sequence and resistance dataframes don't match up\"\n",
    "        #     )\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        numerical index --> get `index`-th sample\n",
    "        string index --> get sample with name `index`\n",
    "        \"\"\"\n",
    "        index = int(index)\n",
    "        if isinstance(index, int):\n",
    "            seqs_comb = self.seq_df[index]\n",
    "            res = self.res_df.iloc[index]\n",
    "            mic = self.mic_df.iloc[index]\n",
    "        elif isinstance(index, str):\n",
    "            seqs_comb = self.seq_df[int(index)]\n",
    "            res = self.res_df.loc[index]\n",
    "            mic = self.mic_df.loc[index]\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Index needs to be an integer or a sample name present in the dataset\"\n",
    "            )\n",
    "\n",
    "        if self.transform:\n",
    "            res = np.log(res)\n",
    "            \n",
    "            # self.res_mean = self.res_df.mean()\n",
    "            # self.res_std = self.res_df.std()\n",
    "            # res = (res - self.res_mean) / self.res_std\n",
    "            # res = self.transform(res)\n",
    "        return torch.unsqueeze(torch.tensor(seqs_comb).float(), 0), torch.tensor(mic).long().flatten().squeeze(), torch.tensor(res).long().flatten().squeeze()\n",
    "    def __len__(self):\n",
    "        return self.res_df.shape[0]\n",
    "\n",
    "training_dataset = Dataset(train_data, train_target, 'EMB_MIC_x','EMB_MIC_y', one_hot_dtype=torch.float, transform=False)\n",
    "# train_dataset, val_dataset = random_split(training_dataset, [int(len(training_dataset)*0.9), len(training_dataset)-int(len(training_dataset)*0.9)])\n",
    "\n",
    "# test_data = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/snps_crypticTest_emb.npy')\n",
    "# train_data = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/drs_crypticTest_emb.npy')\n",
    "testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "\n",
    "train_idx, validation_idx = train_test_split(np.arange(len(train_data)),\n",
    "                                             test_size=0.1,\n",
    "                                             random_state=42,\n",
    "                                             shuffle=True,\n",
    "                                             stratify=train_target)\n",
    "\n",
    "# Subset dataset for train and val\n",
    "train_dataset = Subset(training_dataset, train_idx)\n",
    "val_dataset = Subset(training_dataset, validation_idx)\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# # device = 'cpu'\n",
    "\n",
    "y_true = train_target\n",
    "# y_true = pd.concat([train_target, test_target])\n",
    "\n",
    "column_weight_maps = {}\n",
    "\n",
    "for column in y_true.columns:\n",
    "    column_values = y_true[column].dropna().values\n",
    "    values, counts = np.unique(column_values, return_counts=True)\n",
    "    frequency = counts / len(column_values)\n",
    "    \n",
    "    # Calculate weights as the inverse of frequencies\n",
    "    weights_inverse = 1/frequency\n",
    "    # weights_inverse = 1 - frequency\n",
    "    \n",
    "    # Normalize weights to ensure they sum up to 1\n",
    "    weights_normalized = weights_inverse / np.sum(weights_inverse)\n",
    "    \n",
    "    # Map each MIC value to its corresponding weight\n",
    "    weight_map = {value: weight for value, weight in zip(values, weights_normalized)}\n",
    "    \n",
    "    column_weight_maps[column] = weight_map\n",
    "\n",
    "def get_weighted_masked_cross_entropy_loss(column_weight_maps):\n",
    "    \"\"\"\n",
    "    Creates a loss function that computes a weighted cross entropy loss, taking into account class imbalances.\n",
    "    :param column_weight_maps: Dictionary mapping column names to their corresponding class weight maps.\n",
    "    \"\"\"\n",
    "    def weighted_masked_cross_entropy_loss(y_pred, y_true):\n",
    "        # weighted_losses = torch.Tensor().to(device)\n",
    "        weighted_losses = []\n",
    "        col_weight_map = column_weight_maps\n",
    "        # print(col_weight_map)\n",
    "        mean_weight = np.mean(list(col_weight_map.values())) # just in case if a number is not recognised and the loss doesn't go crazy\n",
    "\n",
    "        # print(y_pred.size())\n",
    "        # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        weights_col = [col_weight_map.get(y.item(), mean_weight) for y in y_true]\n",
    "        # print(weights_col)\n",
    "        # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        loss_fn = F.cross_entropy\n",
    "        col_loss = loss_fn(y_pred, y_true, reduction = 'none').to(device)\n",
    "        \n",
    "        # loss_fn = nn.CrossEntropyLoss(reduction = 'none')\n",
    "        # col_loss = loss_fn(y_pred, y_true)\n",
    "        # print(y_true.dtype)\n",
    "        # print(col_loss)\n",
    "        weights_col = torch.Tensor(weights_col).to(device)\n",
    "        # print(weights_col)\n",
    "        # print(col_loss)\n",
    "        weighted_col_loss = weights_col * col_loss\n",
    "        # print(weighted_col_loss)\n",
    "        weighted_losses.append(weighted_col_loss.mean())\n",
    "\n",
    "        total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        \n",
    "        # for i, column in enumerate(column_weight_maps.keys()):\n",
    "        #     col_weight_map = column_weight_maps[column]\n",
    "        #     print(y_pred.size())\n",
    "        #     # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        #     weights_col = torch.tensor([col_weight_map[y.item()] for y in y_true[:, i]], dtype=torch.float32, device=y_true.device)\n",
    "        #     print(weights_col)\n",
    "        #     # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        #     loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        #     col_loss = loss_fn(y_pred[:, i,], y_true[:, i])\n",
    "            \n",
    "        #     weighted_col_loss = weights_col * col_loss\n",
    "        #     weighted_losses.append(weighted_col_loss.mean())\n",
    "        \n",
    "        # total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        return total_weighted_loss\n",
    "\n",
    "    return weighted_masked_cross_entropy_loss\n",
    "\n",
    "# Also assuming `columns` is a list of your target column names corresponding to y_true and y_pred\n",
    "weighted_cross_entropy_loss_fn = get_weighted_masked_cross_entropy_loss(column_weight_maps['EMB_MIC_x'])\n",
    "# loss = weighted_cross_entropy_loss_fn(y_true_tensor, y_pred_logits, columns)\n",
    "\n",
    "def save_to_file(file_path, appendix, epoch, lr, cnndr, fcdr, l2, train_loss, test_loss, optimizer, model):\n",
    "    train_loss = [float(arr) for arr in train_loss]\n",
    "    test_loss = [float(arr) for arr in test_loss]\n",
    "    with open(file_path, \"a\") as f:\n",
    "        f.write(f\"#>> {appendix}, Epoch: {epoch}, LR: {lr}, fcDR: {fcdr}\\n\")\n",
    "        f.write(f\"Train_Loss= {train_loss}\\n\")\n",
    "        f.write(f\"Test_Loss= {test_loss}\\n\")\n",
    "        f.write(f\"lossGraph(Train_Loss, Test_Loss, '{appendix}-Epoch-{epoch}-LR-{lr}-fcDR-{fcdr}')\\n\")\n",
    "\n",
    "    torch.save({\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model': model.state_dict(),\n",
    "    }, f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/seq-{appendix}-{epoch}-{lr}-{cnndr}-{fcdr}-{l2}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_data = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/snps_crypticTest_emb.npy')\n",
    "# test_target = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/drs_crypticTest_emb.npy')\n",
    "# testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "# testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN architecture patterned after the reference figure:\n",
    "    - Input\n",
    "    - (conv1a) 1D Conv + ReLU [64 filters, kernel_size=5]\n",
    "    - (conv1b) 1D Conv + ReLU [64 filters, kernel_size=12]\n",
    "    - MaxPool1d(kernel_size=3, stride=1)\n",
    "    - (conv2a) 1D Conv + ReLU [32 filters, kernel_size=3]\n",
    "    - (conv2b) 1D Conv + ReLU [32 filters, kernel_size=3]\n",
    "    - MaxPool1d(kernel_size=3, stride=1)\n",
    "    - 2 × Fully-Connected (256 neurons) + ReLU\n",
    "    - Output layer of size 13 (Sigmoid)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_channels=1, \n",
    "                 num_classes=13, \n",
    "                 additional_input_dim=1, \n",
    "                 return_logits=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.return_logits = return_logits\n",
    "\n",
    "        # -------------------------\n",
    "        # Convolution Block 1\n",
    "        # -------------------------\n",
    "        self.conv1a = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 64, kernel_size=5),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv1b = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, kernel_size=12),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=1)\n",
    "\n",
    "        # -------------------------\n",
    "        # Convolution Block 2\n",
    "        # -------------------------\n",
    "        self.conv2a = nn.Sequential(\n",
    "            nn.Conv1d(64, 32, kernel_size=3),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2b = nn.Sequential(\n",
    "            nn.Conv1d(32, 32, kernel_size=3),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=3, stride=1)\n",
    "\n",
    "        # -------------------------\n",
    "        # Fully Connected Layers\n",
    "        # -------------------------\n",
    "        # We will flatten after the second pool, so we do not know the\n",
    "        # exact flattened size up front unless we do a test forward pass\n",
    "        # or can analytically compute it. Here we assume we find it\n",
    "        # dynamically in forward().\n",
    "        #\n",
    "        # Two fully-connected layers of 256 units each (with ReLU):\n",
    "        self.fc1 = nn.Linear(1, 256)   # Placeholder; will be reset in forward()\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        # -------------------------\n",
    "        # Final Output Layer\n",
    "        # -------------------------\n",
    "        # If following the figure exactly, it's a sigmoid output of size 13.\n",
    "        # If you want raw logits, set return_logits=True.\n",
    "        # We also accept an additional input dimension (e.g. 1 scalar).\n",
    "        # We'll incorporate that by adding to the flattened dimension\n",
    "        # right before the FC layers.\n",
    "        self.output_layer = (\n",
    "            nn.Linear(256 + additional_input_dim, num_classes) \n",
    "            if return_logits else\n",
    "            nn.Sequential(\n",
    "                nn.Linear(256 + additional_input_dim, num_classes),\n",
    "                nn.Sigmoid()  # or nn.Softmax(dim=1) if you prefer\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Apply weight initialization\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        \"\"\"Kaiming initialization for Linear layers.\"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, additional_input):\n",
    "        \"\"\"\n",
    "        x shape: [batch_size, in_channels, seq_len]\n",
    "        additional_input shape: [batch_size] or [batch_size, additional_input_dim]\n",
    "        \"\"\"\n",
    "        # Block 1\n",
    "        x = self.conv1a(x)   # -> shape [B, 64, ...]\n",
    "        x = self.conv1b(x)   # -> shape [B, 64, ...]\n",
    "        x = self.pool1(x)    # -> shape [B, 64, ...]\n",
    "\n",
    "        # Block 2\n",
    "        x = self.conv2a(x)   # -> shape [B, 32, ...]\n",
    "        x = self.conv2b(x)   # -> shape [B, 32, ...]\n",
    "        x = self.pool2(x)    # -> shape [B, 32, ...]\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # [B, features]\n",
    "\n",
    "        # Lazy shape detection for FC layers\n",
    "        # (We only know the exact shape after a forward pass.)\n",
    "        if isinstance(self.fc1, nn.Linear) and self.fc1.in_features == 1:\n",
    "            # Reinitialize fc1 in_features based on x\n",
    "            in_features = x.shape[1]\n",
    "            self.fc1 = nn.Linear(in_features, 256).to(x.device)\n",
    "            nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
    "            self.bn_fc1 = nn.BatchNorm1d(256).to(x.device)\n",
    "\n",
    "        # FC1\n",
    "        x = self.fc1(x)            # -> shape [B, 256]\n",
    "        x = self.bn_fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # FC2\n",
    "        x = self.fc2(x)            # -> shape [B, 256]\n",
    "        x = self.bn_fc2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Incorporate additional input\n",
    "        # If it's a single scalar per sample, make sure shape is [B, 1]\n",
    "        if additional_input.dim() == 1:\n",
    "            additional_input = additional_input.unsqueeze(1)\n",
    "\n",
    "        # Concatenate along feature dimension\n",
    "        x = torch.cat([x, additional_input], dim=1)  # shape [B, 256 + additional_input_dim]\n",
    "\n",
    "        # Final output layer\n",
    "        out = self.output_layer(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 50/300 [36:05<2:07:58, 30.71s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Training loss: 0.3329530656337738\n",
      "Validation loss: 0.33460938930511475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 100/300 [59:18<1:26:01, 25.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100\n",
      "Training loss: 0.3318028151988983\n",
      "Validation loss: 0.3336421847343445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 150/300 [1:19:11<56:59, 22.79s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150\n",
      "Training loss: 0.33093103766441345\n",
      "Validation loss: 0.33341822028160095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 200/300 [1:36:25<32:42, 19.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200\n",
      "Training loss: 0.3300253748893738\n",
      "Validation loss: 0.3331473171710968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 250/300 [1:52:18<16:47, 20.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250\n",
      "Training loss: 0.3292286694049835\n",
      "Validation loss: 0.33173686265945435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [2:06:43<00:00, 25.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300\n",
      "Training loss: 0.3285309672355652\n",
      "Validation loss: 0.330413818359375\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_114876/2800227602.py:163: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_0.0001_weighted_balanced.png-emb\n",
      "======================\n",
      "Optimizer details:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "Learning rate: 0.0001\n",
      "Weight decay: 0.0001\n",
      "======================\n",
      "Accuracy: 0.42480211081794195\n",
      "Mae: 0.7255936675461742\n",
      "F1 Score: 0.36825939028233856\n",
      "conf_matrix: [[ 22  73  10   5   1   1]\n",
      " [ 55 291  38   3   9   1]\n",
      " [ 27 201  41   2  25   0]\n",
      " [  3  27  10   7  95   7]\n",
      " [  0   6   5   2  93  31]\n",
      " [  2   1   0   0  14  29]]\n",
      "======================\n",
      "Doubling Dilution Accuracy: 0.8830255057167986\n",
      "AUC: 0.8834329999656323\n",
      "Sensitivity: 0.912568306010929\n",
      "Specificity: 0.8542976939203354\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABMtElEQVR4nO3deXxU1fn48c8zM1nISkjCviRsIsgqoiJqFG3dKqi1Sq1LtbVabWut7Vdr61K//bW22tr2q7VutYtK1apFpeIacJcdRUC2ACEsIUBIyJ55fn+cO8kkmRmCMmaQ5/165ZXMvc+cOXNv5j5zzj33XFFVjDHGmETj6+oKGGOMMZFYgjLGGJOQLEEZY4xJSJagjDHGJCRLUMYYYxKSJShjjDEJyRKUMV1ARKpFZHBX18OYRGYJynwuRKRERE5JgHo8KiL/29X1UNUMVV3X1fUI91n3kYikiMgjIrJHRLaKyPX7iP+6iGwQkb0i8pyI9OhsWSIyTkQWikiN93tc2LojRGSOiOwQEbvQ8yBmCcqYA0xEAl1dh/Y+pzrdBgwDBgEnAT8RkdOi1GcU8BfgYqAXUAPc15myRCQZ+A/wTyAH+BvwH285QCPwJHDFgXtrpitYgjJdyvumfI+IlHk/94hIircuT0ReEJHdIrJTRN4UEZ+37n9EZLOIVInIKhGZegDqcpaILPFe7x0RGRO27kYRWeu93scick7YustE5G0R+b2I7ARu81pq94rIi95z3heRIWHPUREZ6v29r9gvee+xUkTuE5G5IvKtfbyXSHUaIiKvi0iF17p4TES6e/H/AAYCz3vdjz/xlh/jbYvdIrJURIpivOwlwB2quktVVwAPApdFib0IeF5V56lqNfBz4FwRyexEWUVAALhHVetV9Y+AACcDqOoqVX0YWB5rG5nEZwnKdLWbgWOAccBYYBLwM2/dj4BSIB/3LfungIrIYcC1wFGqmgl8GSgBEJEpIrJ7fyshIhOAR4DvALm4b/ezQskSWAscD2QDtwP/FJE+YUUcDawDegK/9JbN8GJzgDVhyyOJGCsiecDTwE1evVYBkzv5ttrXSYBfAX2Bw4EBuJYKqnoxsBH4itf9+BsR6Qe8CPwv0AO4Afi3iOR7dbtRRF7w/s7xyl0a9vpLgVFR6jYqPFZV1wINwPBOlDUKWKZt52lbFuO1zEHKEpTpahcBv1DV7apajjtIX+ytawT6AINUtVFV3/QOSs1ACjBSRJJUtcQ7wKGqb6lq909Rj28Df1HV91W1WVX/BtTjkieq+pSqlqlqUFX/BazGJdOQMlX9k6o2qWqtt+wZVf1AVZuAx3BJOJposWcAy1X1GW/dH4GtnXxPbeqkqmtU9RWv1VEO/A44McbzvwHMVtXZ3vt+BVjg1QlV/bWqnuXFZni/K8OeXwlkEllGu9jw+H2VFeu55gvEEpTpan2BDWGPN3jLAH6La028LCLrRORGAFVdA1yH+/a/XURmikhfPptBwI+8rqzdXitsQKguInJJWPffbuAIIC/s+ZsilBmeSGpoPfBGEi22b3jZXoIu7dQ7alcnEenpbavNIrIHdw4nL/JTAbdNzm+3TabgvjS0V+39zgpblgVURSm7ul1sePy+yor1XPMFYgnKdLUy3IEwZKC3DFWtUtUfqepg4CvA9aFzTar6uKpO8Z6rwJ2fsR6bgF+qavewnzRVfUJEBuHOgVwL5HottI9wXWYh8RottgXoH3ogIhL+eB/a1+lX3rIxqpqFayHFeg+bgH+02ybpqvrrDi+kusur69iwxWOJfh5oeXisuCH3KcAnnShrOTDG2xYhY2K8ljlIWYIyn6ckEUkN+wkATwA/E5F873zLLbhv9qFBC0O9A9EeXNdes4gcJiIne+eH6oBab11n+dvVIxmXgK4SkaPFSReRM72T9um4g3e5V69v4lpQn4cXgdEiMt3bXtcAvT9lWZm41sdu7/zSj9ut3waEX5v1T+ArIvJlEQltsyIRiZYg/47blzkiMgLXbfpolNjHvLKPF5F04Be4bs5QKyhWWcW4/f19cYNsrvWWvw4uiYtIKpDsPU4NO5doDiKWoMznaTYumYR+bsOdgF+AO8n9IbDIWwZumPGruIPqu8B9qlqM+6b9a2AHrmusJ24ABd4BL9RFFM2N7erxuqouwB0E/w/YhetavAxAVT8G7vbqsA0YDbz9KbfBflHVHcD5wG+ACmAkbnvVf4ribgcm4M7XvAg80279r3BJYbeI3KCqm4BpuG1bjmtR/RjvuCEiPxWR/4Y9/1bcYJINwFzgt6r6UmilNzrweO99LQeuwiWq7bjk+d3OlKWqDcB03Ei/3cDlwHRvObhWdS2tLapa3OASc5ARu2GhMQcPccPsS4GLVPWNrq6PMfFkLShjEpzXxdbd66b6Ke680XtdXC1j4s4SlDGJ71hcd9cO3GCR6apaKyL3e91m7X/u79rqGnNgxLWLT9zUJH8A/MBD7Uf/iMg04A4gCDQB16nqW966H+DOCQjwoKreE7eKGmOMSThxS1Ai4gc+AU7F9ZnPB2Z4J5xDMRnAXlVVcdPKPKmqI0TkCGAm7kLIBuAl4GpVXR2XyhpjjEk48ZxAchKwJjRjs4jMxI0IaklQ3hxcIaGhvOCmYXlPVWu8584FzsGNZIoqLy9PCwoKPlVl9+7dS3p6epfGJko9EiE2UeqRCLGJUo+DLTZR6pEIsYlUj0gWLly4Q1XzO6xQ1bj8AF/FdeuFHl8M/F+EuHOAlcBO4Fhv2eG41lcukIYb3vunKK9zJW7Y7YKBAwfqp/XGG290eWyi1CMRYhOlHokQmyj1ONhiE6UeiRCbSPWIBFigEY7v8RwkIRGWdehPVNVnVXUE7rqGO7xlK3AzA7yC695bijtH1bFA1QdUdaKqTszP75iAjTHGHJzimaBKcXOZhfTHm8ImElWdBwzxZhNAVR9W1QmqegKudWXnn4wx5hASzwQ1HxgmIoXeVDIXArPCA8KmsQnd7iAZd7U8ItLT+z0QOBc3JY4xxphDRNwGSahqkzdH1hzcMPNHVHW5iFzlrb8fOA+4REQacdORXOD1R4K770wu7pYL16ibQNIYYz4XjY2NlJaWUldX12FddnY2K1as6FQ5iRCbKPVITU2lf//+JCUldSo+rreBVtXZuPnXwpfdH/b3nUSZhVpVj49n3YwxJpbS0lIyMzMpKCig7cTpUFVVRWZm524/lQixiVAPVaWiooLS0lIKCws7VbbNJGGMMRHU1dWRm5vbITmZT0dEyM3NjdgijcYSlDHGRGHJ6cDa3+1pCcqzYU8zTy6IdFNUY4wxXcESlGfx9mZ+8vQygkG7/YgxputVVFQwbtw4xo0bR+/evenXr1/L44aGhpjPXbBgAd///vf3+RqTJ08+UNWNi7gOkjiY+LyWZ1AVX8RrjI0x5vOTm5vLkiVLALjtttvIyMjghhtuaFm/d+/eqM+dOHEiEydO3OdrvPPOO5+5nvFkLShPKCU12w0cjTEJ6rLLLuP666/npJNO4pZbbuGDDz5g8uTJjB8/nsmTJ7NqlbtxcHFxMWeddRbgktvll1/OGWecweDBg/njH//YUl5GRkZLfFFREV/96lcZMWIEV1xxRWgqOWbPns2IESOYMmUK3//+91vK/TxYC8oTakFZfjLGtHf788v5uGxPy+Pm5mb8fn+nnhstdmTfLG79yqj9rssnn3zCq6++Sk1NDarKvHnzCAQCvPrqq/z0pz/l3//+d4fnrFy5klmz3DwJhx12GFdffXWHa5EWL17M8uXL6du3L8cccwxvv/02EydO5Dvf+Q7z5s2jsLCQGTNm7Hd9PwtLUJ7Q4JJmOwdljElg559/fkvCq6ys5NJLL2X16tWICI2NjRGfc+aZZ5KSkkJmZiY9e/Zk27Zt9O/fv03MpEmTWpaNGTOGkpISMjIyGDx4cMt1SzNmzOCBBx6I47tryxKUJ3TeKWhNKGNMO+1bOvG8oHZfwm9t8fOf/5yTTjqJZ599lpKSEoqKiiI+JyUlpeVvv99PU1PHubfDY3w+H01NTWgXHw/tHJQn1IIKBru2HsYY01mVlZX069cPgEcfffSAlz9ixAjWrVtHSUkJAP/6178O+GvEYgnKEz6KzxhjDgY/+clPuOmmmzjuuONobm4+4OV369aN++67j9NOO40pU6bQq1cvsrOzD/jrRGNdfB4bxWeMSVS33XZbxOXHHnssn3zyScvjO+64A4CioqKW7r7Qc6uqqgD46KOPWuKrq6s7xAPcfffdLd2SJ510EitXrkRVueaaazo1fP1AsRaUx1pQxhjT0YMPPsi4ceMYNWoUlZWVfOc73/ncXttaUB6fnYMyxpgOfvjDH/LDH/6wS17bWlCelmHm1oIyxpiEYAnKE9oQNhefMcYkBktQHjsHZYwxicUSlCd0nxJrQBljTGKwBOUJbQib6sgYkwiKioqYM2dOm2X33HMP3/3ud6PGL1iwAIAzzjiD3bt3d4i57bbbuOuuu2K+7nPPPcfKlStbHt9yyy28+uqr+1n7A8MSlEdaJou1BGWM6XozZsxg5syZbZbNnDmzUxO2zp49m+7du3+q122foH7xi19wyimnfKqyPitLUB6fjeIzxiSQr371q7zwwgvU19cDUFJSQllZGY8//jgTJ05k0qRJ3HrrrRGfW1BQwI4dOwD45S9/yWGHHcbZZ5/dcjsOcNc3HXXUUYwdO5bzzjuPmpoa3nnnHWbNmsXPf/5zxo0bx9q1a7nssst4+umnAXjttdcYP348o0eP5vLLL2+pW0FBAbfeeisTJkxg9OjRbRLcZ2HXQXlCM0nYdVDGmA7+eyNs/bDlYbfmJvB37vAZNbb3aDj911Gfl5uby6RJk3jppZeYNm0aM2fO5IILLuCmm26iR48e7N69m+nTp7Ns2TLGjBkTsYyFCxcyc+ZMFi9ezK5duzjxxBM58sgjATj33HP59re/DcDPfvYzHn74Yb73ve9x9tlnM3XqVC6++OI2ZdXV1XHZZZfx2muvMXz4cC655BL+/Oc/c8UVVwCQl5fHokWLuO+++7jrrrt46KGHOrV9YrEWlMdG8RljEk14N1+oe+/JJ59kwoQJTJkyheXLl/Pxxx9Hff6bb77JOeecQ1paGllZWZx99tkt6z766COOP/54Ro8ezWOPPcby5ctj1mXVqlUUFhYyfPhwAC699FLmzZvXsv7cc88F4Mgjj2yZXPazshaURyxBGWOiadfSqd2PW2jsT2x706dP5/rrr2fRokXU1taSk5PDXXfdxfz58wkEAnzve9+jrq4uZhmhEcrtXXbZZTz33HOMHTuWRx99lOLi4pjl7Ov8fOh2HdFu5/FpWAvK47MbFhpjEkxGRgZFRUVcfvnlzJgxgz179pCenk52djbbt2/nv//9b8znn3DCCTz77LPU1tZSVVXF888/37KuqqqKPn360NjYyGOPPdayPDMzs2US2XAjRoygpKSENWvWAPCPf/yDE0888QC908jimqBE5DQRWSUia0Tkxgjrp4nIMhFZIiILRGRK2LofishyEflIRJ4QkdS41tX7bfnJGJNIZsyYwdKlS7nwwgsZO3Ys48ePZ9SoUXz3u9/luOOOi/ncCRMmcMEFFzBu3Di+8Y1vcPzxx7esu+OOOzj66KM59dRTGTFiRMvyCy+8kD/84Q+MHz+etWvXtixPTU3lr3/9K+effz6jR4/G5/Nx1VVXHfg3HCZuXXwi4gfuBU4FSoH5IjJLVcM7TF8DZqmqisgY4ElghIj0A74PjFTVWhF5ErgQeDRe9fWJ3VHXGJN4zjnnnDbda6EbE7a/U294F134OaCbb76Zm2++uUP81VdfzdVXX93h9Y477jjmz5/fEht+I8SpU6eyePHiNvENDQ1tXm/ixIn77C7srHi2oCYBa1R1nao2ADOBaeEBqlqtrVs+HQjPDgGgm4gEgDSgLI51DZvN3BKUMcYkgngmqH7AprDHpd6yNkTkHBFZCbwIXA6gqpuBu4CNwBagUlVfjvQiInKl1z24oLy8/FNX1m5YaIwxiSWeCSrS0JEOR39VfVZVRwDTgTsARCQH19oqBPoC6SLyjUgvoqoPqOpEVZ2Yn5//qSvra5lJ4lMXYYz5grGZZQ6s/d2e8UxQpcCAsMf9idFNp6rzgCEikgecAqxX1XJVbQSeASbHsa42is8Y00ZqaioVFRWWpA4QVaWiooLU1M6Pd4vndVDzgWEiUghsxg1y+Hp4gIgMBdZ6gyQmAMlABa5r7xgRSQNqganAgjjW1a6DMsa00b9/f0pLS4l06qCurq7TB9pEiE2UeqSmptK/f/9OxUIcE5SqNonItcAcwA88oqrLReQqb/39wHnAJSLSiEtEF3iDJt4XkaeBRUATsBh4IF51hbAbFlqCMsYASUlJFBYWRlxXXFzM+PHjO1VOIsQmUj32R1xnklDV2cDsdsvuD/v7TuDOKM+9FYg8E2IctI7i+7xe0RhjTCw2k4RHbDZzY4xJKJagPKELde2EqDHGJAZLUJ6W66Csi88YYxKCJSiP3W7DGGMSiyUoT+tksZagjDEmEViC8lgLyhhjEoslKE/LKD47B2WMMQnBEpTHWlDGGJNYLEF5Ws5B2Vx8xhiTECxBeVpbUF1bD2OMMY4lKI/NJGGMMYnFEpTHZpIwxpjEYgnKE9oQdj8oY4xJDJagPGLnoIwxJqFYgvK03m7DMpQxxiQCS1Aeu2GhMcYkFktQHhvFZ4wxicUSlCfUxWf5yRhjEoMlKE/r/aAsQxljTCKwBOWxufiMMSaxWILyiHcSykbxGWNMYrAEFcbvE7sOyhhjEoQlqDA+sVF8xhiTKCxBhfGJ2DkoY4xJEJagwvhE7ByUMcYkiLgmKBE5TURWicgaEbkxwvppIrJMRJaIyAIRmeItP8xbFvrZIyLXxbOuYOegjDEmkQTiVbCI+IF7gVOBUmC+iMxS1Y/Dwl4DZqmqisgY4ElghKquAsaFlbMZeDZedW2ts10HZYwxiSKeLahJwBpVXaeqDcBMYFp4gKpWa+sNmNKBSNlhKrBWVTfEsa6Aa0HZ/aCMMSYxxDNB9QM2hT0u9Za1ISLniMhK4EXg8gjlXAg8Ee1FRORKr3twQXl5+WeqsE/ERvEZY0yCiGeCkgjLOhz9VfVZVR0BTAfuaFOASDJwNvBUtBdR1QdUdaKqTszPz/9MFXaj+D5TEcYYYw6QeCaoUmBA2OP+QFm0YFWdBwwRkbywxacDi1R1W3yq2JZPbCYJY4xJFPFMUPOBYSJS6LWELgRmhQeIyFDx5hgSkQlAMlARFjKDGN17B5obxWcJyhhjEkHcRvGpapOIXAvMAfzAI6q6XESu8tbfD5wHXCIijUAtcEFo0ISIpOFGAH4nXnVszydCc/DzejVjjDGxxC1BAajqbGB2u2X3h/19J3BnlOfWALnxrF97Ph82is8YYxKEzSQRxkbxGWNM4rAEFcZvo/iMMSZhWIIKIzaKzxhjEoYlqDA2is8YYxKHJagwbhSfJShjjEkElqDC2EwSxhiTOCxBhfH5sC4+Y4xJEJagwvjtjrrGGJMwLEGFETsHZYwxCcMSVBh3P6iuroUxxhiwBNWGz+6oa4wxCcMSVBifnYMyxpiEYQkqjCUoY4xJHJagwriZJLq6FsYYY8ASVBti56CMMSZhWIIK40bxWYIyxphEYAkqjN0PyhhjEoclqDA+EYJ2y3djjEkIlqDC+MTm4jPGmERhCSqM3Q/KGGMShyWoMHY/KGOMSRyWoML4bC4+Y4xJGJagPGl7NzKh6g0bxWeMMQnCEpQnv/wdvll2OwSburoqxhhjsATVQiUAgC/Y3MU1McYYA3FOUCJymoisEpE1InJjhPXTRGSZiCwRkQUiMiVsXXcReVpEVorIChE5Np51DfqSAPBrQzxfxhhjTCcF4lWwiPiBe4FTgVJgvojMUtWPw8JeA2apqorIGOBJYIS37g/AS6r6VRFJBtLiVVcIb0E1xvNljDHGdFKnWlAiki4iPu/v4SJytogk7eNpk4A1qrpOVRuAmcC08ABVrdbWye/SAfVeIws4AXjYi2tQ1d2dfE+fSmsLys5BGWNMIuhsF988IFVE+uFaPd8EHt3Hc/oBm8Iel3rL2hCRc0RkJfAicLm3eDBQDvxVRBaLyEMikh7pRUTkSq97cEF5eXkn305HQZ9rQQXUWlDGGJMIOpugRFVrgHOBP6nqOcDIfT0nwrIOY7hV9VlVHQFMB+7wFgeACcCfVXU8sBfocA7Le/4DqjpRVSfm5+d36s1ELMfr4vNbgjLGmITQ6QTlDVK4CNfSgX2fvyoFBoQ97g+URQtW1XnAEBHJ855bqqrve6ufxiWsuGnt4rMEZYwxiaCzCeo64CbgWVVdLiKDgTf28Zz5wDARKfQGOVwIzAoPEJGhIiLe3xOAZKBCVbcCm0TkMC90KhA+uOKAC7WgkixBGWNMQujUKD5VnQvMBfAGS+xQ1e/v4zlNInItMAfwA494ye0qb/39wHnAJSLSCNQCF4QNmvge8JiX3NbhznvFTegclI3iM8aYxNCpBCUijwNXAc3AQiBbRH6nqr+N9TxVnQ3Mbrfs/rC/7wTujPLcJcDEztTvQAh18QWwUXzGGJMIOtvFN1JV9+AGMswGBgIXx6tSXSHUxRewC3WNMSYhdDZBJXnXPU0H/qOqjUQYkXcwax1mbi0oY4xJBJ1NUH8BSnAX084TkUHAnnhVqiuo2IW6xhiTSDqVoFT1j6raT1XPUGcDcFKc6/a5CrWgkmlE7ZYbxhjT5To71VG2iPwuNGODiNyNa019YYTOQSVLk91V1xhjEkBnu/geAaqAr3k/e4C/xqtSXSE0ii+JZiw/GWNM1+vsbOZDVPW8sMe3i8iSONSny4R38QWti88YY7pcZ1tQte3u1XQc7sLaL4yWmSRosgRljDEJoLMtqKuAv4tItvd4F3BpfKrUNVq7+OwclDHGJILOTnW0FBjr3acJVd0jItcBy+JYt89V+CAJy0/GGNP19uuW76q6x5tRAuD6ONSn64jQLAGSaSJoGcoYY7rcfiWodiLd7+mgFvQl2TkoY4xJEJ8lQX3hjuJBX7I7B2UJyhhjulzMc1AiUkXkRCRAt7jUqAsFfUneTBJdXRNjjDExE5SqZn5eFUkEQUkiWZptFJ8xxiSAz9LF94UT9CfbMHNjjEkQlqDCqNfFV9/U3NVVMcaYQ54lqHBeC6qmwRKUMcZ0NUtQYSSQQrIlKGOMSQiWoMIFkkiWJmoa7KaFxhjT1SxBhfH5U6yLzxhjEoQlqDC+JEtQxhiTKCxBhfElJZNME7WWoIwxpstZggrjD1gLyhhjEoUlqDD+5BSSpYlaGyRhjDFdLq4JSkROE5FVIrJGRG6MsH6aiCwTkSUisqDdXXtLROTD0Lp41rPlNf3JpFgLyhhjEkJn76i730TED9wLnAqUAvNFZJaqfhwW9howS1VVRMYATwIjwtafpKo74lXHDvzJbph5oyUoY4zpavFsQU0C1qjqOlVtAGYC08IDVLVatWXu8HS6+hYe3jBzGyRhjDFdL54Jqh+wKexxqbesDRE5R0RWAi8Cl4etUuBlEVkoIldGexERudLrHlxQXl7+2WrsTyKJRrtQ1xhjEkA8E1SkO+52aCGp6rOqOgKYDtwRtuo4VZ0AnA5cIyInRHoRVX1AVSeq6sT8/PzPVmO/G2ZeU28Jyhhjulo8E1QpMCDscX+gLFqwqs4DhohInve4zPu9HXgW12UYX/5kABoaGuL+UsYYY2KLZ4KaDwwTkUIRSQYuBGaFB4jIUBER7+8JQDJQISLpIpLpLU8HvgR8FMe6OgGXoBob6uL+UsYYY2KL2yg+VW0SkWuBOYAfeERVl4vIVd76+4HzgEtEpBGoBS7wRvT1Ap71clcAeFxVX4pXXVt4Laimxvq4v5QxxpjY4pagAFR1NjC73bL7w/6+E7gzwvPWAWPjWbeI/EkANNZbC8oYY7qazSQRzp8CQLDREpQxxnQ1S1DhUrMBSGqqovXyLGOMMV3BElS4tFwAsnUPDc3BLq6MMcYc2ixBhfMSVA+qbDYJY4zpYpagwnkJKkeqbMJYY4zpYpagwnXLAaCHJShjjOlylqDC+QM0JmeTY118xhjT5SxBtdOUkkOOVFNV19jVVTHGmEOaJah2JD2XHKrYuseuhTLGmK5kCaqdpMx8ekgVWyotQRljTFeyBNWOPz2PXF81Wypru7oqxhhzSLME1V5aD3KoYssuS1DGGNOVLEG1l5ZLCg3srKzs6poYY8whzRJUe97FuvWV27u4IsYYc2izBNWel6D8dTvtWihjjOlClqDay+4HwEDZbgMljDGmC1mCai/vMFT8jPBtZKsNNTfGmC5jCaq9pFSacoZyuGxgfcXerq6NMcYcsixBRRDoO5pR/k0s3LCrq6tijDGHLEtQEUjvI+jDDlaVbOrqqhhjzCHLElQkvY4AIH3XKrZX2XkoY4zpCpagIukzFoBxvjUsLLFuPmOM6QqWoCLJ6In2GMqxgVW8t66iq2tjjDGHJEtQUUjBcRztW8Xbq7d1dVWMMeaQZAkqmkHHkaZ7Sa5YyebddsGuMcZ83uKaoETkNBFZJSJrROTGCOunicgyEVkiIgtEZEq79X4RWSwiL8SznhEVHAfAmf73eGt1+ef+8sYYc6iLW4ISET9wL3A6MBKYISIj24W9BoxV1XHA5cBD7db/AFgRrzrGlN0fHX0+VwVeYPUHc7qkCsYYcyiLZwtqErBGVdepagMwE5gWHqCq1aqq3sN0IPQ3ItIfOJOOSetzI2f9ntrkXI7aOpO15dVdVQ1jjDkkxTNB9QPCr3Qt9Za1ISLniMhK4EVcKyrkHuAnQDDWi4jIlV734ILy8gPcFZeSiX/4qRztW8Hf3153YMs2xhgTUzwTlERYph0WqD6rqiOA6cAdACJyFrBdVRfu60VU9QFVnaiqE/Pz8z9jlTvqNqyI7rKXJQvessESxhjzOYpngioFBoQ97g+URQtW1XnAEBHJA44DzhaRElzX4Mki8s841jW6wuMB+IZvDv96blaXVMEYYw5F8UxQ84FhIlIoIsnAhUCbI7yIDBUR8f6eACQDFap6k6r2V9UC73mvq+o34ljX6LL6Qt5wzvcVc8n6G3j6A+vqM8aYz0PcEpSqNgHXAnNwI/GeVNXlInKViFzlhZ0HfCQiS3Aj/i4IGzSROGbMpHnq7eTJHoqff4zK+sSrojHGfNEE4lm4qs4GZrdbdn/Y33cCd+6jjGKgOA7V67zcIfgnX0vTu/cyvfp1nlw3qe1wRGOMMQeczSTRWf4AgYmXcYpvEVL6Pmu227BzY4yJJ0tQ++P4H9HYayy/CfyZX8x8g5qGpq6ukTHGfGFZgtofSakknf8wGVLPeeX3ccfdd1OyvbKra2WMMV9IlqD2V94wyvp/hWn+d/hV/f/j5Qd/ajc1NMaYOLAE9SmsL/w6nPsglf2LuKjhKdb+/nTKX7kHEnAAojHGHKwsQX0KQX8KjPka2ef9gZTUNAqDG8l/+1b2/PlUKN3n5BfGGGM6wRLUZ5FTQOCmEhq/t4zfpV5D/bZP0IemEnzjV11dM2OMOejF9TqoQ8WA3HSu+P7t/OifUzlr011Mn/trylIH0zelgeGrZsGIXNj6IQyZCv4kKF8J+SMgrUdXV90YYxKWJagDJDstiQe/fTL/XTKYkv+cQcGcKwHojQ/uf9kF5Y+A6u1QuxMyesPVb0Nqd9i9AXKHdF3ljTEmAVmCOoBEhDPGF1KR/U+efe6v/KN8CDk98rl36Puk9hwKr94O2f3hy7+E538Af58OwSYoXwHnPQyp3Zn0/vch83/gyEth1Uuwcx0cczVIpMnhjTHmi8sSVBzkDj6SaddNYPe7JfzyxY+Zsnwa1/QcwlcumkJe30LXtecLwJt3g/ig9xj49xUApIof5vwUBh4Lz13tWlu1u+Dkm13ha9+Aj/4NR36TntuKYW0QhpzsuhA/mQNTrgdfx1OLgcZqqNnZ2q3Y3AT+fez+siXwys9h2r3QfeCB20DGGNMJlqDixOcTvnlcIYGd63muNJXbn/+Y24Gx/Wv5wSnDOHnM12DM11xw9XaYeyf0GcuCLTBp8Y/hwZOhoQoGnwTzfgPdukPuMHj8fPecpTMZGWyEFb+HY6+F1S/Djk+gbjcUnuhmYc8/HP5zDVRt4diSd+F94MSfAAJv/g6m/Qne/gMccR4UHA+Zvd0PQDAIL/wQyhbB6/8L5z7glleshZQsSM2CNa/Se8u70HQsBFLisyGrtrmk6k+KT/nGmIRlCSrOBmT6+PfVk1mxZQ9vri5n5gebuPzRBUwq7EF+ZgpFw/OZNq4fyWfeDUBNcTF8/UmYdS0MPhG+9nd46jLXqkrvCTmFcPEz8MyVbPANZFCvHHj3/9yL9RkL7/zJ/QAMnAwb34HcYezIO4peud3htV94NRNXrvigbHFrhXMKGBfMgNUpLjn1GQfLnoRxF7nk9eDJ0C0H0vOhbBEjAO6bDZe94JJiyVvuOaHBIN1yIC0XPv4PQ1e/AH0bYNBkaNgLGT1d12XZEti9EQ7/ikuwmxfBoMn03DYP7jkfRk6H8x509Qs2Q+1uSM+NvMFXvAAN1TD2wsjrFz5Kj4qdQFH0nVa2BLL6QcaBvwGmMabzLEF9Tg7vk8XhfbK4dHIBv3vlE95ft5NFG3bx4rIt3PPqak48LJ9TD+/Fxj3N1A0oIvUHy9yFvz4fnPcQPHEhrH0dvvwg9BgM33qV9cXFDDrxRNey2rMZpt4CG98Ffwos+hsseQz6HwWXv8yKefPoVVTkzmtteh8GHA3PfBvO/hP4k6G5AXauhW0fIxuXg/jhlNvgyG/Cw1+Cx78GSd1cS6l2F1RthXMfZNnqTYxZ9TuY+XXX1fjm3TD+G1C+CkrnuwSYkgl1lfSVADz+YutGGXqKa+29fod7/ax+sHcHNNeDP5mRzQ0uwX34JEy6ErL7uaRatsQlxOz+8Px1MOU6V96798Gcm1zdU7vDrhIYfX5rMqvaBi/+iJGSDKd8zT2/aiusK4bRX3PbunIzPHyq69L81qv03/Qc/P0euPjZfZ4H9DU3wIZ3YdCxbVds/QhSs6H7gMhPNMZEZAnqc5YS8HPT6YcDoKoUryrn7++WMGtJGY+/vxGAuxa9yskjejJlaB5j+nenZ2YKORc85lpDQ6a2LVAEjrmq9XHhCe53vyMhb7hrlYSfkzrsNPcD8JP1Ec9DLS4upqioqHXBZS/CSze6vydfC74kl1D6TWDnzmLX/ffUZa4llpINi72bHxf9FJpqobIUjvwmb66r4cT+CluXuhbUW/fAmlddohpxJqyf55JU/4mw9g1W7s1kxLQfwX3HugQtAg01rvX2+AWuBVa+EsoWMzxrLGx5BQ47w7XinrjA1eG12+GEG6CpASo3QbAJ8fng/yZBzxEuae0pde9nwiWu9Rlshl0bYOY3KChdAM117stBUz0M/zLsLXcJeW+5a9UecR4c+12GrH0E3vwvnHk3HPUtqK+Gxlp45MuuW/Q7c12dP3mZ8Yt+BqP+CfnDXT3XzYUehS4xbnjXnVOc9O2WpJhRtQ5ef8u9v34TOv8PZ8xBzBJUFxIRThrRk5NG9KSusZkFJbt4Z8EStgfyKV61nf8sKWuJHd4rgwkDe5K+cgWTCntQ09DEhu1NHL6njp6ZKUj7b/f+QGvLIpp9DZIIyciHrz4cff2IM+GGT9z5qaQ0+POxrlV3wg3g87eEaUkxDC+C4V/y3tRprvUWOuBOvLy1zFHnsLW4mBFpPeDSWa6119QAl/3NtZBeuM61Fk/+Obx5N322vAYTr4DTfwNLn3CtrinXu4TT0q0JFJ7AsuzTGJ+8wbUkk7q5QSqv3AKBbrDwrzDmAiiYAv/5Ln7ELZ/5dWiqg8FFLqnt3ujed8Vq12qr3ESfLa9AUjq8eAPMfxi2fwxpeS5JBZvhn+fCif8D/7mG7LpK+OvpkDMIhp8Ob/zStYyPv96d+2tugMa9cORlMOt7TFzxvKv/e/e7Lt4eQ2DTe6512ncCrHwRUBgylSMX/BBqToHT7ow4YGa/BYORW4+7N1mr0MSVJagEkZrkZ8qwPJo2BygqGkswqKzcWsXa8mo27qzhvXUVzFm+lZqGZh5+a33L8+5Z9BqpST56ZaVyRN9sBuWmUV5VT8XeBo4q6EFywMeI3pmEblSsqh2T2YHQLce1fADOugd6HdEmOUU08JjOlZ1/GFw5t7XLE1wXX1MDBJJh4uW8+e58Tpj6ZbduwsXuB1wX4talkNELFv4NRpxB5apdUHRNa/nbV8KjZ8Az34KcAtdVmtUH6ipZ/8lyBnfHtQpHnOVafOk93bnBYae40ZDPfAveuw8kCa58Az58Gja84xLmksddS2joKfD0FfCvb0C3HD4adSNH1H3guiHf+F/XKty5zg1qGXC0q++rt8EHD8LectYXXETh6de45//tKxBIdefrAJIz3Hk3gNTuZNbthg8egI3vuVbhpG+7FuBrv3DJbNR0+O//QGMtw1OHw7BM1youW+RacIOmuO0KsP5NeO67MOhYBu+qh7W/ggsfhw1vw5MXw9efckmy9xHu9/7asYbBa/8GJxzv/l82vg99xrgvDuaQZwkqQfl8wsi+WYzsmwXANScNBaC+qZmFG3aRm57C3Hc/ILnnYMoq69i0s4alpbuZ/dEWemamkJES4PWV21vKS/FDcvEcGoNBDuudxVGDcshJTyY92U9aSoBgUBnZN4thPTOpqA2ysaKG5ICPXlkRWmf7MvGbB2w7tBDp+C0+dBBN6+HmR4zE54O+493fJ93kfq8qbhvTcwRc/S7Mf9Ad0LP6uOXHfpeN9cUMPmq0SzAjp7dNkuBaoV/9K0y9hQXvvsWk/MNaLwkAOPUXrlXp88E178H2FdDvSHa8vwSKboL6Knj9lzDmfNj2sWulHflN0CC8fDMs+juc/ygbtqZT2GsUXPGK604NNrlEumUZrJ4Dx10Hu9bD8z9gc9/T6TemCJY+DrNvgJ3rXTnJ6bD9MVjwsBu4MmgyvVbNgQfntN0eaXmum3L+Q1Dypuu2XfYvWi40+Mc014oFeO4qqKlw5x8v+jdseAvm/ZbxFdvh2Ffdechty11LMJTA9la4fZnWA97+PQM3PQObvu3O0z3yJZj8ffjSHZH3pzmkWII6yKQE/EwekgfAlhw/RccVtlkfDCo+nzuQl+2uJeAX3l6zg/++t5w+ffvh8wkfba7k7+9uoKE5GP2F5r4BQN/sVMYN7M7e+mZ6ZaWQkZJE7+wUquuaqG8Kkl3bzK7FpZw8ohfZ3Q7ioeCZveDkn0Vel54Lo85xf0dK1iLQYzA16Rs7rkvJaP07q6/7abM+E07/tfu735Ft153xW/jy/3MjIrcWe3XJc63HkEGTw85BnggFx7N6WQn9jpkKR10B/zwP3rvXdR9e+oLrOnz3/1x3aq9RvPPqixzffbtbPuxU2LHanW986lJIzoQv/dK1Rv91Mbt3VtD9jJ/Bk5e4+ILjXQLLKXAttL8cD9s+gsy+ZFVtgT+Mg5odrmolb7mW17J/udYbuEE4oa7LlS+6MsG1dE/8SdsWWWMdLP4Hheveg6OOcOcTm+rdiNGkVPfFYeFfYc1rcM5fXAtsXy34kA3vulabSTiWoL5gQskJoG93101yzvj+5FSuoajoiJZ1qkp9U5C99U3UNDQDMPeTcnbtbWDXlhKOGHk4e2obmV+yi2Wbd5OVmsTyskrqGoNU1zfhE/CJ0BRUmL+Ubkl+Jg/JpaE5yOZdtQAU5KWTkRKgvKqe3tmpNOxuYG1gPY3NQQbkpNEjPZkkv5AS8JOW4icvI4Xsbkltkuwhb3+v/8odArKp9bkXPwd7t7suw1By9S5pAGgOpLdt8fYY7Lpn5/3WjZzsNdItv/g5lswtpuiwk+GCx9wI0bP/5LoyC6bA+/e75HDMd+Hkn7Phseso2PKi6+5trHGXSdw1zLW2Bk52Lc8XrwegMZBJ0opZUFsJvUe7ASJ/+4obZLJrPfSf5JZ/8BcGAdz/DlR552ez+rnzdhvfd+cdAf4+zZ3/mzGTlLrtbmBNw143onT5M7B0phvYs+Y1dw7w8a/BcT+ApJPd8+sqXfJLz3fnVXOH2EwuXcQS1CFKREhN8pOa5Cd0RdE3jhkEQHHxZoom9AfgsnYtNIAd1fX4RahpbObJl9/mhKMn8Myizby7roLM1CQO75MFAiu27KGuoZne2anML9nJlt2NvLj+46h18vuEQT3SKKnYS1a3JKrrGhm34h22VNbROzuVCyYOIOAXRFxLclBuGntqm+iRnsyH5U3sWVpGj7RkBuWmkZ+ZQmpSJ79Bf5H5fK0XX3dWdj/4yj0dyxGva3P4l1oHuoRGhJ5wg/vxlBR+nYJL7nVJUtUlkmX/gkHHuam7NAjPfgc2L6IkdyrD1jzkukLP/pObLWXlCy55FB4PCx5xI1jHzuDD5kJGr7jLtQAHnwRv3wMv/ggQOOlnUL3VdU2KD168nqN3lsDKO9wsKklpLvk018O9k1w3KV7iWfwYo9I+gKU/cJceBFJcq3nho+683Nivu/OFR14KA47G11zvBr+oQnJa63aqr3KvnZwOdXv2b7ubDixBmf2Wl+HO9+QA43sGOHJQD44ctO+Z2V9/4w0mHH0cAb+Pkh172VPXSFOza8nVNDSxamsVK7dWcerIXuxtaGLbljJKG5oZ0jODlVv28JN/L4v9AgtbLzhO8gsTBuYwtGcGQYVPtlUxND+DPt1TWb++ga1pGynIS6eiuoH6pmZ8Ipx0WE/qm1xrMjM1CRGoalBqGppIS7aPyn4Ltf5E3MCMUdPDV8JXH4HmJsrmvsGw0Ue583zpee6c4fHXt4bWV8HHs6DoRiqWlsCNm1y3HrjLKDYvcucje4/2LgU4HSrWwEv/Q0NKPqm+gBspumONG1xywo9d0ptwsRsVOeZ8mP8Q+TU73DD+IVNdK2vho65OTfUw99eAuK7MlAxO2LkO3sQNUBlcBJs+gHFfh/f+7BJganZLgpqQORQCF7lLDXaVuK7HMRe4GVoGTXatztB2AtfaW/QPl0zHXugSNbiRoOJr25prrHXdmevmQs+RrmWq+oW5U4J96sznxidC9zQ3sOGIftn7jC8urqCo6HgA6hqb2banruWmxdX1TWyoqCG7WxIVe+spW7uSU4+fxLY99WzeVcvq7VV8sH4nsz/cAsDg/Axmf7iFqvomAP6z9sMOrycS+abISXNf5oh+2ST5ffTMTGFvfRN5GSkU5KXTLclPRkoABBaXNNL08TZSknxs21NPXkYy4wfmuDL8YkkuEn8A9SVFn/kDYPr9cMrtrmVHSWtyArfT+oeduwukuNGVBVNg90Y+bBrOUWde6uJqdrpklzPItfZE4MQbAYWd61kX7MvgGd6sLIXHw5In3OUVKZmuVVW9HR46BYD1BTMoLBwCpR+4pJXZx7Xm+k10lx/sKXMDUUTwzX/CjcgEdxF9sAne+r17vPIFmPsbN3XYmAsYsm4lrLoFtix169+7F878nXuN5c+5Sz5O+hk07mVQyXsw9xzoOQq2fejOCYbulvDNlyBvaNvtqOp+whNc2WI36nTUOa2jcIGMqjWwJcddgtGF3Zv2iTEHhdQkP4Ny09ssC09yxbtXM7RnJkN7Rh/qrKqowty5xfQbOZGtlXX0zEqhW5KfXTWNvLx8Kz0zU/D7fVTVNQKwecN60vP7s6x0N8EgfLi5koyUAMvL9vDUwtIOr/HEygVtHocnvazUAKlJflKSfDTX15Gz7E3SkwOcMDyPvt27Ud8UpL6xmZz0ZAblplPX2MzAHmk0NSubq4Js31NHY1DJz0ghOXAI3WvUH/CS035ISoXT/h97i4tbD7BpPVpbFqFloRGZFz/DxuJiBoeeP+qc1oEx0DpP5XfmQVouGxYsp/DEotb1jbWw/FnXoms33H4Bx1A0aYxrGWX1czO2zP6xmz6sfIW7kH37Cpj3W/r6UiE1HWbMhJ6Hw1PfbJlImvEXu+7PZ74FQCG4pFS+yk0uXfKmi0vJgoemepNIL3Ov60/mhMrNMC8IYy50rbjyle4Si4YqN3DmoqfdQJkPn+bIhT+GhUF3reKFj3d+wMkBZgnKHDJExButLgzvlcnwXq0HkkG5MG5A9w7PKS4upajo8Ijl1TQ00dAUpLK2kaDCh4s+YODh46lvbKZnVipbK+t4b10FqUl+mpqD7Kiud0moKcimsq1kZ6WyY28Dd738Safqf/PbrwHQOyuVcQO6U9/UTJLfx5rt1eRmJJOXkUJ9UxD21vNW9cekpwQYlJtG3+7dSPL7aGgKcuSgHDbu3Mug3HSS/IdQkjtQeo6IvDypm+viiyY8OeYNg0uea7teFZrqefPt99rO4nL5HHjnD66VNOIMN83Y9hWQ0YtFb73KhLOvdMm2udG17vJHuNbhnJ/C+rkucXXrDo11lGYeycCe2W60o/hcl2ifsXDW7901bc9f5wbMvPFLKrMPp/v4s6H4V/Dvb7lrFivWusEnh53ukvcLP3R/U/Bpt+Y+xTVBichpwB8AP/CQqv663fppwB1AEGgCrlPVt0QkFZgHpHh1fFpVb41nXY3ZX2nJAdKSaem23JAsbZJcYV46xw6JPKltcXExRUVHAbBtTx11jc2kBPwkB3yU7a6lbHctKUl+Nu+qJTngY+0nK+lbMBREeH5pGSu37iE1yU9NQzOH98lkd00jn2yrIjngZ/32Jt7dupG6puYOXZZJfqGxWRnYIw0R2LyzBt8r/2VYrwzGD+zOpp21jO6XTVCVusYgw3plMLRnBnWNzayoaKawYi/b9tSzfkc1Ewt6MLBHmiW6A0GkbddlSCDZnTML6ZbjzlsBe7I3tbYE/Unw7ddbz1Fd9FSHotYVFzOwqAhGnu1mPBl4dOvKaffC385282IOPoll/a7mhKIvuwvA3/uzG/0YSHWXFMz5Kbz8c9BmWP0KWeN+SczJlz+DuCUoEfED9wKnAqXAfBGZparhw7heA2apqorIGOBJYARQD5ysqtUikgS8JSL/VdX34lVfY7pKr6y2B6Ye6ckdztEVV62h6NgCAC72RltGU+zNpVjf1MymnbVsrayjMRikrqGZd9ZWUJCXzpzlW+neLYlRWY0MGDiA4pXlPLWglP453Zj7STl+n5Ds91Hb2Nym7DvnF7d5LOIGzeSkuQER3ZID+AR2767lqbJFZKUm0Tc7lbqmZgbkpFG2u5bB+RkMzk9n/Y69pCcH2FUT5MPSSnbXNjAkP4M+2anxme3ki66z3XBDTu64rP9E+PFq113Y6wiCb73jln/pf+HkW9yAjW45rrt1wztuUujR50Pxrxm04Sng6gP2NsLFswU1CVijqusARGQmMA1oSVCqWh0Wnw6ot1yB0Lok7yfC6WtjTDQpAT9De7oWUMjpo90sGVdMcZcPuGR2ODeeNoKguqH+jc1BAt51aOt37GXTrlpSAj4WLFpC78LDyEgJMDg/ncUbd1G2u46tlXXsrm1AEPY2NNEcVBqS4OOyPeypbaRib0PUASgt5r3V8mdqko/05AADeqShqlTWNtIjPZn1O/ZSkBFkXWA9Cuza20Baip9+3btR29DMjup6emam0jMrhb7duzHMe9+NzUF8Ivjt2rrYktMjT0QcSG5765lBk1tacQyYxPLFqzkhTlWKZ4LqB2wKe1wKHN0+SETOAX4F9ATODFvuBxYCQ4F7VfX9SC8iIlcCVwIMHGh3fTXm0xAR/N7xO7zLzrV23IG+bqOfoiP7t6wLP4fXXqgVB256roDPx4aKveRnpvDB+p1U1zcxqm8WO/c2MvuthRw7YTSZqQHWbK9m084a9jY088nWKkAY1iuT8qp6Th7Ri3krNvOLF9x3XJ9AMEbSK8xLp7Guhm2vvIRPhMK8dIb2zKBHejJ7ahuZVJjLhoq9BFUpyEtn27Ym1r21Hp9AekqAXTUNfG3iAFRh3upyRvXNbpPsDdBjMEF/hBlUDpB4JqhIX1c6/Dup6rPAsyJyAu581Cne8mZgnIh099YfoaofRXj+A8ADABMnTrRWljEJJiXgup5CiW7q4b3arK/ZkETRKHcxcWgar2jeeGMnh084ltQkH1mpSdQ1NbspvXw+emenUl5Vz7Y9dazeXs3sD7ewZ3cdZx45iGBQWVu+lyWbdrO7ppHkgI/nlpSR7PchghtcArC47YXkv/rvyjYtv95ZqUwekstHJbXcOv8NBuWmU9vQxHFD8yjbXYvfJxTkptO3ezcqaxtpaApSXd5E3uZK3l+/E7/AmAHd6ZvdjZ6ZKWyvqqeytpGBPdLolmwXlrcXzwRVCoTPxd8fKIsSi6rOE5EhIpKnqjvClu8WkWLgNKBDgjLGHDpEhN7Zrefs0pIDbS4tGNAjjQE90phY0IMZkwa2dGG219QcZN2OvRTkppPkF0p31fLam+9yWtFxNDYHqaprQgRmf7iFJL+PY4fksmLLHhaU7OLVFdtIQhlXkMGWyjqCqtzz6mryMtxgmR3VDR1e73cL3+qwLCs1wJ46d11eapKP3lmpbN1dw5iV79IjPRm/T/D5hL7ZqSju3GRBbjqVtQ00B+GV5fU05G+lOahkpAYY3iuTnXsbSE8OMDA3rc1r1TQ0tdzR4GASzwQ1HxgmIoXAZuBCoM04TBEZCqz1BklMAJKBChHJBxq95NQN16q6M451NcYcQgJ+X5suygE90ijI9rdJfuDuhB1yVEEPLjm2gGBQmTu3mJNOcqMwVZU9tU1kdQsgIuza20B5dT2ZqQFSAn7+OXsevQqGU3RYT1RhyaZdlFc38FFpJf1yulGQl86Ckp1srayjMK2BapR1O6ppDirNQWXOR3Ug0BBq5YXeg8Ab/1jY4b35xNW1qq6J0l01ZHVLonRXLd0CcHLZIvp2T2V3TSOVtY00B5WeWSn0ykqlV1YqPTNT8IlQVhWkdFcNlbWNBHw+kgM+tu2pY8LAHAJe4gy99+ZY/ayfUdwSlKo2ici1wBzcMPNHVHW5iFzlrb8fOA+4REQagVrgAi9Z9QH+5p2H8gFPquoLkV/JGGM+Pz6ftBllKCJkp7VO6puTnkxOenLL4zH5AYqOaj0/flp2nw5lnj3WzXLvWnyT26xrDio+gc27a6mobqBHejJ1jc2ULF9AtwGj6Z6WRHV9Eyu27CEjJcDq7dUsKNlJXmYK4wd2Z3dtI+dN6M/iVet5b10FNQ3NdE9LIrtbEj4Rlpbujtjq4+03OixK9vtoaA6SmuQjqC5pTurtZ2qEgYEHQlyvg1LV2cDsdsvuD/v7TiK0jFR1GTA+nnUzxpiDQWj0Yf+cNPrntHbdbV4hTBnWes7umMGRr7kLKU4q48QTT4w4hL+hKUh5dT3lVfUEVXnprYUMGTac7G5J1DcFqW1opntaMos27iI1yU9dY3PLpM3NFQfnIAljjDEJJNr1ZckBH/26d6Ofd4uePX3btvpCTjui48z4xcVRhxZ8ZnYJuDHGmIRkCcoYY0xCsgRljDEmIVmCMsYYk5AsQRljjElIlqCMMcYkJEtQxhhjEpIlKGOMMQlJDsYJBKMRkXJgw6d8eh6wY59R8Y1NlHokQmyi1CMRYhOlHgdbbKLUIxFiE6kekQxS1fwOS1XVflySXtDVsYlSj0SITZR6JEJsotTjYItNlHokQmwi1WN/fqyLzxhjTEKyBGWMMSYhWYJq9UACxCZKPRIhNlHqkQixiVKPgy02UeqRCLGJVI9O+0INkjDGGPPFYS0oY4wxCckSlDHGmMQUr+GBB9MPcBqwClgD3Nhu3SPAduCjsGU9gFeA1d7vHG/5AOANYAWwHPhBtHggFfgAWOrF3h6rbG+dH1gMvLCPepQAHwJL8IaAxojtDjwNrPTqfWyM2MO8MkM/e4DrYsT/0HtvHwFPeO85WuwPvLjlwHXt6lwJNAAfd2IfPAJUA/XePv0ycL5XbhCY2H6ILNDkxX/ZW/Zbb3ssA54Fusco+w4vbgnwMtA3WmzYa94AKJAXo9zbgM1h2/qMWOUC3/MeLwd+E6Pcf4WVWQIs2ce2GAe858UvACbFKHss8C7uf+95ICvsc7HW24f1wJ9jfC5Cn6HtXuy2aPswRmyH/RcjtsP+ixYbaf/FKLfD/otVbpT9F227ddiHMerRYf/FiO2w/2g9Tm3xYnd4sbGOUzfhjqVt/u8/1bG5q5NDV//gDvprgcFAMi5hjAxbfwIwgbYJ6jd4iQy4EbjT+7sPMMH7OxP4BBgZKR4QIMNblgS8DxwTrWzv8fXA47QmqGj1KME7+HWizn8DvuX9nYz7MEetQ7vtthUYFOX99QPWA9285U8Cl0WJPQKXnNJwd3l+FRgWivX2wR+B8k68n4u97b4cKPT27Shcci0mLEF5+2YNcLT3YVrrva8vAQEv5s59lN09rLzvA/fHiPXjDg5zcBeU58WIvR24IcJ2jxQ71dtmKV5Mz1h1CCvrbuCWfWyLl4HTvZgzgOIYZc8HTvTWXw7cEfYZ+gRI8fZ1g/c70v9CH+A83OcwF1gHbIy0D2PEntZ+/8WI7bD/YsR22H8xYjvsvxix0fZfxO3Wrsy7gVtilN1h/8WI7bD/cMepiV5sOi7RleK+BET6/I30YlOI8D+3vz/Wxee+UaxR1XWq2gDMBKaFVqrqPGBnu+dMwx3Y8X5P92K3qOoi7+8qXIukX6R4daq9ZUnej0YrW0T6A2cCD+2rHlF0iBWRLNyH4GGvzg2quruT5U4F1qrqhhjxAaCbiARwyacsSuzhwHuqWqOqTcBc4JxQrLcP/oH7Rrev994f961ZVXU97qCbpaqromyTh3HfIBu92Emq+rJXD3DfPvvHKPvwsPLScfswWuwk4PfAT8LiosX2i1DfaLE/BX6tqvW4Fdv3UQfE3fv7a7iWbdRt4dUztN2zcfswWtkjgXne+ldwB0GA44C/qmq9qn4E7AZOJvLnYgswHJipqhW4b/PbibAPY8RWtt9/MWI77L8YsR32X4zYDvsvRmy0/RdtuwFt92Gs7Ua7/RcjtsP+U5d1TsUdFxXXgi31XjfS52+aV259+/+5T8MSlPtH2hT2uJToB4eQXt5ODv3T9WwfICIFwHhcyyhivIj4RWQJ7p/jFVWNGgvcg/tgBDtRDwVeFpGFInJljNjBQDnwVxFZLCIPiUh6Z94fcCGtB7cO8aq6GbgL981sC+6g8XKUsj8CThCRXBFJo7U7pCXWq2egE++9n/d6IbH2Z2f2/eXAf2OVLSK/FJFNwEW4b7PRYqcBm1V1aYR6tI/NAq4VkWUi8oiI5MSIHQwcLyLvi8hcETkqVn29v48Htqnq6rDYSNviOuC33vu7C9d9E63sUuBs7/H5uH3Ypmzvc5EG7CL2PtwU9hn6mH3swxix7fdfh9gY+699bKz91z421v5rHxtr/0XabiER92G7sp8g+v5rHxtt//UHrsI7TuFazXmx9l1YHTtzPI3KEpRrwranEZZ1vkCRDODfuHMpe6LFqWqzqo7D/QNMEpEjopR3FrBdVRd2sgrHqeoE4HTgGhE5IUpcANd9+WdVHQ/sxTXXYxKRZNw/8lMxYnJwH+hCXL9+uoh8I1Ksqq7AdcO8AryE6yJoihTbCfuzP2PGisjNXj0eixWvqjer6gAv7toosX7gXFoPgPuqx0vAENw5hC24rpxosX7c+ZtjgB8DT3rfrmO9vxm0fsGIVq4CVwM/9N7fD/Fa21Hi78X9vy3EdXE3hMeGfS7eA2oiPD+8Lile7HW4Fl2sfRgxNsr+6xAbY/+FxwaJvf/alxtr/7WPjbn/Ymy3SPuwfdlfIvr+ax8bbf8B3Ix3nPLqGs0BPZ5agnIZfkDY4/60dmNEs01E+gB4v0NNckQkCbfTH1PVZ/YVD+B1qxXj+s4jxR4HnC0iJbim9ski8s9o5apqmfd7O64bZlKU2FKg1Gu5gRssMWFf9cUlvkWqui3G+zsFWK+q5araCDwDTI5R54dVdYKqnoDrUl0dHgvk0zZpRatjKa6PPSTW/oy670XkUuAs4CKvm6MzZT9Oa7dW+9hhuHMWS7392B9YJCK9o5S7wvsCEwQepLWbJFodnvG6jT/AHUzzosV6Xa7n4k6272tbXIrbd+C+kMSqx3xV/ZKqHok7cK4Nix2E97kAmr2yo+3DMuB/aP0MxdqHEWOj7L99lRu+/9rHxtp/kcqNtv+i1SHa/ou03YiyDyOVfSKR91+k2Fj7b0DYcWoUsCPG529/j6dRWYJyJwaHiUih1zK4EJi1j+fMwn1w8X7/B1r6hB/G/XP+Lla8iOSLSHfved1wB/SVkWJV9SZV7a+qBV79XlfVb0QpN11EMr1y03HfoD6KUu5WXDP/MG/5VFxTP+L7C9P+m1uk+I3AMSKS5m2XqbhzctG2XajbcyDug/dEu9iv4kYNxnrN0PKvuKKkEHdg+YDIZuG2ZzLuHOAw4AMROQ334T1bVWvaxbcvO/z85Nm4fRgptg+Qr6oF3n4sxQ2o2Rql3I1h5Z6D24fR6vA3vHMTIjLcez87YmyLU4CVqlq6r22BO7ic6MWcjPviEK0e6706+ICf4QYchGJ/hDvh/2xY2ZH+f4XWb+n3xtqHMWJzaLf/YsR22H9RYiPuP9w5u0jldth/MeoQa/9F2m7Qbh/GKHsT7fZfjNgO+09E8nEj/i4UkWzcefBsXLKL9vm7UERSOvH52zdNgJF0Xf2DO+fxCe4bw83t1j2Ba6I34v4pr8CNfHkN92F9DejhxU7BNWdDw1aXeGV3iAfG4IaML8MdfEKjqSKWHVafIlpH8UUqdzCui2wprq/45ljl4rogFnj1eA73Txu1Drh+8AogO2xZtLJvxx2wP8INckiJEfsmLjkuBaa2K7cKN8S1M/vgCS9evfjf4w4QpbQOqZ0TVvdQd6LiznNdgTuxuylsH94fo+x/e+9vGW5obr9ose32Ywmto/gilfsP3MnrZbgPfZ8YscnAP716LAJOjlUH4FHgqgifg0jbYgqw0Fv3PnBkjHr8APc5+gT4Na0z1YQ+F/VAHe5zFu1zEYrdGhb/80j7MEZsh/0XI7bD/osWG2n/xSi3w/6LERtt/0XcbpH2YYyyO+y/GLEd9h+tx6lQ7A5cD0qsY8TNXl1X4Y0g/LQ/NtWRMcaYhGRdfMYYYxKSJShjjDEJyRKUMcaYhGQJyhhjTEKyBGWMMSYhWYIyhzQRURG5O+zxDSJy2wEq+1ER+eqBKGsfr3O+iKwQkTfaLS8QkVoRWRL2c8kBfN0iEXnhQJVnTHuBfYcY84VWD5wrIr9S1R1dXZkQEfGranMnw68Avquqb0RYt1bddFrGHHSsBWUOdU3AA7h5ytpo3wISkWrvd5G4ST2fFJFPROTXInKRiHwgIh+KyJCwYk4RkTe9uLO85/tF5LciMl/cZKLfCSv3DRF5HHeRZ/v6zPDK/0hE7vSW3YK78PJ+EfltZ9+0iFSLyN0iskhEXvNmDEBExonIe169nhVvklMRGSoir4rIUu85ofeYISJPi8hKEXnMm6XAmAPCEpQxbpLMi7ypXDprLO7K+9G4eyMNV9VJuNuhfC8srgA31cyZuCSSimvxVKrqUcBRwLe9aWHATUFzs6qODH8xEemLm1D3ZNzsH0eJyHRV/QVuJpCLVPXHEeo5pF0X3/He8nTcfIoTcLc3udVb/nfgf1R1DC5JhpY/BtyrqmNxcyqGZjIfj5tsdCRuFpPj9rnljOkk6+IzhzxV3SMif8fdsK62k0+br97tBkRkLe7GcOAO6ieFxT2pbsLQ1SKyDhiBmx9xTFjrLBs3Z1kD8IG6++i0dxTuZoHl3ms+hruX13P7qGe0Lr4grRON/hN4xkvQ3VV1rrf8b8BT4uZ27KeqzwKoap1XB7z6huaDW4JLyG/to07GdIolKGOce3DzoP01bFkTXi+D13WVHLauPuzvYNjjIG0/V+3nElPcHGffU9U54StEpAh3y5NI4t11FmvOs1ivHb4dmrFjijmArIvPGEBVd+JuS39F2OIS3OSa4O5tlfQpij5fRHzeOZvBuAk05wBXi7s1CyIyXNzM87G8D5woInki4sfNKD93H8+JxYebIR7g68BbqloJ7ArrBrwYmKvunmalIjLdq2+KuBtLGhNX9m3HmFZ303rDOnD38fmPiHyAm7E5WusmllW4RNILN/t0nYg8hOsKW+S1zMppvWV2RKq6RURuwt36QIDZqtr+NiiRDPG63kIeUdU/4t7LKHE3p6sELvDWX4o7V5YGrAO+6S2/GPiLiPwCN3v5+Z14bWM+E5vN3JhDkIhUq2pGV9fDmFisi88YY0xCshaUMcaYhGQtKGOMMQnJEpQxxpiEZAnKGGNMQrIEZYwxJiFZgjLGGJOQ/j9uXEdOrKeYsgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#input parameter\n",
    "lr = 1e-4\n",
    "epoch = 300\n",
    "conv_dropout_rate=0.4\n",
    "dense_dropout_rate=0.7\n",
    "weight_decay=1e-4\n",
    "######################################\n",
    "\n",
    "model = CNNModel(\n",
    "    in_channels=1,       # e.g. 1 channel, like a single feature dimension\n",
    "    num_classes=16,      # matches the reference figure\n",
    "    additional_input_dim=1,\n",
    "    return_logits=False  # set True if you need raw logits\n",
    "    ).to(device)\n",
    "\n",
    "# model = Model( #! way too memory intensive\n",
    "# num_classes=13,\n",
    "# num_filters=128,\n",
    "# num_conv_layers=2,\n",
    "# num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "# num_dense_layers=2,\n",
    "# return_logits=True,\n",
    "# conv_dropout_rate=0,\n",
    "# dense_dropout_rate=0\n",
    "# ).to(device)\n",
    "## early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "patience_counter = 0\n",
    "lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "batch_size = 64\n",
    "# lr = 0.0085\n",
    "# lr = 0.00002\n",
    "lr = lr\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "# test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = masked_weighted_MAE\n",
    "# criterion = masked_weighted_MSE\n",
    "criterion = weighted_cross_entropy_loss_fn\n",
    "# criterion = masked_MAE\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "# scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "#%%\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "# ic.enable()\n",
    "ic.disable()\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    # print(f'Epoch {e}')\n",
    "    for x_train, y_train, y_train_res in train_loader:\n",
    "        x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "        y_batch = y_train.to(device)\n",
    "        y_batch_res = y_train_res.to(device)\n",
    "        \n",
    "        x_batch = x_batch.float()\n",
    "        pred = model(x_batch.float(),y_batch_res.float())\n",
    "\n",
    "        # break\n",
    "        # loss_train = loss_corn(pred, y_batch, 3, class_weights)\n",
    "        # print(pred, y_batch)\n",
    "        loss_train = criterion(pred,y_batch)\n",
    "        # print(pred)\n",
    "        # print(y_batch)\n",
    "        # print(loss_train)\n",
    "        train_batch_loss.append(loss_train)        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update the learning rate\n",
    "        # break\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # print('>> test')\n",
    "        for x_test, y_test, y_test_res in test_loader:\n",
    "            x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            y_batch = y_test.to(device)\n",
    "            y_batch_res = y_test_res.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float(), y_batch_res.float())\n",
    "            loss_test = criterion(pred,y_batch)\n",
    "            # pred = pred.unsqueeze(0)\n",
    "            # print(pred[:10])\n",
    "            # print(y_batch[:10])\n",
    "\n",
    "            # loss_test = loss_corn(pred, y_batch, 3, class_weights)\n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "    if e%50 == 0:\n",
    "        print(f'Epoch {e}')\n",
    "        print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "        print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    # #! implementing early stopping\n",
    "    # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "    # print(f'Current val loss: {current_val_loss}')\n",
    "    # print(f'Best val loss: {best_val_loss}')\n",
    "    # if current_val_loss < best_val_loss:\n",
    "    #     best_val_loss = current_val_loss\n",
    "    #     patience_counter = 0  # reset patience counter\n",
    "    #     # Save the best model\n",
    "    #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         torch.save({\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #         'model': model.state_dict(),\n",
    "    #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "    #         break  # Early stopping\n",
    "    \n",
    "print('==='*10)\n",
    "# torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "             train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, epoch+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "#%%\n",
    "# testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "# testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "# testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "# Ensure the model is on the same device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test, y_test_res in testing_loader1:\n",
    "        # Move input and target data to the correct device\n",
    "        x_test = x_test.to(device).float()\n",
    "        y_test = y_test.to(device).float()\n",
    "        y_test_res = y_test_res.to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(x_test, y_test_res)\n",
    "        \n",
    "        # Append predictions and targets to lists\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "        \n",
    "# Flatten the target list\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min.values-1, target_max.values+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "# Calculate AUC\n",
    "cutoff = 4\n",
    "test_target_bi = (target_list >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8834329999656323\n",
      "Sensitivity: 0.912568306010929\n",
      "Specificity: 0.8542976939203354\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate AUC\n",
    "cutoff = 4\n",
    "test_target_bi = (target_list >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doubling Dilution Accuracy: 0.8830255057167986\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min.values-1, target_max.values+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved_model1115/emb_resFeed_working17122024.pth'\n",
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_y = []\n",
    "errors_y_ = []\n",
    "for y_, y in zip(pred_list, target_list):\n",
    "    # if y_ != y:\n",
    "    if y not in [y_, y_+1, y_-1]:\n",
    "        errors_y.append(y)\n",
    "        errors_y_.append(y_)\n",
    "        \n",
    "for a, b in zip(errors_y, errors_y_):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7.,  0., 48.,  0.,  2.,  0.,  3.,  0., 51., 12.]),\n",
       " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(errors_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = []\n",
    "for a, b in zip(errors_y, errors_y_):\n",
    "    joint.append(f'{a}_{b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, '')])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(15, 6), dpi=80)\n",
    "plt.hist(joint, width=0.8)\n",
    "plt.xticks(rotation=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([13.,  0.,  9.,  0., 44.,  0., 42.,  0.,  9.,  6.]),\n",
       " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ],\n",
       "       dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(errors_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0 1\n",
      "2.0 4\n",
      "2.0 5\n",
      "3.0 5\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "2.0 4\n",
      "5.0 1\n",
      "4.0 1\n",
      "3.0 1\n",
      "0.0 4\n",
      "2.0 0\n",
      "3.0 1\n",
      "4.0 1\n",
      "2.0 0\n",
      "3.0 1\n",
      "2.0 0\n",
      "5.0 1\n",
      "3.0 1\n",
      "2.0 4\n",
      "5.0 1\n",
      "3.0 1\n",
      "3.0 1\n",
      "0.0 5\n",
      "1.0 5\n",
      "2.0 4\n",
      "0.0 3\n",
      "0.0 4\n",
      "2.0 4\n",
      "1.0 4\n",
      "1.0 4\n",
      "3.0 1\n",
      "0.0 3\n",
      "3.0 1\n",
      "2.0 4\n",
      "0.0 2\n",
      "3.0 1\n",
      "0.0 4\n",
      "1.0 3\n",
      "4.0 1\n",
      "3.0 1\n",
      "0.0 4\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "3.0 5\n",
      "2.0 4\n",
      "4.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "3.0 1\n",
      "3.0 1\n",
      "3.0 1\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 0\n",
      "4.0 1\n",
      "2.0 4\n",
      "3.0 5\n",
      "0.0 4\n",
      "2.0 4\n",
      "0.0 4\n",
      "2.0 4\n",
      "2.0 4\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "3.0 1\n",
      "5.0 0\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "2.0 4\n",
      "0.0 5\n",
      "3.0 1\n",
      "2.0 4\n",
      "3.0 1\n",
      "3.0 1\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "3.0 5\n",
      "2.0 4\n",
      "3.0 1\n",
      "3.0 1\n",
      "1.0 4\n",
      "2.0 5\n",
      "3.0 5\n",
      "3.0 1\n",
      "2.0 0\n",
      "2.0 4\n",
      "3.0 0\n",
      "0.0 4\n",
      "1.0 4\n",
      "3.0 1\n",
      "3.0 5\n",
      "4.0 1\n",
      "3.0 1\n",
      "2.0 4\n",
      "3.0 1\n",
      "2.0 4\n",
      "3.0 1\n",
      "4.0 1\n",
      "3.0 5\n",
      "3.0 1\n",
      "3.0 1\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "4.0 1\n",
      "2.0 4\n",
      "3.0 1\n",
      "2.0 4\n",
      "0.0 2\n",
      "1.0 4\n",
      "2.0 4\n",
      "1.0 4\n",
      "4.0 1\n",
      "5.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "1.0 4\n"
     ]
    }
   ],
   "source": [
    "for a, b in zip(errors_y, errors_y_):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved_model1115/resFeed1.pth'\n",
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "Optimizer details:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "Learning rate: 0.0001\n",
      "Weight decay: 0.0001\n",
      "======================\n",
      "Accuracy: 0.5224274406332454\n",
      "Mae: 0.503957783641161\n",
      "F1 Score: 0.48083488925470447\n",
      "conf_matrix: [[ 18  86   8   0   0   0]\n",
      " [ 48 330  19   0   0   0]\n",
      " [ 15 230  51   0   0   0]\n",
      " [  0   0   0 104  44   1]\n",
      " [  0   0   0  52  65  20]\n",
      " [  0   0   0   6  14  26]]\n",
      "======================\n",
      "Doubling Dilution Accuracy: 0.9736147757255936\n",
      "AUC: 0.8179451489844314\n",
      "Sensitivity: 0.6830601092896175\n",
      "Specificity: 0.9528301886792453\n"
     ]
    }
   ],
   "source": [
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "# Ensure the model is on the same device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test, y_test_res in testing_loader1:\n",
    "        # Move input and target data to the correct device\n",
    "        x_test = x_test.to(device).float()\n",
    "        y_test = y_test.to(device).float()\n",
    "        y_test_res = y_test_res.to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(x_test, y_test_res)\n",
    "        \n",
    "        # Append predictions and targets to lists\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "        \n",
    "# Flatten the target list\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "\n",
    "# Calculate AUC\n",
    "cutoff = 4\n",
    "test_target_bi = (target_list >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypterparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 50/400 [02:56<20:30,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Training loss: 0.1510869264602661\n",
      "Validation loss: 0.13646160066127777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 100/400 [05:52<17:28,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100\n",
      "Training loss: 0.12380748987197876\n",
      "Validation loss: 0.11622773110866547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 114/400 [06:41<16:51,  3.54s/it]"
     ]
    }
   ],
   "source": [
    "#input parameter\n",
    "for _ in [1e-4, 5e-4,1e-3, 5e-3,1e-2]:\n",
    "    lr = 1e-4\n",
    "    epoch = 400\n",
    "    conv_dropout_rate=0.4\n",
    "    dense_dropout_rate=0.7\n",
    "    weight_decay= _ #1e-4\n",
    "    ######################################\n",
    "\n",
    "    model = Model(\n",
    "    num_classes=6,\n",
    "    num_filters=64,\n",
    "    num_conv_layers=2,\n",
    "    # num_dense_neurons=256, # batch_size = 64\n",
    "    num_dense_neurons=128, # batch_size = 64\n",
    "    num_dense_layers=2,\n",
    "    return_logits=False,\n",
    "    conv_dropout_rate=conv_dropout_rate,\n",
    "    dense_dropout_rate=dense_dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "    # model = Model( #! way too memory intensive\n",
    "    # num_classes=13,\n",
    "    # num_filters=128,\n",
    "    # num_conv_layers=2,\n",
    "    # num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "    # num_dense_layers=2,\n",
    "    # return_logits=True,\n",
    "    # conv_dropout_rate=0,\n",
    "    # dense_dropout_rate=0\n",
    "    # ).to(device)\n",
    "    ## early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "    patience_counter = 0\n",
    "    lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "    batch_size = 64\n",
    "    # lr = 0.0085\n",
    "    # lr = 0.00002\n",
    "    lr = lr\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "    test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "    # train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "    # test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "    # criterion = nn.MSELoss()\n",
    "    # criterion = masked_weighted_MAE\n",
    "    # criterion = masked_weighted_MSE\n",
    "    criterion = weighted_cross_entropy_loss_fn\n",
    "    # criterion = masked_MAE\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "    # scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "    #%%\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc; gc.collect()\n",
    "    # ic.enable()\n",
    "    ic.disable()\n",
    "\n",
    "    train_epoch_loss = []\n",
    "    test_epoch_loss = []\n",
    "\n",
    "    for e in tqdm(range(1, epoch+1)):\n",
    "        model.train()\n",
    "        train_batch_loss = []\n",
    "        test_batch_loss = []\n",
    "        # print(f'Epoch {e}')\n",
    "        for x_train, y_train, y_train_res in train_loader:\n",
    "            x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "            y_batch = y_train.to(device)\n",
    "            y_batch_res = y_train_res.to(device)\n",
    "            \n",
    "            x_batch = x_batch.float()\n",
    "            pred = model(x_batch.float(),y_batch_res.float())\n",
    "\n",
    "            # break\n",
    "            # loss_train = loss_corn(pred, y_batch, 3, class_weights)\n",
    "            # print(pred, y_batch)\n",
    "            loss_train = criterion(pred,y_batch)\n",
    "            # print(pred)\n",
    "            # print(y_batch)\n",
    "            # print(loss_train)\n",
    "            train_batch_loss.append(loss_train)        \n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()  # Update the learning rate\n",
    "            # break\n",
    "        train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # print('>> test')\n",
    "            for x_test, y_test, y_test_res in test_loader:\n",
    "                x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "                x_batch = x_batch.float()\n",
    "                y_batch = y_test.to(device)\n",
    "                y_batch_res = y_test_res.to(device)\n",
    "                # print(x_batch.size())\n",
    "                # y_batch = torch.Tensor.float(y).to(device)\n",
    "                # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "                pred = model(x_batch.float(), y_batch_res.float())\n",
    "                loss_test = criterion(pred,y_batch)\n",
    "                # pred = pred.unsqueeze(0)\n",
    "                # print(pred[:10])\n",
    "                # print(y_batch[:10])\n",
    "\n",
    "                # loss_test = loss_corn(pred, y_batch, 3, class_weights)\n",
    "                test_batch_loss.append(loss_test)\n",
    "            test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "        if e%50 == 0:\n",
    "            print(f'Epoch {e}')\n",
    "            print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "            print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "        # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "        # print(train_batch_loss)\n",
    "        # print(test_batch_loss)\n",
    "        # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "        # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "        # #! implementing early stopping\n",
    "        # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "        # print(f'Current val loss: {current_val_loss}')\n",
    "        # print(f'Best val loss: {best_val_loss}')\n",
    "        # if current_val_loss < best_val_loss:\n",
    "        #     best_val_loss = current_val_loss\n",
    "        #     patience_counter = 0  # reset patience counter\n",
    "        #     # Save the best model\n",
    "        #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     if patience_counter >= patience:\n",
    "        #         print(\"Early stopping triggered\")\n",
    "        #         torch.save({\n",
    "        #         'optimizer': optimizer.state_dict(),\n",
    "        #         'model': model.state_dict(),\n",
    "        #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "        #         break  # Early stopping\n",
    "            \n",
    "    print('==='*10)\n",
    "    # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "    save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "                train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    x = np.arange(1, epoch+1, 1)\n",
    "    ax.plot(x, train_epoch_loss,label='Training')\n",
    "    ax.plot(x, test_epoch_loss,label='Validation')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Number of Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "    ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "    # ax_2 = ax.twinx()\n",
    "    # ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "    # ax_2.set_yscale(\"log\")\n",
    "    # ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "    ax.grid(axis=\"x\")\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "    print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "    #%%\n",
    "    # testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "    testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Ensure the model is on the same device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    ic.disable()\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    target_list  = []\n",
    "    mse_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test, y_test_res in testing_loader1:\n",
    "            # Move input and target data to the correct device\n",
    "            x_test = x_test.to(device).float()\n",
    "            y_test = y_test.to(device).float()\n",
    "            y_test_res = y_test_res.to(device).float()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(x_test, y_test_res)\n",
    "            \n",
    "            # Append predictions and targets to lists\n",
    "            pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "            target_list.append(y_test.detach().cpu().numpy())\n",
    "            \n",
    "    # Flatten the target list\n",
    "    target_list = np.array(target_list).flatten()\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "    from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "    def calculate_metrics(true_labels, predictions):\n",
    "        \"\"\"\n",
    "        Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "        Parameters:\n",
    "        - true_labels: List or array of true labels\n",
    "        - predictions: List or array of predicted labels\n",
    "\n",
    "        Returns:\n",
    "        - accuracy: Overall accuracy of predictions\n",
    "        - f1: Weighted average F1 score\n",
    "        - conf_matrix: Multiclass confusion matrix\n",
    "        - mae: Mean Absolute Error of predictions\n",
    "        \"\"\"\n",
    "        # Ensure inputs are numpy arrays for consistency\n",
    "        true_labels = np.array(true_labels)\n",
    "        predictions = np.array(predictions)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "        # Calculate MAE\n",
    "        mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "        return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "    # Example usage\n",
    "    # true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "    # predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "    accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "    print(\"======================\")\n",
    "    # print(\"Model's Named Parameters:\")\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     print(f\"Name: {name}\")\n",
    "    #     print(f\"Shape: {param.size()}\")\n",
    "    #     print(f\"Requires grad: {param.requires_grad}\")\n",
    "    #     print('-----')\n",
    "    print(\"Optimizer details:\")\n",
    "    print(optimizer)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"Learning rate:\", param_group['lr'])\n",
    "        print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "        \n",
    "    print(\"======================\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Mae: {mae}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"conf_matrix: {conf_matrix}\")\n",
    "    print(\"======================\")\n",
    "    doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "    print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "\n",
    "    # Calculate AUC\n",
    "    cutoff = 4\n",
    "    test_target_bi = (target_list >= cutoff).astype(int)\n",
    "    test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "    auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "    print(\"AUC:\", auc)\n",
    "\n",
    "    # Calculate confusion matrix components\n",
    "    tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "    # Calculate sensitivity (recall)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "    # Calculate specificity\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One cycle lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 50/400 [02:28<17:21,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Training loss: 0.11537401378154755\n",
      "Validation loss: 0.14441072940826416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 100/400 [04:58<14:56,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100\n",
      "Training loss: 0.11210744082927704\n",
      "Validation loss: 0.1536770612001419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 150/400 [07:28<12:28,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150\n",
      "Training loss: 0.11056072264909744\n",
      "Validation loss: 0.16507422924041748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 200/400 [09:57<09:53,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200\n",
      "Training loss: 0.1097438856959343\n",
      "Validation loss: 0.17116251587867737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 250/400 [12:32<07:39,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250\n",
      "Training loss: 0.10918270796537399\n",
      "Validation loss: 0.1748015433549881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 300/400 [15:02<04:53,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300\n",
      "Training loss: 0.10868095606565475\n",
      "Validation loss: 0.17704497277736664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 350/400 [17:41<02:47,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350\n",
      "Training loss: 0.10827459394931793\n",
      "Validation loss: 0.18684180080890656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [20:27<00:00,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400\n",
      "Training loss: 0.10818441957235336\n",
      "Validation loss: 0.17728930711746216\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_1e-07_weighted_balanced.png-emb\n",
      "======================\n",
      "Optimizer details:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    base_momentum: 0.85\n",
      "    betas: (0.9499999993756804, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    initial_lr: 0.0004\n",
      "    lr: 4.006243171376002e-08\n",
      "    max_lr: 0.01\n",
      "    max_momentum: 0.95\n",
      "    maximize: False\n",
      "    min_lr: 4e-08\n",
      "    weight_decay: 0\n",
      ")\n",
      "Learning rate: 4.006243171376002e-08\n",
      "Weight decay: 0\n",
      "======================\n",
      "Accuracy: 0.47568523430592397\n",
      "Mae: 0.6578249336870027\n",
      "F1 Score: 0.44584107920478183\n",
      "conf_matrix: [[ 34  66   7   3   1   1]\n",
      " [ 63 285  32   9   5   0]\n",
      " [ 40 188  41  19   3   2]\n",
      " [  4  20   7  75  42   1]\n",
      " [  2   8   0  34  77  16]\n",
      " [  0   3   0   5  12  26]]\n",
      "======================\n",
      "Doubling Dilution Accuracy: 0.8992042440318302\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#input parameter\n",
    "lr = 1e-7\n",
    "epoch = 400\n",
    "conv_dropout_rate=0\n",
    "dense_dropout_rate=0\n",
    "# weight_decay=1e-8\n",
    "weight_decay=0\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "patience_counter = 0\n",
    "lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "batch_size = 128\n",
    "# lr = 0.0085\n",
    "# lr = 0.00002\n",
    "lr = lr\n",
    "######################################\n",
    "\n",
    "model = Model(\n",
    "num_classes=6,\n",
    "num_filters=64,\n",
    "num_conv_layers=2,\n",
    "# num_dense_neurons=256, # batch_size = 64\n",
    "num_dense_neurons=128, # batch_size = 64\n",
    "num_dense_layers=2,\n",
    "return_logits=False,\n",
    "conv_dropout_rate=conv_dropout_rate,\n",
    "dense_dropout_rate=dense_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "criterion = weighted_cross_entropy_loss_fn\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=epoch)\n",
    "# model = Model( #! way too memory intensive\n",
    "# num_classes=13,\n",
    "# num_filters=128,\n",
    "# num_conv_layers=2,\n",
    "# num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "# num_dense_layers=2,\n",
    "# return_logits=True,\n",
    "# conv_dropout_rate=0,\n",
    "# dense_dropout_rate=0\n",
    "# ).to(device)\n",
    "## early stopping\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "# test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = masked_weighted_MAE\n",
    "# criterion = masked_weighted_MSE\n",
    "# criterion = weighted_cross_entropy_loss_fn\n",
    "\n",
    "\n",
    "# criterion = masked_MAE\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "#%%\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "# ic.enable()\n",
    "ic.disable()\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    # print(f'Epoch {e}')\n",
    "    for x_train, y_train in train_loader:\n",
    "        x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "        y_batch = y_train.to(device)\n",
    "        x_batch = x_batch.float()\n",
    "        pred = model(x_batch.float())\n",
    "\n",
    "        # break\n",
    "        loss_train = criterion(pred,y_batch)\n",
    "\n",
    "        train_batch_loss.append(loss_train)        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update the learning rate\n",
    "\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # print('>> test')\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            y_batch = y_test.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float())\n",
    "\n",
    "            # pred = pred.unsqueeze(0)\n",
    "            # print(pred[:10])\n",
    "            # print(y_batch[:10])\n",
    "\n",
    "            loss_test = criterion(pred,y_batch)\n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "    if e % 50 == 0:\n",
    "        print(f'Epoch {e}')\n",
    "        print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "        print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    # #! implementing early stopping\n",
    "    # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "    # print(f'Current val loss: {current_val_loss}')\n",
    "    # print(f'Best val loss: {best_val_loss}')\n",
    "    # if current_val_loss < best_val_loss:\n",
    "    #     best_val_loss = current_val_loss\n",
    "    #     patience_counter = 0  # reset patience counter\n",
    "    #     # Save the best model\n",
    "    #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         torch.save({\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #         'model': model.state_dict(),\n",
    "    #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "    #         break  # Early stopping\n",
    "        \n",
    "print('==='*10)\n",
    "# torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "             train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, epoch+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "#%%\n",
    "\n",
    "model.eval()  # For inference\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in testing_loader1:\n",
    "        xtest1 = x_test.to(device).float()\n",
    "        ytest1 = y_test.to(device).float()\n",
    "        pred = model(xtest1)\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclical lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input parameter\n",
    "lr = 1e-7\n",
    "epoch = 250\n",
    "conv_dropout_rate=0.05\n",
    "dense_dropout_rate=0.5\n",
    "weight_decay=1e-8\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-9, max_lr=0.001)\n",
    "######################################\n",
    "\n",
    "model = Model(\n",
    "num_classes=3,\n",
    "num_filters=64,\n",
    "num_conv_layers=2,\n",
    "# num_dense_neurons=256, # batch_size = 64\n",
    "num_dense_neurons=128, # batch_size = 64\n",
    "num_dense_layers=2,\n",
    "return_logits=False,\n",
    "conv_dropout_rate=conv_dropout_rate,\n",
    "dense_dropout_rate=dense_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# model = Model( #! way too memory intensive\n",
    "# num_classes=13,\n",
    "# num_filters=128,\n",
    "# num_conv_layers=2,\n",
    "# num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "# num_dense_layers=2,\n",
    "# return_logits=True,\n",
    "# conv_dropout_rate=0,\n",
    "# dense_dropout_rate=0\n",
    "# ).to(device)\n",
    "## early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "patience_counter = 0\n",
    "lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "batch_size = 128\n",
    "# lr = 0.0085\n",
    "# lr = 0.00002\n",
    "lr = lr\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "# test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = masked_weighted_MAE\n",
    "# criterion = masked_weighted_MSE\n",
    "# criterion = weighted_cross_entropy_loss_fn\n",
    "# criterion = masked_MAE\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "# scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "#%%\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "# ic.enable()\n",
    "ic.disable()\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    # print(f'Epoch {e}')\n",
    "    for x_train, y_train in train_loader:\n",
    "        x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "        y_batch = y_train.to(device)\n",
    "        x_batch = x_batch.float()\n",
    "        pred = model(x_batch.float())\n",
    "\n",
    "        # break\n",
    "        loss_train = criterion(pred,y_batch)\n",
    "\n",
    "        train_batch_loss.append(loss_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update the learning rate\n",
    "\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # print('>> test')\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            y_batch = y_test.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float())\n",
    "            loss_test = loss_corn(pred, y_batch, 3, class_weights)\n",
    "\n",
    "            # pred = pred.unsqueeze(0)\n",
    "            # print(pred[:10])\n",
    "            # print(y_batch[:10])\n",
    "\n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "\n",
    "    print(f'Epoch {e}')\n",
    "    print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "    print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    # #! implementing early stopping\n",
    "    # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "    # print(f'Current val loss: {current_val_loss}')\n",
    "    # print(f'Best val loss: {best_val_loss}')\n",
    "    # if current_val_loss < best_val_loss:\n",
    "    #     best_val_loss = current_val_loss\n",
    "    #     patience_counter = 0  # reset patience counter\n",
    "    #     # Save the best model\n",
    "    #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         torch.save({\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #         'model': model.state_dict(),\n",
    "    #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "    #         break  # Early stopping\n",
    "        \n",
    "print('==='*10)\n",
    "# torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "             train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, epoch+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "# ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "#%%\n",
    "testing_dataset = Dataset(test_data, test_target, one_hot_dtype=torch.float, transform=False)\n",
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, collate_fn=collate_padded_batch, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "model.eval()  # For inference\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in testing_loader1:\n",
    "        xtest1 = x_test.to(device).float()\n",
    "        ytest1 = y_test.to(device).float()\n",
    "        pred = model(xtest1)\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_corn(logits, y_train, num_classes):\n",
    "    sets = []\n",
    "    for i in range(num_classes-1):\n",
    "        label_mask = y_train > i-1\n",
    "        label_tensor = (y_train[label_mask] > i).to(torch.int64)\n",
    "        sets.append((label_mask, label_tensor))\n",
    "\n",
    "    num_examples = 0\n",
    "    losses = 0.\n",
    "    for task_index, s in enumerate(sets):\n",
    "        train_examples = s[0]\n",
    "        train_labels = s[1]\n",
    "\n",
    "        if len(train_labels) < 1:\n",
    "            continue\n",
    "\n",
    "        num_examples += len(train_labels)\n",
    "        pred = logits[train_examples, task_index]\n",
    "\n",
    "        loss = -torch.sum(F.logsigmoid(pred)*train_labels\n",
    "                          + (F.logsigmoid(pred) - pred)*(1-train_labels)\n",
    "                          )\n",
    "        losses += loss\n",
    "    return losses/num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred  = torch.tensor([[0.0000, 0.5111],\n",
    "        [0.1329, 1.1051]], device='cuda:0')\n",
    "target = torch.tensor([0, 0], device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7275, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "out = loss_corn(pred, target, 3)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost with snps and fed in res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "#     # Ensure target_min and target_max are scalars\n",
    "#     target_min = target_min.item() if isinstance(target_min, np.ndarray) or isinstance(target_min, pd.Series) else target_min\n",
    "#     target_max = target_max.item() if isinstance(target_max, np.ndarray) or isinstance(target_max, pd.Series) else target_max\n",
    "\n",
    "#     # Create a range based on the scalar values of target_min and target_max\n",
    "#     dilution_range = np.arange(target_min - 1, target_max + 2, 1)\n",
    "    \n",
    "#     # Find the index of the target value\n",
    "#     index = np.where(dilution_range == target)[0][0]  # Use np.where to find the index\n",
    "    \n",
    "#     # Check if prediction is within the acceptable range\n",
    "#     return dilution_range[index - 1] <= pred <= dilution_range[index + 1]\n",
    "\n",
    "# Example usage\n",
    "target_min, target_max = cryptic_drs.min().values, cryptic_drs.max()\n",
    "\n",
    "# Load the data\n",
    "cryptic_drs = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_sample_drs_cryptic_emb.npy')\n",
    "cryptic_snps = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_snps_cryptic_emb.npy')\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "cryptic_drs = pd.DataFrame(cryptic_drs)\n",
    "\n",
    "# Combine the features and target variable\n",
    "data = cryptic_snps\n",
    "target = cryptic_drs\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the XGBoost model\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "target_min, target_max = cryptic_drs.min().values, cryptic_drs.max().values\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(y_pred, y_test)])\n",
    "print(f\"Doubling Dilution Accuracy: {doubling_dilution_accuracy * 100:.2f}%\")\n",
    "\n",
    "#testing\n",
    "cutoff = 4\n",
    "test_target_bi = (np.squeeze(np.array(y_test)) >= cutoff).astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.squeeze(np.array(y_pred)) >= cutoff).astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
