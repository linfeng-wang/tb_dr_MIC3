{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/nn_model_script_emb_test.py - starting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9b5c10df30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "print('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/nn_model_script_emb_test.py - starting')\n",
    "\n",
    "from array import array\n",
    "from cmath import nan\n",
    "from pyexpat import model\n",
    "import statistics\n",
    "from tkinter.ttk import Separator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torchviz import make_dot\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import variable\n",
    "from itertools import chain\n",
    "from sklearn import metrics as met\n",
    "import pickle\n",
    "from icecream import ic\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from importlib import reload\n",
    "# import util\n",
    "# import model_torch_simple\n",
    "# from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#%%\n",
    "seed = 42\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# train_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/aa_data_train_gene.csv', delimiter = ',')\n",
    "# train_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/mic_aa_train_hml.csv')\n",
    "# train_target = train_target[['EMB_MIC']]\n",
    "# # don't touch test data, split out validation data from training data during training\n",
    "# # test_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_EMB/aa_data_test_pca4k.csv', delimiter = ',')\n",
    "# test_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/aa_data_test_gene.csv', delimiter = ',')\n",
    "# test_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/mic_aa_test_hml.csv')\n",
    "# test_target = test_target[['EMB_MIC']]\n",
    "\n",
    "# all_data = np.concatenate((train_data, test_data), axis=0)\n",
    "# all_target = pd.concat((train_target, test_target), axis=0)\n",
    "\n",
    "# train_data, test_data, train_target, test_target = train_test_split(all_data, all_target, test_size=0.2, random_state=42, stratify=all_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(aa_array, encoded_mic):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Perform stratified train-test split\n",
    "    train_data, test_data, train_target, test_target = train_test_split(\n",
    "        aa_array,\n",
    "        encoded_mic,\n",
    "        test_size=0.1,  # 10% for testing\n",
    "        stratify=encoded_mic.iloc[:,0],  # Ensures the proportion of each class is preserved\n",
    "        random_state=42  # For reproducibility\n",
    "    )\n",
    "    return train_data, test_data, train_target, test_target\n",
    "\n",
    "# def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "#     _ = np.arange(target_min-1, target_max+2, 1)\n",
    "#     index = [i for i, x in enumerate(_) if x == target][0]\n",
    "#     return (_[index-1] <= pred <= _[index+1])\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min.values-1, target_max.values+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "# def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "#     # Convert Series to scalar if needed\n",
    "#     if isinstance(pred, pd.Series):\n",
    "#         pred = pred.iloc[0]  # or pred.item()\n",
    "#     if isinstance(target, pd.Series):\n",
    "#         target = target.iloc[0]  # or target.item()\n",
    "\n",
    "#     _ = np.arange(target_min - 1, target_max + 2, 1)\n",
    "#     # Now target is guaranteed to be a scalar\n",
    "#     index = [i for i, x in enumerate(_) if x == target][0]\n",
    "#     return (_[index - 1] <= pred <= _[index + 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_(cryptic, gene_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    \n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    variants = variants[variants['type'] != 'non_coding_transcript_exon_variant']\n",
    "\n",
    "    overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    # print(variants.shape)\n",
    "    # print(variants['sample_id'].unique().shape)\n",
    "\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        \n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "\n",
    "    aa = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table \n",
    "    print('Getting all snp data', len(all_snp))\n",
    "    for x in tqdm(overlap):\n",
    "    # for x in tqdm(variants['sample_id'].unique()):\n",
    "        if x in variants['sample_id'].tolist():\n",
    "            aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "        else:\n",
    "            aa.append([0]*len(all_snp))\n",
    "        # print('SNP')\n",
    "        \n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "\n",
    "    mic_aa = cryptic[cryptic['ENA_RUN'].isin(overlap)]#.iloc[:,14:27]\n",
    "    # mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # print(mic_aa.shape)\n",
    "    # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(overlap)\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    mic_aa = mic_aa.sort_values([\"ENA_RUN\"])  ## 'sort' changed to 'sort_values'\n",
    "    # print(mic_aa.shape)\n",
    "\n",
    "    return aa_array, mic_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "drug = 'INH'\n",
    "cutoff = 5  # this the cutoff for order encoding that correspond to the MIC value 0.8\n",
    "cutoff_mic = 5\n",
    "df = pd.read_csv('../CRyPTIC_reuse_table_20231208.csv')\n",
    "\n",
    "aa_array = np.load(f'./generated_data18122024/all_sample_snps_cryptic_{drug}.npy')\n",
    "drs = np.load(f'./generated_data18122024/all_sample_drs_cryptic_{drug}.npy')\n",
    "tbp = np.load(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC3/individual_models/generated_data18122024/all_sample_drs_cryptic_{drug}-tbp.npy')\n",
    "\n",
    "encoded_mic = pd.DataFrame(drs, columns=[f'{drug}_MIC'])\n",
    "# mic_series = np.log2(encoded_mic)\n",
    "# mic_series += 1\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "encoded_mic['INH_MIC'] = ordinal_encoder.fit_transform(encoded_mic[['INH_MIC']])\n",
    "mic_series = encoded_mic\n",
    "# sample_ids = mic_aa['ENA_RUN']\n",
    "mic_series_bi = encoded_mic[f'{drug}_MIC'].apply(lambda x: 1 if x >= cutoff_mic else 0)\n",
    "mic_series_all = pd.merge(mic_series, mic_series_bi, left_index=True, right_index=True)\n",
    "# train_data, test_data,  test_target_y, test_target = data_split(aa_array, mic_series_all)\n",
    "mic_series_all['tbp'] = tbp\n",
    "\n",
    "train_data, test_data, train_target, test_target = data_split(aa_array, mic_series_all)\n",
    "\n",
    "target_min, target_max = mic_series.min(), mic_series.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[6012  744]\n",
      " [ 135 3197]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the absolute difference between the two columns\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(mic_series_all[f'{drug}_MIC_y'], mic_series_all['tbp'])\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INH_MIC_x</th>\n",
       "      <th>INH_MIC_y</th>\n",
       "      <th>tbp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2171</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6230</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7498</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5845</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9960</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9079 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      INH_MIC_x  INH_MIC_y  tbp\n",
       "2171        0.0          0    0\n",
       "2544        6.0          1    1\n",
       "5207        5.0          1    1\n",
       "6230        1.0          0    0\n",
       "2281        1.0          0    0\n",
       "...         ...        ...  ...\n",
       "3377        0.0          0    0\n",
       "7498        7.0          1    1\n",
       "1285        1.0          0    0\n",
       "5845        4.0          0    1\n",
       "9960        2.0          0    0\n",
       "\n",
       "[9079 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INH_MIC_x\n",
      "1.0    3047\n",
      "0.0    2188\n",
      "7.0    1445\n",
      "6.0    1244\n",
      "3.0     326\n",
      "5.0     310\n",
      "4.0     274\n",
      "2.0     245\n",
      "Name: count, dtype: int64\n",
      "Counter({1.0: 3047, 0.0: 3000, 6.0: 2500, 7.0: 2500, 5.0: 2000, 3.0: 2000, 4.0: 2000, 2.0: 2000})\n",
      "(19047,)\n",
      "(19047, 932)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Assuming train_data is your feature array and train_target['EMB_MIC_x'] is your target array\n",
    "X = train_data\n",
    "y = train_target[f'{drug}_MIC_x']\n",
    "print(train_target[f'{drug}_MIC_x'].value_counts())\n",
    "\n",
    "target_counts = {\n",
    "    1.0:3047,\n",
    "    0.0:3000,\n",
    "    7.0:2500,\n",
    "    6.0:2500,\n",
    "    3.0: 2000,\n",
    "    5.0: 2000,\n",
    "    4.0: 2000,\n",
    "    2.0: 2000,\n",
    "}\n",
    "# Initialize the RandomOverSampler\n",
    "ros= RandomOverSampler(sampling_strategy=target_counts, random_state=42)\n",
    "\n",
    "# ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Fit and resample the data\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Verify the new class distribution\n",
    "from collections import Counter\n",
    "print(Counter(y_resampled))\n",
    "print(y_resampled.shape)\n",
    "print(X_resampled.shape)\n",
    "\n",
    "# train_mic_series_bi = y_resampled.apply(lambda x: 1 if x >= 4 else 0)\n",
    "# train_target = pd.DataFrame({'EMB_MIC_x': y_resampled, 'EMB_MIC_y': train_mic_series_bi})\n",
    "# train_data = X_resampled\n",
    "train_mic_series_bi = y_resampled.apply(lambda x: 1 if x >= cutoff else 0)\n",
    "train_target = pd.DataFrame({f'{drug}_MIC_x': y_resampled, f'{drug}_MIC_y': train_mic_series_bi})\n",
    "train_data = X_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cornloss weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_counts = torch.from_numpy(train_target.values).flatten()\n",
    "# train_target_counts = torch.tensor([0,1,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(aa_array)\n",
    "\n",
    "aa_array = np.load(f'./generated_data18122024/all_sample_snps_50k_{drug}.npy')\n",
    "# Read the text file line by line into a list and convert to floats\n",
    "mic_series = np.load(f'./generated_data18122024/all_sample_drs_50k_{drug}.npy')\n",
    "\n",
    "# print(mic_series)\n",
    "mic_series_50k = mic_series\n",
    "\n",
    "aa_array_50k_pos = []\n",
    "mic_series_50k_pos = []\n",
    "for x, a in zip(mic_series_50k, aa_array):\n",
    "    if x == 1:\n",
    "        aa_array_50k_pos.append(a)\n",
    "        mic_series_50k_pos.append(x)\n",
    "aa_array_50k_pos = np.array(aa_array_50k_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Prepare your training and testing data\n",
    "# ------------------------------------------------\n",
    "\n",
    "# Assume you already have:\n",
    "#   train_data, test_data\n",
    "#   train_target, test_target\n",
    "#   aa_array_50k_pos, mic_series_50k_pos \n",
    "#   => as per your code snippet.\n",
    "\n",
    "train_target_y = train_target[f'{drug}_MIC_y'].values\n",
    "test_target_y  = test_target[f'{drug}_MIC_y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_data, test_data, train_target_y, test_target_y\n",
    "\n",
    "# Optionally concatenate the additional data you mentioned:\n",
    "X_train = np.concatenate((X_train, aa_array_50k_pos), axis=0)\n",
    "y_train = np.concatenate((y_train, mic_series_50k_pos), axis=0)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Define a function for checking doubling dilution \n",
    "# (if needed)\n",
    "# ------------------------------------------------\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Set up the XGBoost model & parameter grid\n",
    "# ------------------------------------------------\n",
    "# Create a base model\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Define a parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3], \n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    # 'reg_alpha': [0, 1, 10],       # optionally add\n",
    "    # 'reg_lambda': [0, 1, 10],     # optionally add\n",
    "}\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Run GridSearchCV\n",
    "# ------------------------------------------------\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',      # You can choose other metrics, e.g., 'roc_auc'\n",
    "    cv=3,                    # 3-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1               # Use all available CPU cores\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Evaluate on the test set\n",
    "# ------------------------------------------------\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Binarize predictions at cutoff=4 (per your example)\n",
    "cutoff = cutoff\n",
    "test_target_bi = y_test.astype(int)\n",
    "test_predictions_bi = y_pred.astype(int)\n",
    "\n",
    "# Compute AUC\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Compute confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_target_bi, test_predictions_bi))\n",
    "\n",
    "# Compute sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Compute specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9395441030723488\n",
      "AUC: 0.9244073955612417\n",
      "[[655  21]\n",
      " [ 40 293]]\n",
      "Sensitivity: 0.8798798798798799\n",
      "Specificity: 0.9689349112426036\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_data = np.column_stack((train_data, train_target['EMB_MIC_y'].values))\n",
    "# test_data = np.column_stack((test_data, test_target['EMB_MIC_y'].values))\n",
    "train_target_y = train_target[f'{drug}_MIC_y'].values\n",
    "test_target_y = test_target[f'{drug}_MIC_y'].values\n",
    "\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_data, test_data, train_target_y, test_target_y\n",
    "X_test = test_data\n",
    "y_test = test_target[f'{drug}_MIC_y'].values\n",
    "X_train = np.concatenate((X_train, aa_array_50k_pos), axis=0)\n",
    "y_train = np.concatenate((y_train, mic_series_50k_pos), axis=0)\n",
    "# # Create the XGBoost model\n",
    "model_bi = xgb.XGBClassifier(    \n",
    "    max_depth=3,\n",
    "    learning_rate=0.9,\n",
    "    n_estimators=4,\n",
    "    # gamma=0.1,\n",
    "    # min_child_weight=24,\n",
    "    # subsample=0.2,\n",
    "    # colsample_bytree=1,\n",
    "    # reg_alpha=15, reg_lambda=15,\n",
    "    random_state=42 \n",
    "    )\n",
    "# # Create the XGBoost model\n",
    "# model_bi = xgb.XGBClassifier(colsample_bytree= 0.5, gamma= 0.1, learning_rate= 0.01, max_depth= 4, min_child_weight= 1, n_estimators= 200, subsample= 0.8,random_state=42)\n",
    "# model_bi = xgb.XGBClassifier()\n",
    "\n",
    "model_bi.fit(X_train, y_train)\n",
    "\n",
    "# model_bi = pickle.load(open(\"xgb_bi_mix1.pkl\", \"rb\"))\n",
    "# model_bi = pickle.load(open(\"xgb_bi_mix.pkl\", \"rb\"))\n",
    "# model_bi = pickle.load(open(\"xgb_bi_50kbalanced.pkl\", \"rb\"))\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model_bi.predict(X_test)\n",
    "\n",
    "# Evaluate the model_bi\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "#testing\n",
    "cutoff = cutoff\n",
    "test_target_bi = y_test.astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "test_predictions_bi = y_pred.astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "print(confusion_matrix(test_target_bi, test_predictions_bi))\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)  \n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9569041336851363\n",
      "AUC: 0.932873675643259\n",
      "[[775   1]\n",
      " [ 48 313]]\n",
      "Sensitivity: 0.8670360110803325\n",
      "Specificity: 0.9987113402061856\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Define training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_data, test_data, train_target_y, test_target_y\n",
    "X_test = test_data\n",
    "y_test = test_target[f'{drug}_MIC_y'].values\n",
    "X_train = np.concatenate((X_train, aa_array_50k_pos), axis=0)\n",
    "y_train = np.concatenate((y_train, mic_series_50k_pos), axis=0)\n",
    "\n",
    "# Apply RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create the XGBoost model\n",
    "model_bi = xgb.XGBClassifier(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.9,\n",
    "    n_estimators=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model using the resampled dataset\n",
    "model_bi.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model_bi.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Compute AUC\n",
    "cutoff = cutoff\n",
    "test_target_bi = y_test.astype(int)\n",
    "test_predictions_bi = y_pred.astype(int)\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix and performance metrics\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "print(confusion_matrix(test_target_bi, test_predictions_bi))\n",
    "\n",
    "sensitivity = tp / (tp + fn)  # Recall\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_bi.save_model('/mnt/storageG1/lwang/Projects/tb_dr_MIC3/individual_models/saved_model1115/xgb_model_bi.json')  # or \"xgb_model.bin\"\n",
    "# loaded_model.load_model('/mnt/storageG1/lwang/Projects/tb_dr_MIC3/individual_models/saved_model1115/xgb_model.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running with xgb predictions\n",
    "test_target[f'{drug}_MIC_y'] = test_predictions_bi\n",
    "# runninng with tbp predictions\n",
    "# test_target[f'{drug}_MIC_y'] = test_target['tbp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from collections import Counter\n",
    "\n",
    "N_samples = train_data.shape[0]\n",
    "DRUGS = train_target.columns\n",
    "# LOCI = train_data.columns\n",
    "assert set(DRUGS) == set(train_target.columns)\n",
    "N_drugs = len(DRUGS)\n",
    "#%%\n",
    "\n",
    "def my_padding(seq_tuple):\n",
    "    list_x_ = list(seq_tuple)\n",
    "    max_len = len(max(list_x_, key=len))\n",
    "    for i, x in enumerate(list_x_):\n",
    "        list_x_[i] = x + \"N\"*(max_len-len(x))\n",
    "    return list_x_\n",
    "\n",
    "#! faster than my_padding try to incorporate\n",
    "def collate_padded_batch(batch):\n",
    "    # get max length of seqs in batch\n",
    "    max_len = max([x[0].shape[1] for x in batch])\n",
    "    return torch.utils.data.default_collate(\n",
    "        [(F.pad(x[0], (0, max_len - x[0].shape[1])), x[1]) for x in batch] #how does F.pad work\n",
    "    )\n",
    "\n",
    "# Julian's code - implement this, might be faster\n",
    "class Dataset(torch.utils.data.Dataset): #? what's the difference between using inheritance and not?\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_df,\n",
    "        res_df,\n",
    "        # target_loci=LOCI,\n",
    "        target_mic,\n",
    "        target_res,\n",
    "        one_hot_dtype=torch.int8,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        # self.seq_df = seq_df[target_loci]\n",
    "        self.seq_df = seq_df\n",
    "        self.res_df = res_df[target_res]\n",
    "        self.mic_df = res_df[target_mic]\n",
    "        # if not self.seq_df.index.equals(self.res_df.index):\n",
    "        #     raise ValueError(\n",
    "        #         \"Indices of sequence and resistance dataframes don't match up\"\n",
    "        #     )\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        numerical index --> get `index`-th sample\n",
    "        string index --> get sample with name `index`\n",
    "        \"\"\"\n",
    "        index = int(index)\n",
    "        if isinstance(index, int):\n",
    "            seqs_comb = self.seq_df[index]\n",
    "            res = self.res_df.iloc[index]\n",
    "            mic = self.mic_df.iloc[index]\n",
    "        elif isinstance(index, str):\n",
    "            seqs_comb = self.seq_df[int(index)]\n",
    "            res = self.res_df.loc[index]\n",
    "            mic = self.mic_df.loc[index]\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Index needs to be an integer or a sample name present in the dataset\"\n",
    "            )\n",
    "\n",
    "        if self.transform:\n",
    "            res = np.log(res)\n",
    "            \n",
    "            # self.res_mean = self.res_df.mean()\n",
    "            # self.res_std = self.res_df.std()\n",
    "            # res = (res - self.res_mean) / self.res_std\n",
    "            # res = self.transform(res)\n",
    "        return torch.unsqueeze(torch.tensor(seqs_comb).float(), 0), torch.tensor(mic).long().flatten().squeeze(), torch.tensor(res).long().flatten().squeeze()\n",
    "    def __len__(self):\n",
    "        return self.res_df.shape[0]\n",
    "\n",
    "training_dataset = Dataset(train_data, train_target, f'{drug}_MIC_x',f'{drug}_MIC_y', one_hot_dtype=torch.float, transform=False)\n",
    "# train_dataset, val_dataset = random_split(training_dataset, [int(len(training_dataset)*0.9), len(training_dataset)-int(len(training_dataset)*0.9)])\n",
    "\n",
    "# test_data = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/snps_crypticTest_emb.npy')\n",
    "# train_data = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/drs_crypticTest_emb.npy')\n",
    "testing_dataset = Dataset(test_data, test_target, f'{drug}_MIC_x',f'{drug}_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "\n",
    "train_idx, validation_idx = train_test_split(np.arange(len(train_data)),\n",
    "                                             test_size=0.1,\n",
    "                                             random_state=42,\n",
    "                                             shuffle=True,\n",
    "                                             stratify=train_target)\n",
    "\n",
    "# Subset dataset for train and val\n",
    "train_dataset = Subset(training_dataset, train_idx)\n",
    "val_dataset = Subset(training_dataset, validation_idx)\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# # device = 'cpu'\n",
    "\n",
    "y_true = train_target\n",
    "# y_true = pd.concat([train_target, test_target])\n",
    "\n",
    "column_weight_maps = {}\n",
    "\n",
    "for column in y_true.columns:\n",
    "    column_values = y_true[column].dropna().values\n",
    "    values, counts = np.unique(column_values, return_counts=True)\n",
    "    frequency = counts / len(column_values)\n",
    "    \n",
    "    # Calculate weights as the inverse of frequencies\n",
    "    weights_inverse = 1/frequency\n",
    "    # weights_inverse = 1 - frequency\n",
    "    \n",
    "    # Normalize weights to ensure they sum up to 1\n",
    "    weights_normalized = weights_inverse / np.sum(weights_inverse)\n",
    "    \n",
    "    # Map each MIC value to its corresponding weight\n",
    "    weight_map = {value: weight for value, weight in zip(values, weights_normalized)}\n",
    "    \n",
    "    column_weight_maps[column] = weight_map\n",
    "\n",
    "def get_weighted_masked_cross_entropy_loss(column_weight_maps):\n",
    "    \"\"\"\n",
    "    Creates a loss function that computes a weighted cross entropy loss, taking into account class imbalances.\n",
    "    :param column_weight_maps: Dictionary mapping column names to their corresponding class weight maps.\n",
    "    \"\"\"\n",
    "    def weighted_masked_cross_entropy_loss(y_pred, y_true):\n",
    "        # weighted_losses = torch.Tensor().to(device)\n",
    "        weighted_losses = []\n",
    "        col_weight_map = column_weight_maps\n",
    "        # print(col_weight_map)\n",
    "        mean_weight = np.mean(list(col_weight_map.values())) # just in case if a number is not recognised and the loss doesn't go crazy\n",
    "\n",
    "        # print(y_pred.size())\n",
    "        # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        weights_col = [col_weight_map.get(y.item(), mean_weight) for y in y_true]\n",
    "        # print(weights_col)\n",
    "        # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        loss_fn = F.cross_entropy\n",
    "        col_loss = loss_fn(y_pred, y_true, reduction = 'none').to(device)\n",
    "        \n",
    "        # loss_fn = nn.CrossEntropyLoss(reduction = 'none')\n",
    "        # col_loss = loss_fn(y_pred, y_true)\n",
    "        # print(y_true.dtype)\n",
    "        # print(col_loss)\n",
    "        weights_col = torch.Tensor(weights_col).to(device)\n",
    "        # print(weights_col)\n",
    "        # print(col_loss)\n",
    "        weighted_col_loss = weights_col * col_loss\n",
    "        # print(weighted_col_loss)\n",
    "        weighted_losses.append(weighted_col_loss.mean())\n",
    "\n",
    "        total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        \n",
    "        # for i, column in enumerate(column_weight_maps.keys()):\n",
    "        #     col_weight_map = column_weight_maps[column]\n",
    "        #     print(y_pred.size())\n",
    "        #     # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        #     weights_col = torch.tensor([col_weight_map[y.item()] for y in y_true[:, i]], dtype=torch.float32, device=y_true.device)\n",
    "        #     print(weights_col)\n",
    "        #     # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        #     loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        #     col_loss = loss_fn(y_pred[:, i,], y_true[:, i])\n",
    "            \n",
    "        #     weighted_col_loss = weights_col * col_loss\n",
    "        #     weighted_losses.append(weighted_col_loss.mean())\n",
    "        \n",
    "        # total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        return total_weighted_loss\n",
    "\n",
    "    return weighted_masked_cross_entropy_loss\n",
    "\n",
    "# Also assuming `columns` is a list of your target column names corresponding to y_true and y_pred\n",
    "weighted_cross_entropy_loss_fn = get_weighted_masked_cross_entropy_loss(column_weight_maps[f'{drug}_MIC_x'])\n",
    "# loss = weighted_cross_entropy_loss_fn(y_true_tensor, y_pred_logits, columns)\n",
    "\n",
    "def save_to_file(file_path, appendix, epoch, lr, cnndr, fcdr, l2, train_loss, test_loss, optimizer, model):\n",
    "    train_loss = [float(arr) for arr in train_loss]\n",
    "    test_loss = [float(arr) for arr in test_loss]\n",
    "    with open(file_path, \"a\") as f:\n",
    "        f.write(f\"#>> {appendix}, Epoch: {epoch}, LR: {lr}, fcDR: {fcdr}\\n\")\n",
    "        f.write(f\"Train_Loss= {train_loss}\\n\")\n",
    "        f.write(f\"Test_Loss= {test_loss}\\n\")\n",
    "        f.write(f\"lossGraph(Train_Loss, Test_Loss, '{appendix}-Epoch-{epoch}-LR-{lr}-fcDR-{fcdr}')\\n\")\n",
    "\n",
    "    torch.save({\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model': model.state_dict(),\n",
    "    }, f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/seq-{appendix}-{epoch}-{lr}-{cnndr}-{fcdr}-{l2}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_data = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/snps_crypticTest_emb.npy')\n",
    "# test_target = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/drs_crypticTest_emb.npy')\n",
    "# testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "# testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        num_classes=6,\n",
    "        num_filters=64,\n",
    "        filter_length=25,\n",
    "        num_conv_layers=2,\n",
    "        filter_scaling_factor=1,  # New parameter\n",
    "        num_dense_neurons=256,\n",
    "        num_dense_layers=2,\n",
    "        conv_dropout_rate=0.0,\n",
    "        dense_dropout_rate=0.2,\n",
    "        l1_strength = 0.1,\n",
    "        return_logits=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_length = filter_length\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.num_dense_layers = num_dense_layers\n",
    "        self.conv_dropout_rate = conv_dropout_rate\n",
    "        self.dense_dropout_rate = dense_dropout_rate\n",
    "        self.return_logits = return_logits\n",
    "        \n",
    "        # now define the actual model\n",
    "        # self.feature_extraction_layer = self._conv_layer(\n",
    "            # in_channels, num_filters, filter_length\n",
    "        # )\n",
    "        self.feature_extraction_layer = self._conv_layer_extract(\n",
    "            in_channels, num_filters, filter_length\n",
    "        )\n",
    "        #dynamic filter scaling from deepram\n",
    "        current_num_filters1 = num_filters\n",
    "        self.conv_layers1 = nn.ModuleList()\n",
    "        for i in range(num_conv_layers):\n",
    "            layer = self._conv_layer(current_num_filters1, int(current_num_filters1 * filter_scaling_factor), 3)\n",
    "            self.conv_layers1.append(layer)\n",
    "            current_num_filters1 = int(current_num_filters1 * filter_scaling_factor)\n",
    "            \n",
    "        current_num_filters2 = 32\n",
    "        self.conv_layers2 = nn.ModuleList()\n",
    "        for i in range(num_conv_layers):\n",
    "            layer = self._conv_layer(current_num_filters1, int(current_num_filters2 * filter_scaling_factor), 3)\n",
    "            self.conv_layers2.append(layer)\n",
    "            current_num_filters1 = current_num_filters2\n",
    "            \n",
    "        self.dense_layers = nn.ModuleList(\n",
    "            self._dense_layer(input_dim, num_dense_neurons)\n",
    "            for input_dim in [28672]\n",
    "            + [num_dense_neurons] * (num_dense_layers - 1) #how does this work?\n",
    "        )\n",
    "        \n",
    "        # self.dense_layers = nn.ModuleList(\n",
    "            # self._dense_layer(input_dim, num_dense_neurons)\n",
    "            # for input_dim in [current_num_filters2]\n",
    "            # + [num_dense_neurons] * (num_dense_layers - 1) #how does this work?\n",
    "        # )\n",
    "        \n",
    "        # self.prediction_layer = (\n",
    "        #     nn.Linear(num_dense_neurons, num_classes)\n",
    "        #     if return_logits\n",
    "        #     else nn.Sequential(nn.Linear(num_dense_neurons, num_classes), nn.ReLU()) #difference between sequential and nn.moduleList?\n",
    "        # )\n",
    "        \n",
    "        dense_output_size = num_dense_neurons  # Assuming dense layer output is num_dense_neurons\n",
    "        additional_input_size = 1  # Assuming additional input is a single value\n",
    "        total_input_size = dense_output_size + additional_input_size  # Total input size for the prediction layer\n",
    "\n",
    "        self.prediction_layer = (\n",
    "            nn.Linear(total_input_size, num_classes)  # If logits are returned directly\n",
    "            if return_logits\n",
    "            else nn.Sequential(\n",
    "                nn.Linear(total_input_size, int(total_input_size * 0.7)),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dense_dropout_rate),  # Dropout layer after the first ReLU activation\n",
    "                nn.Linear(int(total_input_size * 0.7), int(total_input_size * 0.5)),  # Optional additional layer with reduced size\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dense_dropout_rate),  # Dropout layer after the second ReLU activation\n",
    "                nn.Linear(int(total_input_size * 0.5), num_classes)  # Final layer to match the number of classes\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.m = nn.MaxPool1d(3, stride=1)\n",
    "        \n",
    "        self.apply(self.init_weights)    \n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _conv_layer(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(p=self.conv_dropout_rate),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def _conv_layer_extract(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def _dense_layer(self, n_in, n_out):\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(p=self.dense_dropout_rate),\n",
    "            nn.Linear(n_in, n_out),\n",
    "            nn.BatchNorm1d(n_out),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def l1_regularization(self):\n",
    "        l1_loss_example = 0\n",
    "        for param in self.parameters():\n",
    "            l1_loss_example += torch.sum(torch.abs(param))\n",
    "        return self.l1_strength * l1_loss_example\n",
    "\n",
    "    def forward(self, x, additional_input):\n",
    "        # Feature extraction\n",
    "        x = self.feature_extraction_layer(x)\n",
    "\n",
    "        # Convolutional layers\n",
    "        for layer in self.conv_layers1:\n",
    "            x = layer(x)\n",
    "        x = self.m(x)\n",
    "        for layer in self.conv_layers2:\n",
    "            x = layer(x)\n",
    "        x = self.m(x)\n",
    "        \n",
    "        # Flatten the tensor to [batch_size, features]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Dense layers\n",
    "        for layer in self.dense_layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        additional_input = additional_input.unsqueeze(1)\n",
    "        # Concatenate additional input value\n",
    "        x = torch.cat((x, additional_input), dim=1)  # Concatenate along the feature dimension\n",
    "\n",
    "        # Prediction layer\n",
    "        x = self.prediction_layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# def l1loss(layer): # https://stackoverflow.com/questions/50054049/lack-of-sparse-solution-with-l1-regularization-in-pytorch\n",
    "#     return torch.norm(layer.weight, p=1)\n",
    "\n",
    "# def l1loss(sequence):\n",
    "#     l1_regularization = 0\n",
    "#     for module in sequence.modules():\n",
    "#         if isinstance(module, nn.Conv1d):  # Check if the module is a Conv1d layer\n",
    "#             l1_regularization += torch.norm(module.weight, p=1)\n",
    "#     return l1_regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 50/500 [04:53<47:13,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Training loss: 0.18479906022548676\n",
      "Validation loss: 0.17559367418289185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 100/500 [12:50<1:11:00, 10.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100\n",
      "Training loss: 0.17152315378189087\n",
      "Validation loss: 0.16986994445323944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 150/500 [21:42<1:02:21, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150\n",
      "Training loss: 0.16369569301605225\n",
      "Validation loss: 0.16062621772289276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 200/500 [30:36<52:29, 10.50s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200\n",
      "Training loss: 0.15908744931221008\n",
      "Validation loss: 0.15614254772663116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 219/500 [33:58<51:16, 10.95s/it]"
     ]
    }
   ],
   "source": [
    "# input parameter\n",
    "lr = 1e-4\n",
    "epoch = 500\n",
    "conv_dropout_rate=0.4\n",
    "dense_dropout_rate=0.7\n",
    "weight_decay=1e-4\n",
    "######################################\n",
    "\n",
    "model = Model(\n",
    "num_classes=len(np.unique(drs)),\n",
    "num_filters=64,\n",
    "num_conv_layers=2,\n",
    "# num_dense_neurons=256, # batch_size = 64\n",
    "num_dense_neurons=128, # batch_size = 64\n",
    "num_dense_layers=2,\n",
    "return_logits=False,\n",
    "conv_dropout_rate=conv_dropout_rate,\n",
    "dense_dropout_rate=dense_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# model = Model( #! way too memory intensive\n",
    "# num_classes=13,\n",
    "# num_filters=128,\n",
    "# num_conv_layers=2,\n",
    "# num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "# num_dense_layers=2,\n",
    "# return_logits=True,\n",
    "# conv_dropout_rate=0,\n",
    "# dense_dropout_rate=0\n",
    "# ).to(device)\n",
    "## early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "patience_counter = 0\n",
    "lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "batch_size = 64\n",
    "# lr = 0.0085\n",
    "# lr = 0.00002\n",
    "lr = lr\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "# test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = masked_weighted_MAE\n",
    "# criterion = masked_weighted_MSE\n",
    "criterion = weighted_cross_entropy_loss_fn\n",
    "# criterion = masked_MAE\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "# scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "#%%\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "# ic.enable()\n",
    "ic.disable()\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    # print(f'Epoch {e}')\n",
    "    for x_train, y_train, y_train_res in train_loader:\n",
    "        x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "        y_batch = y_train.to(device).long()  # Convert to torch.long\n",
    "        y_batch_res = y_train_res.to(device)\n",
    "        \n",
    "        x_batch = x_batch.float()\n",
    "        pred = model(x_batch.float(),y_batch_res.float())\n",
    "        \n",
    "        pred = pred.float()  # Convert predictions to float if necessary\n",
    "        y_batch = y_batch.long()  # Ensure targets are long integers\n",
    "        # break\n",
    "        # loss_train = loss_corn(pred, y_batch, 3, class_weights)\n",
    "        # print(pred, y_batch)\n",
    "        loss_train = criterion(pred,y_batch)\n",
    "        # print(pred)\n",
    "        # print(y_batch)\n",
    "        # print(loss_train)\n",
    "        train_batch_loss.append(loss_train)        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update the learning rate\n",
    "        # break\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # print('>> test')\n",
    "        for x_test, y_test, y_test_res in test_loader:\n",
    "            x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            y_batch = y_test.to(device)\n",
    "            y_batch_res = y_test_res.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float(), y_batch_res.float())\n",
    "            loss_test = criterion(pred,y_batch)\n",
    "            # pred = pred.unsqueeze(0)\n",
    "            # print(pred[:10])\n",
    "            # print(y_batch[:10])\n",
    "\n",
    "            # loss_test = loss_corn(pred, y_batch, 3, class_weights)\n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "    if e%50 == 0:\n",
    "        print(f'Epoch {e}')\n",
    "        print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "        print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    # #! implementing early stopping\n",
    "    # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "    # print(f'Current val loss: {current_val_loss}')\n",
    "    # print(f'Best val loss: {best_val_loss}')\n",
    "    # if current_val_loss < best_val_loss:\n",
    "    #     best_val_loss = current_val_loss\n",
    "    #     patience_counter = 0  # reset patience counter\n",
    "    #     # Save the best model\n",
    "    #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         torch.save({\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #         'model': model.state_dict(),\n",
    "    #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "    #         break  # Early stopping\n",
    "    \n",
    "print('==='*10)\n",
    "# torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-inh' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "             train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, epoch+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-{drug}.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-{drug}')\n",
    "\n",
    "#%%\n",
    "# testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "# testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "# testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "# Ensure the model is on the same device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test, y_test_res in testing_loader1:\n",
    "        # Move input and target data to the correct device\n",
    "        x_test = x_test.to(device).float()\n",
    "        y_test = y_test.to(device).float()\n",
    "        y_test_res = y_test_res.to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(x_test, y_test_res)\n",
    "        \n",
    "        # Append predictions and targets to lists\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "        \n",
    "# Flatten the target list\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min.values-1, target_max.values+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "# Calculate AUC\n",
    "cutoff = cutoff\n",
    "test_target_bi = (target_list >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_0.0001_weighted_balanced.png-INH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13519/3551832087.py:16: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "Optimizer details:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "Learning rate: 0.0001\n",
      "Weight decay: 0.0001\n",
      "======================\n",
      "Accuracy: 0.45391476709613476\n",
      "Mae: 0.8265609514370664\n",
      "F1 Score: 0.322805867169881\n",
      "conf_matrix: [[  0 238   1   1   1   0   2   0]\n",
      " [  0 328   2   4   4   0   1   0]\n",
      " [  0  15   1   1   2   2   5   1]\n",
      " [  0  29   1   2   0   3   0   1]\n",
      " [  0  19   2   3   1   0   5   1]\n",
      " [  0   1   3   2   2   5  21   0]\n",
      " [  0  10   3   0   1   8 113   3]\n",
      " [  0  12   3   1   2   4 131   8]]\n",
      "======================\n",
      "Doubling Dilution Accuracy: 0.8751238850346879\n",
      "AUC: 0.9244073955612417\n",
      "Sensitivity: 0.8798798798798799\n",
      "Specificity: 0.9689349112426036\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, epoch+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-{drug}.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-{drug}')\n",
    "\n",
    "#%%\n",
    "# testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "# testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "# testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "# Ensure the model is on the same device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test, y_test_res in testing_loader1:\n",
    "        # Move input and target data to the correct device\n",
    "        x_test = x_test.to(device).float()\n",
    "        y_test = y_test.to(device).float()\n",
    "        y_test_res = y_test_res.to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(x_test, y_test_res)\n",
    "        \n",
    "        # Append predictions and targets to lists\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "        \n",
    "# Flatten the target list\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min.values-1, target_max.values+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "# Calculate AUC\n",
    "cutoff = cutoff\n",
    "test_target_bi = (target_list >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction dtype: torch.float32\n",
      "Target dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction dtype:\", pred.dtype)\n",
    "print(\"Target dtype:\", y_batch.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13519/1703092247.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdoubling_dilution_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mis_within_doubling_dilution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_max\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Doubling Dilution Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoubling_dilution_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_list' is not defined"
     ]
    }
   ],
   "source": [
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min.values-1, target_max.values+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8741451008695054\n",
      "Sensitivity: 0.9453551912568307\n",
      "Specificity: 0.8029350104821803\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate AUC\n",
    "cutoff = cutoff\n",
    "test_target_bi = (target_list >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './saved_model1115/emb_resFeed_working17122024.pth'\n",
    "# torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred, true in zip(pred_list, target_list):\n",
    "    print(pred, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_y = []\n",
    "errors_y_ = []\n",
    "for y_, y in zip(pred_list, target_list):\n",
    "    # if y_ != y:\n",
    "    if y not in [y_, y_+1, y_-1]:\n",
    "        errors_y.append(y)\n",
    "        errors_y_.append(y_)\n",
    "        \n",
    "for a, b in zip(errors_y, errors_y_):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7.,  0., 48.,  0.,  2.,  0.,  3.,  0., 51., 12.]),\n",
       " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(errors_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = []\n",
    "for a, b in zip(errors_y, errors_y_):\n",
    "    joint.append(f'{a}_{b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, '')])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(15, 6), dpi=80)\n",
    "plt.hist(joint, width=0.8)\n",
    "plt.xticks(rotation=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([13.,  0.,  9.,  0., 44.,  0., 42.,  0.,  9.,  6.]),\n",
       " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ],\n",
       "       dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(errors_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0 1\n",
      "2.0 4\n",
      "2.0 5\n",
      "3.0 5\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "2.0 4\n",
      "5.0 1\n",
      "4.0 1\n",
      "3.0 1\n",
      "0.0 4\n",
      "2.0 0\n",
      "3.0 1\n",
      "4.0 1\n",
      "2.0 0\n",
      "3.0 1\n",
      "2.0 0\n",
      "5.0 1\n",
      "3.0 1\n",
      "2.0 4\n",
      "5.0 1\n",
      "3.0 1\n",
      "3.0 1\n",
      "0.0 5\n",
      "1.0 5\n",
      "2.0 4\n",
      "0.0 3\n",
      "0.0 4\n",
      "2.0 4\n",
      "1.0 4\n",
      "1.0 4\n",
      "3.0 1\n",
      "0.0 3\n",
      "3.0 1\n",
      "2.0 4\n",
      "0.0 2\n",
      "3.0 1\n",
      "0.0 4\n",
      "1.0 3\n",
      "4.0 1\n",
      "3.0 1\n",
      "0.0 4\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "3.0 5\n",
      "2.0 4\n",
      "4.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "3.0 1\n",
      "3.0 1\n",
      "3.0 1\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 0\n",
      "4.0 1\n",
      "2.0 4\n",
      "3.0 5\n",
      "0.0 4\n",
      "2.0 4\n",
      "0.0 4\n",
      "2.0 4\n",
      "2.0 4\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "3.0 1\n",
      "5.0 0\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "2.0 4\n",
      "0.0 5\n",
      "3.0 1\n",
      "2.0 4\n",
      "3.0 1\n",
      "3.0 1\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "3.0 5\n",
      "2.0 4\n",
      "3.0 1\n",
      "3.0 1\n",
      "1.0 4\n",
      "2.0 5\n",
      "3.0 5\n",
      "3.0 1\n",
      "2.0 0\n",
      "2.0 4\n",
      "3.0 0\n",
      "0.0 4\n",
      "1.0 4\n",
      "3.0 1\n",
      "3.0 5\n",
      "4.0 1\n",
      "3.0 1\n",
      "2.0 4\n",
      "3.0 1\n",
      "2.0 4\n",
      "3.0 1\n",
      "4.0 1\n",
      "3.0 5\n",
      "3.0 1\n",
      "3.0 1\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "4.0 1\n",
      "2.0 4\n",
      "3.0 1\n",
      "2.0 4\n",
      "0.0 2\n",
      "1.0 4\n",
      "2.0 4\n",
      "1.0 4\n",
      "4.0 1\n",
      "5.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "1.0 4\n"
     ]
    }
   ],
   "source": [
    "for a, b in zip(errors_y, errors_y_):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved_model1115/resFeed1.pth'\n",
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "Optimizer details:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "Learning rate: 0.0001\n",
      "Weight decay: 0.0001\n",
      "======================\n",
      "Accuracy: 0.5224274406332454\n",
      "Mae: 0.503957783641161\n",
      "F1 Score: 0.48083488925470447\n",
      "conf_matrix: [[ 18  86   8   0   0   0]\n",
      " [ 48 330  19   0   0   0]\n",
      " [ 15 230  51   0   0   0]\n",
      " [  0   0   0 104  44   1]\n",
      " [  0   0   0  52  65  20]\n",
      " [  0   0   0   6  14  26]]\n",
      "======================\n",
      "Doubling Dilution Accuracy: 0.9736147757255936\n",
      "AUC: 0.8179451489844314\n",
      "Sensitivity: 0.6830601092896175\n",
      "Specificity: 0.9528301886792453\n"
     ]
    }
   ],
   "source": [
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "# Ensure the model is on the same device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test, y_test_res in testing_loader1:\n",
    "        # Move input and target data to the correct device\n",
    "        x_test = x_test.to(device).float()\n",
    "        y_test = y_test.to(device).float()\n",
    "        y_test_res = y_test_res.to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(x_test, y_test_res)\n",
    "        \n",
    "        # Append predictions and targets to lists\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "        \n",
    "# Flatten the target list\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "\n",
    "# Calculate AUC\n",
    "cutoff = cutoff\n",
    "test_target_bi = (target_list >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypterparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–Ž        | 50/400 [02:56<20:30,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Training loss: 0.1510869264602661\n",
      "Validation loss: 0.13646160066127777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 100/400 [05:52<17:28,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100\n",
      "Training loss: 0.12380748987197876\n",
      "Validation loss: 0.11622773110866547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 114/400 [06:41<16:51,  3.54s/it]"
     ]
    }
   ],
   "source": [
    "#input parameter\n",
    "for _ in [1e-4, 5e-4,1e-3, 5e-3,1e-2]:\n",
    "    lr = 1e-4\n",
    "    epoch = 400\n",
    "    conv_dropout_rate=0.4\n",
    "    dense_dropout_rate=0.7\n",
    "    weight_decay= _ #1e-4\n",
    "    ######################################\n",
    "\n",
    "    model = Model(\n",
    "    num_classes=6,\n",
    "    num_filters=64,\n",
    "    num_conv_layers=2,\n",
    "    # num_dense_neurons=256, # batch_size = 64\n",
    "    num_dense_neurons=128, # batch_size = 64\n",
    "    num_dense_layers=2,\n",
    "    return_logits=False,\n",
    "    conv_dropout_rate=conv_dropout_rate,\n",
    "    dense_dropout_rate=dense_dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "    # model = Model( #! way too memory intensive\n",
    "    # num_classes=13,\n",
    "    # num_filters=128,\n",
    "    # num_conv_layers=2,\n",
    "    # num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "    # num_dense_layers=2,\n",
    "    # return_logits=True,\n",
    "    # conv_dropout_rate=0,\n",
    "    # dense_dropout_rate=0\n",
    "    # ).to(device)\n",
    "    ## early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "    patience_counter = 0\n",
    "    lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "    batch_size = 64\n",
    "    # lr = 0.0085\n",
    "    # lr = 0.00002\n",
    "    lr = lr\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "    test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "    # train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "    # test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "    # criterion = nn.MSELoss()\n",
    "    # criterion = masked_weighted_MAE\n",
    "    # criterion = masked_weighted_MSE\n",
    "    criterion = weighted_cross_entropy_loss_fn\n",
    "    # criterion = masked_MAE\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "    # scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "    #%%\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc; gc.collect()\n",
    "    # ic.enable()\n",
    "    ic.disable()\n",
    "\n",
    "    train_epoch_loss = []\n",
    "    test_epoch_loss = []\n",
    "\n",
    "    for e in tqdm(range(1, epoch+1)):\n",
    "        model.train()\n",
    "        train_batch_loss = []\n",
    "        test_batch_loss = []\n",
    "        # print(f'Epoch {e}')\n",
    "        for x_train, y_train, y_train_res in train_loader:\n",
    "            x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "            y_batch = y_train.to(device)\n",
    "            y_batch_res = y_train_res.to(device)\n",
    "            \n",
    "            x_batch = x_batch.float()\n",
    "            pred = model(x_batch.float(),y_batch_res.float())\n",
    "\n",
    "            # break\n",
    "            # loss_train = loss_corn(pred, y_batch, 3, class_weights)\n",
    "            # print(pred, y_batch)\n",
    "            loss_train = criterion(pred,y_batch)\n",
    "            # print(pred)\n",
    "            # print(y_batch)\n",
    "            # print(loss_train)\n",
    "            train_batch_loss.append(loss_train)        \n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()  # Update the learning rate\n",
    "            # break\n",
    "        train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # print('>> test')\n",
    "            for x_test, y_test, y_test_res in test_loader:\n",
    "                x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "                x_batch = x_batch.float()\n",
    "                y_batch = y_test.to(device)\n",
    "                y_batch_res = y_test_res.to(device)\n",
    "                # print(x_batch.size())\n",
    "                # y_batch = torch.Tensor.float(y).to(device)\n",
    "                # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "                pred = model(x_batch.float(), y_batch_res.float())\n",
    "                loss_test = criterion(pred,y_batch)\n",
    "                # pred = pred.unsqueeze(0)\n",
    "                # print(pred[:10])\n",
    "                # print(y_batch[:10])\n",
    "\n",
    "                # loss_test = loss_corn(pred, y_batch, 3, class_weights)\n",
    "                test_batch_loss.append(loss_test)\n",
    "            test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "        if e%50 == 0:\n",
    "            print(f'Epoch {e}')\n",
    "            print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "            print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "        # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "        # print(train_batch_loss)\n",
    "        # print(test_batch_loss)\n",
    "        # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "        # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "        # #! implementing early stopping\n",
    "        # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "        # print(f'Current val loss: {current_val_loss}')\n",
    "        # print(f'Best val loss: {best_val_loss}')\n",
    "        # if current_val_loss < best_val_loss:\n",
    "        #     best_val_loss = current_val_loss\n",
    "        #     patience_counter = 0  # reset patience counter\n",
    "        #     # Save the best model\n",
    "        #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     if patience_counter >= patience:\n",
    "        #         print(\"Early stopping triggered\")\n",
    "        #         torch.save({\n",
    "        #         'optimizer': optimizer.state_dict(),\n",
    "        #         'model': model.state_dict(),\n",
    "        #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "        #         break  # Early stopping\n",
    "            \n",
    "    print('==='*10)\n",
    "    # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "    save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "                train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    x = np.arange(1, epoch+1, 1)\n",
    "    ax.plot(x, train_epoch_loss,label='Training')\n",
    "    ax.plot(x, test_epoch_loss,label='Validation')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Number of Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "    ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "    # ax_2 = ax.twinx()\n",
    "    # ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "    # ax_2.set_yscale(\"log\")\n",
    "    # ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "    ax.grid(axis=\"x\")\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "    print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "    #%%\n",
    "    # testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "    testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Ensure the model is on the same device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    ic.disable()\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    target_list  = []\n",
    "    mse_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test, y_test_res in testing_loader1:\n",
    "            # Move input and target data to the correct device\n",
    "            x_test = x_test.to(device).float()\n",
    "            y_test = y_test.to(device).float()\n",
    "            y_test_res = y_test_res.to(device).float()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(x_test, y_test_res)\n",
    "            \n",
    "            # Append predictions and targets to lists\n",
    "            pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "            target_list.append(y_test.detach().cpu().numpy())\n",
    "            \n",
    "    # Flatten the target list\n",
    "    target_list = np.array(target_list).flatten()\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "    from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "    def calculate_metrics(true_labels, predictions):\n",
    "        \"\"\"\n",
    "        Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "        Parameters:\n",
    "        - true_labels: List or array of true labels\n",
    "        - predictions: List or array of predicted labels\n",
    "\n",
    "        Returns:\n",
    "        - accuracy: Overall accuracy of predictions\n",
    "        - f1: Weighted average F1 score\n",
    "        - conf_matrix: Multiclass confusion matrix\n",
    "        - mae: Mean Absolute Error of predictions\n",
    "        \"\"\"\n",
    "        # Ensure inputs are numpy arrays for consistency\n",
    "        true_labels = np.array(true_labels)\n",
    "        predictions = np.array(predictions)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "        # Calculate MAE\n",
    "        mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "        return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "    # Example usage\n",
    "    # true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "    # predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "    accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "    print(\"======================\")\n",
    "    # print(\"Model's Named Parameters:\")\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     print(f\"Name: {name}\")\n",
    "    #     print(f\"Shape: {param.size()}\")\n",
    "    #     print(f\"Requires grad: {param.requires_grad}\")\n",
    "    #     print('-----')\n",
    "    print(\"Optimizer details:\")\n",
    "    print(optimizer)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"Learning rate:\", param_group['lr'])\n",
    "        print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "        \n",
    "    print(\"======================\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Mae: {mae}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"conf_matrix: {conf_matrix}\")\n",
    "    print(\"======================\")\n",
    "    doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "    print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "\n",
    "    # Calculate AUC\n",
    "    cutoff = cutoff\n",
    "    test_target_bi = (target_list >= cutoff).astype(int)\n",
    "    test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "    auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "    print(\"AUC:\", auc)\n",
    "\n",
    "    # Calculate confusion matrix components\n",
    "    tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "    # Calculate sensitivity (recall)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "    # Calculate specificity\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One cycle lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–Ž        | 50/400 [02:28<17:21,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Training loss: 0.11537401378154755\n",
      "Validation loss: 0.14441072940826416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 100/400 [04:58<14:56,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100\n",
      "Training loss: 0.11210744082927704\n",
      "Validation loss: 0.1536770612001419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 150/400 [07:28<12:28,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150\n",
      "Training loss: 0.11056072264909744\n",
      "Validation loss: 0.16507422924041748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 200/400 [09:57<09:53,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200\n",
      "Training loss: 0.1097438856959343\n",
      "Validation loss: 0.17116251587867737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 250/400 [12:32<07:39,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250\n",
      "Training loss: 0.10918270796537399\n",
      "Validation loss: 0.1748015433549881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 300/400 [15:02<04:53,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300\n",
      "Training loss: 0.10868095606565475\n",
      "Validation loss: 0.17704497277736664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 350/400 [17:41<02:47,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350\n",
      "Training loss: 0.10827459394931793\n",
      "Validation loss: 0.18684180080890656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [20:27<00:00,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400\n",
      "Training loss: 0.10818441957235336\n",
      "Validation loss: 0.17728930711746216\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_1e-07_weighted_balanced.png-emb\n",
      "======================\n",
      "Optimizer details:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    base_momentum: 0.85\n",
      "    betas: (0.9499999993756804, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    initial_lr: 0.0004\n",
      "    lr: 4.006243171376002e-08\n",
      "    max_lr: 0.01\n",
      "    max_momentum: 0.95\n",
      "    maximize: False\n",
      "    min_lr: 4e-08\n",
      "    weight_decay: 0\n",
      ")\n",
      "Learning rate: 4.006243171376002e-08\n",
      "Weight decay: 0\n",
      "======================\n",
      "Accuracy: 0.47568523430592397\n",
      "Mae: 0.6578249336870027\n",
      "F1 Score: 0.44584107920478183\n",
      "conf_matrix: [[ 34  66   7   3   1   1]\n",
      " [ 63 285  32   9   5   0]\n",
      " [ 40 188  41  19   3   2]\n",
      " [  4  20   7  75  42   1]\n",
      " [  2   8   0  34  77  16]\n",
      " [  0   3   0   5  12  26]]\n",
      "======================\n",
      "Doubling Dilution Accuracy: 0.8992042440318302\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#input parameter\n",
    "lr = 1e-7\n",
    "epoch = 400\n",
    "conv_dropout_rate=0\n",
    "dense_dropout_rate=0\n",
    "# weight_decay=1e-8\n",
    "weight_decay=0\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "patience_counter = 0\n",
    "lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "batch_size = 128\n",
    "# lr = 0.0085\n",
    "# lr = 0.00002\n",
    "lr = lr\n",
    "######################################\n",
    "\n",
    "model = Model(\n",
    "num_classes=6,\n",
    "num_filters=64,\n",
    "num_conv_layers=2,\n",
    "# num_dense_neurons=256, # batch_size = 64\n",
    "num_dense_neurons=128, # batch_size = 64\n",
    "num_dense_layers=2,\n",
    "return_logits=False,\n",
    "conv_dropout_rate=conv_dropout_rate,\n",
    "dense_dropout_rate=dense_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "criterion = weighted_cross_entropy_loss_fn\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=epoch)\n",
    "# model = Model( #! way too memory intensive\n",
    "# num_classes=13,\n",
    "# num_filters=128,\n",
    "# num_conv_layers=2,\n",
    "# num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "# num_dense_layers=2,\n",
    "# return_logits=True,\n",
    "# conv_dropout_rate=0,\n",
    "# dense_dropout_rate=0\n",
    "# ).to(device)\n",
    "## early stopping\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "# test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = masked_weighted_MAE\n",
    "# criterion = masked_weighted_MSE\n",
    "# criterion = weighted_cross_entropy_loss_fn\n",
    "\n",
    "\n",
    "# criterion = masked_MAE\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "#%%\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "# ic.enable()\n",
    "ic.disable()\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    # print(f'Epoch {e}')\n",
    "    for x_train, y_train in train_loader:\n",
    "        x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "        y_batch = y_train.to(device)\n",
    "        x_batch = x_batch.float()\n",
    "        pred = model(x_batch.float())\n",
    "\n",
    "        # break\n",
    "        loss_train = criterion(pred,y_batch)\n",
    "\n",
    "        train_batch_loss.append(loss_train)        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update the learning rate\n",
    "\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # print('>> test')\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            y_batch = y_test.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float())\n",
    "\n",
    "            # pred = pred.unsqueeze(0)\n",
    "            # print(pred[:10])\n",
    "            # print(y_batch[:10])\n",
    "\n",
    "            loss_test = criterion(pred,y_batch)\n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "    if e % 50 == 0:\n",
    "        print(f'Epoch {e}')\n",
    "        print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "        print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    # #! implementing early stopping\n",
    "    # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "    # print(f'Current val loss: {current_val_loss}')\n",
    "    # print(f'Best val loss: {best_val_loss}')\n",
    "    # if current_val_loss < best_val_loss:\n",
    "    #     best_val_loss = current_val_loss\n",
    "    #     patience_counter = 0  # reset patience counter\n",
    "    #     # Save the best model\n",
    "    #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         torch.save({\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #         'model': model.state_dict(),\n",
    "    #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "    #         break  # Early stopping\n",
    "        \n",
    "print('==='*10)\n",
    "# torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "             train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, epoch+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "#%%\n",
    "\n",
    "model.eval()  # For inference\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in testing_loader1:\n",
    "        xtest1 = x_test.to(device).float()\n",
    "        ytest1 = y_test.to(device).float()\n",
    "        pred = model(xtest1)\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclical lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input parameter\n",
    "lr = 1e-7\n",
    "epoch = 250\n",
    "conv_dropout_rate=0.05\n",
    "dense_dropout_rate=0.5\n",
    "weight_decay=1e-8\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-9, max_lr=0.001)\n",
    "######################################\n",
    "\n",
    "model = Model(\n",
    "num_classes=3,\n",
    "num_filters=64,\n",
    "num_conv_layers=2,\n",
    "# num_dense_neurons=256, # batch_size = 64\n",
    "num_dense_neurons=128, # batch_size = 64\n",
    "num_dense_layers=2,\n",
    "return_logits=False,\n",
    "conv_dropout_rate=conv_dropout_rate,\n",
    "dense_dropout_rate=dense_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# model = Model( #! way too memory intensive\n",
    "# num_classes=13,\n",
    "# num_filters=128,\n",
    "# num_conv_layers=2,\n",
    "# num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "# num_dense_layers=2,\n",
    "# return_logits=True,\n",
    "# conv_dropout_rate=0,\n",
    "# dense_dropout_rate=0\n",
    "# ).to(device)\n",
    "## early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "patience_counter = 0\n",
    "lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "batch_size = 128\n",
    "# lr = 0.0085\n",
    "# lr = 0.00002\n",
    "lr = lr\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "# test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = masked_weighted_MAE\n",
    "# criterion = masked_weighted_MSE\n",
    "# criterion = weighted_cross_entropy_loss_fn\n",
    "# criterion = masked_MAE\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "# scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "#%%\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "# ic.enable()\n",
    "ic.disable()\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    # print(f'Epoch {e}')\n",
    "    for x_train, y_train in train_loader:\n",
    "        x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "        y_batch = y_train.to(device)\n",
    "        x_batch = x_batch.float()\n",
    "        pred = model(x_batch.float())\n",
    "\n",
    "        # break\n",
    "        loss_train = criterion(pred,y_batch)\n",
    "\n",
    "        train_batch_loss.append(loss_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update the learning rate\n",
    "\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # print('>> test')\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            y_batch = y_test.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float())\n",
    "            loss_test = loss_corn(pred, y_batch, 3, class_weights)\n",
    "\n",
    "            # pred = pred.unsqueeze(0)\n",
    "            # print(pred[:10])\n",
    "            # print(y_batch[:10])\n",
    "\n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "\n",
    "    print(f'Epoch {e}')\n",
    "    print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "    print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    # #! implementing early stopping\n",
    "    # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "    # print(f'Current val loss: {current_val_loss}')\n",
    "    # print(f'Best val loss: {best_val_loss}')\n",
    "    # if current_val_loss < best_val_loss:\n",
    "    #     best_val_loss = current_val_loss\n",
    "    #     patience_counter = 0  # reset patience counter\n",
    "    #     # Save the best model\n",
    "    #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         torch.save({\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #         'model': model.state_dict(),\n",
    "    #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "    #         break  # Early stopping\n",
    "        \n",
    "print('==='*10)\n",
    "# torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "             train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, epoch+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "# ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "#%%\n",
    "testing_dataset = Dataset(test_data, test_target, one_hot_dtype=torch.float, transform=False)\n",
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, collate_fn=collate_padded_batch, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "model.eval()  # For inference\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in testing_loader1:\n",
    "        xtest1 = x_test.to(device).float()\n",
    "        ytest1 = y_test.to(device).float()\n",
    "        pred = model(xtest1)\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_corn(logits, y_train, num_classes):\n",
    "    sets = []\n",
    "    for i in range(num_classes-1):\n",
    "        label_mask = y_train > i-1\n",
    "        label_tensor = (y_train[label_mask] > i).to(torch.int64)\n",
    "        sets.append((label_mask, label_tensor))\n",
    "\n",
    "    num_examples = 0\n",
    "    losses = 0.\n",
    "    for task_index, s in enumerate(sets):\n",
    "        train_examples = s[0]\n",
    "        train_labels = s[1]\n",
    "\n",
    "        if len(train_labels) < 1:\n",
    "            continue\n",
    "\n",
    "        num_examples += len(train_labels)\n",
    "        pred = logits[train_examples, task_index]\n",
    "\n",
    "        loss = -torch.sum(F.logsigmoid(pred)*train_labels\n",
    "                          + (F.logsigmoid(pred) - pred)*(1-train_labels)\n",
    "                          )\n",
    "        losses += loss\n",
    "    return losses/num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred  = torch.tensor([[0.0000, 0.5111],\n",
    "        [0.1329, 1.1051]], device='cuda:0')\n",
    "target = torch.tensor([0, 0], device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7275, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "out = loss_corn(pred, target, 3)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost with snps and fed in res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "#     # Ensure target_min and target_max are scalars\n",
    "#     target_min = target_min.item() if isinstance(target_min, np.ndarray) or isinstance(target_min, pd.Series) else target_min\n",
    "#     target_max = target_max.item() if isinstance(target_max, np.ndarray) or isinstance(target_max, pd.Series) else target_max\n",
    "\n",
    "#     # Create a range based on the scalar values of target_min and target_max\n",
    "#     dilution_range = np.arange(target_min - 1, target_max + 2, 1)\n",
    "    \n",
    "#     # Find the index of the target value\n",
    "#     index = np.where(dilution_range == target)[0][0]  # Use np.where to find the index\n",
    "    \n",
    "#     # Check if prediction is within the acceptable range\n",
    "#     return dilution_range[index - 1] <= pred <= dilution_range[index + 1]\n",
    "\n",
    "# Example usage\n",
    "target_min, target_max = cryptic_drs.min().values, cryptic_drs.max()\n",
    "\n",
    "# Load the data\n",
    "cryptic_drs = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_sample_drs_cryptic_emb.npy')\n",
    "cryptic_snps = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_snps_cryptic_emb.npy')\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "cryptic_drs = pd.DataFrame(cryptic_drs)\n",
    "\n",
    "# Combine the features and target variable\n",
    "data = cryptic_snps\n",
    "target = cryptic_drs\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the XGBoost model\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "target_min, target_max = cryptic_drs.min().values, cryptic_drs.max().values\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(y_pred, y_test)])\n",
    "print(f\"Doubling Dilution Accuracy: {doubling_dilution_accuracy * 100:.2f}%\")\n",
    "\n",
    "#testing\n",
    "cutoff = cutoff\n",
    "test_target_bi = (np.squeeze(np.array(y_test)) >= cutoff).astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.squeeze(np.array(y_pred)) >= cutoff).astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
