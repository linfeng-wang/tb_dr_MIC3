{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/nn_model_script_emb_test.py - starting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f81a8a8e4d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "print('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/nn_model_script_emb_test.py - starting')\n",
    "from captum.attr import Saliency, IntegratedGradients, LayerGradCam\n",
    "\n",
    "from array import array\n",
    "from cmath import nan\n",
    "from pyexpat import model\n",
    "import statistics\n",
    "from tkinter.ttk import Separator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torchviz import make_dot\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import variable\n",
    "from itertools import chain\n",
    "from sklearn import metrics as met\n",
    "import pickle\n",
    "from icecream import ic\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from importlib import reload\n",
    "# import util\n",
    "# import model_torch_simple\n",
    "# from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from icecream import ic\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#%%\n",
    "seed = 42\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# train_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/aa_data_train_gene.csv', delimiter = ',')\n",
    "# train_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/mic_aa_train_hml.csv')\n",
    "# train_target = train_target[['EMB_MIC']]\n",
    "# # don't touch test data, split out validation data from training data during training\n",
    "# # test_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_EMB/aa_data_test_pca4k.csv', delimiter = ',')\n",
    "# test_data = np.loadtxt('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/aa_data_test_gene.csv', delimiter = ',')\n",
    "# test_target = pd.read_csv('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/data_new_emb/mic_aa_test_hml.csv')\n",
    "# test_target = test_target[['EMB_MIC']]\n",
    "\n",
    "# all_data = np.concatenate((train_data, test_data), axis=0)\n",
    "# all_target = pd.concat((train_target, test_target), axis=0)\n",
    "\n",
    "# train_data, test_data, train_target, test_target = train_test_split(all_data, all_target, test_size=0.2, random_state=42, stratify=all_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(aa_array, encoded_mic):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Perform stratified train-test split\n",
    "    train_data, test_data, train_target, test_target = train_test_split(\n",
    "        aa_array,\n",
    "        encoded_mic,\n",
    "        test_size=0.1,  # 10% for testing\n",
    "        stratify=encoded_mic.iloc[:,0],  # Ensures the proportion of each class is preserved\n",
    "        random_state=42  # For reproducibility\n",
    "    )\n",
    "    return train_data, test_data, train_target, test_target\n",
    "\n",
    "# def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "#     _ = np.arange(target_min-1, target_max+2, 1)\n",
    "#     index = [i for i, x in enumerate(_) if x == target][0]\n",
    "#     return (_[index-1] <= pred <= _[index+1])\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min.values-1, target_max.values+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "# def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "#     # Convert Series to scalar if needed\n",
    "#     if isinstance(pred, pd.Series):\n",
    "#         pred = pred.iloc[0]  # or pred.item()\n",
    "#     if isinstance(target, pd.Series):\n",
    "#         target = target.iloc[0]  # or target.item()\n",
    "\n",
    "#     _ = np.arange(target_min - 1, target_max + 2, 1)\n",
    "#     # Now target is guaranteed to be a scalar\n",
    "#     index = [i for i, x in enumerate(_) if x == target][0]\n",
    "#     return (_[index - 1] <= pred <= _[index + 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_(cryptic, gene_list):\n",
    "    # overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    # variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    # variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    \n",
    "    variants = pd.read_csv('../variants_full.csv')\n",
    "    variants = variants[variants['gene'] != 'PPE35']\n",
    "    variants = variants[variants['type'] != 'synonymous_variant']\n",
    "    variants = variants[variants['type'] != 'non_coding_transcript_exon_variant']\n",
    "\n",
    "    overlap = set(variants['sample_id']).intersection(set(cryptic['ENA_RUN'].to_list()))\n",
    "    # variants = variants[variants['drugs'].isin(['ethambutol'])]\n",
    "    variants = variants[variants['gene'].isin(gene_list)]\n",
    "    variants = variants[variants['sample_id'].isin(overlap)]\n",
    "    variants['SNP'] = variants['gene'] + '-'+ variants['change']\n",
    "    # print(variants.shape)\n",
    "    # print(variants['sample_id'].unique().shape)\n",
    "\n",
    "    def compare_snp_lists_with_values_optimized(set_list, query_list, values_list):\n",
    "        # Create a dictionary from query_list and values_list for direct mapping\n",
    "        query_dict = dict(zip(query_list, values_list))\n",
    "        \n",
    "        # Use list comprehension to build the output list directly\n",
    "        output_list = [query_dict.get(snp, 0) for snp in set_list]\n",
    "        \n",
    "        return output_list\n",
    "\n",
    "    # Example usage\n",
    "    # set_list = ['SNP1', 'SNP2', 'SNP3', 'SNP4']\n",
    "    # query_list = ['SNP2', 'SNP4']\n",
    "    # values_list = [5, 10]  # Corresponding values for 'SNP2' and 'SNP4'\n",
    "    # output_list = compare_snp_lists_with_values_optimized(set_list, query_list, values_list)\n",
    "    # print(output_list)  # Expected output: [0, 5, 0, 10]# Getting all snp data\n",
    "\n",
    "    aa = []\n",
    "    all_snp = variants['SNP'].unique() # here is a list of all snps values title for the row in the final table \n",
    "    print('Getting all snp data', len(all_snp))\n",
    "    for x in tqdm(overlap):\n",
    "    # for x in tqdm(variants['sample_id'].unique()):\n",
    "        if x in variants['sample_id'].tolist():\n",
    "            aa.append(compare_snp_lists_with_values_optimized(all_snp, variants[variants['sample_id']==x]['SNP'].to_list(), variants[variants['sample_id']==x]['freq'].to_list()))\n",
    "        else:\n",
    "            aa.append([0]*len(all_snp))\n",
    "        # print('SNP')\n",
    "        \n",
    "    aa_array = np.array(aa)\n",
    "    aa_array[aa_array < 0.8] = 0\n",
    "    aa_array[aa_array >= 0.8] = 1\n",
    "\n",
    "    mic_aa = cryptic[cryptic['ENA_RUN'].isin(overlap)]#.iloc[:,14:27]\n",
    "    # mic_aa = cryptic[cryptic['ENA_RUN'].isin(variants['sample_id'].unique())]#.iloc[:,14:27]\n",
    "    # print(mic_aa.shape)\n",
    "    # mic_aa['wgs_id'] = pd.Categorical(mic_aa['ENA_RUN'], categories=variants['sample_id'].unique().tolist(), ordered=True)\n",
    "    # mic_aa = mic_aa.sort_values('ENA_RUN')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.astype('category')\n",
    "    mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(overlap)\n",
    "    # mic_aa.ENA_RUN = mic_aa.ENA_RUN.cat.set_categories(variants['sample_id'].unique().tolist())\n",
    "    mic_aa = mic_aa.sort_values([\"ENA_RUN\"])  ## 'sort' changed to 'sort_values'\n",
    "    # print(mic_aa.shape)\n",
    "\n",
    "    return aa_array, mic_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug = 'EMB'\n",
    "df = pd.read_csv('../CRyPTIC_reuse_table_20231208.csv')\n",
    "\n",
    "aa_array = np.load(f'./generated_data18122024/all_sample_snps_cryptic_{drug}.npy')\n",
    "drs = np.load(f'./generated_data18122024/all_sample_drs_cryptic_{drug}.npy')\n",
    "\n",
    "encoded_mic = pd.DataFrame(drs, columns=['EMB_MIC'])\n",
    "mic_series = np.log2(encoded_mic)\n",
    "mic_series += 1\n",
    "# sample_ids = mic_aa['ENA_RUN']\n",
    "\n",
    "mic_series_bi = encoded_mic['EMB_MIC'].apply(lambda x: 1 if x >= 8 else 0)\n",
    "mic_series_all = pd.merge(mic_series, mic_series_bi, left_index=True, right_index=True)\n",
    "# train_data, test_data,  test_target_y, test_target = data_split(aa_array, mic_series_all)\n",
    "train_data, test_data, train_target, test_target = data_split(aa_array, mic_series_all)\n",
    "\n",
    "target_min, target_max = mic_series.min(), mic_series.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11362 entries, 0 to 11361\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   EMB_MIC_x  11362 non-null  float64\n",
      " 1   EMB_MIC_y  11362 non-null  int64  \n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 177.7 KB\n"
     ]
    }
   ],
   "source": [
    "mic_series_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = np.load('./train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(train_data, train_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1.0: 3571, 2.0: 2661, 3.0: 2000, 4.0: 2000, 0.0: 2000, 5.0: 2000})\n",
      "(14232,)\n",
      "(14232, 1710)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Assuming train_data is your feature array and train_target['EMB_MIC_x'] is your target array\n",
    "X = train_data\n",
    "y = train_target['EMB_MIC_x']\n",
    "\n",
    "target_counts = {\n",
    "    1.0: 3571,  # Keep the majority class as is\n",
    "    2.0: 2661,  # Bring other classes closer\n",
    "    3.0: 2000,\n",
    "    4.0: 2000,\n",
    "    0.0: 2000,\n",
    "    5.0: 2000\n",
    "}\n",
    "# Initialize the RandomOverSampler\n",
    "ros= RandomOverSampler(sampling_strategy=target_counts, random_state=42)\n",
    "# ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Fit and resample the data\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Verify the new class distribution\n",
    "from collections import Counter\n",
    "print(Counter(y_resampled))\n",
    "print(y_resampled.shape)\n",
    "print(X_resampled.shape)\n",
    "\n",
    "# train_mic_series_bi = y_resampled.apply(lambda x: 1 if x >= 4 else 0)\n",
    "# train_target = pd.DataFrame({'EMB_MIC_x': y_resampled, 'EMB_MIC_y': train_mic_series_bi})\n",
    "# train_data = X_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mic_series_bi = y_resampled.apply(lambda x: 1 if x >= 4 else 0)\n",
    "train_target = pd.DataFrame({'EMB_MIC_x': y_resampled, 'EMB_MIC_y': train_mic_series_bi})\n",
    "train_data = X_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14232, 1710)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cornloss weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_counts = torch.from_numpy(train_target.values).flatten()\n",
    "# train_target_counts = torch.tensor([0,1,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "drug = 'EMB'\n",
    "df = pd.read_csv('../CRyPTIC_reuse_table_20231208.csv')\n",
    "\n",
    "aa_array = np.load(f'./generated_data18122024/all_sample_snps_50k_{drug}.npy')\n",
    "drs = np.load(f'./generated_data18122024/all_sample_drs_50k_{drug}.npy')\n",
    "aa_array_50k_pos = aa_array\n",
    "mic_series_50k_pos = drs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa_array_50k_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgb training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Prepare your training and testing data\n",
    "# ------------------------------------------------\n",
    "\n",
    "# Assume you already have:\n",
    "#   train_data, test_data\n",
    "#   train_target, test_target\n",
    "#   aa_array_50k_pos, mic_series_50k_pos \n",
    "#   => as per your code snippet.\n",
    "\n",
    "train_target_y = train_target['EMB_MIC_y'].values\n",
    "test_target_y  = test_target['EMB_MIC_y'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_data, test_data, train_target_y, test_target_y\n",
    "\n",
    "# Optionally concatenate the additional data you mentioned:\n",
    "X_train = np.concatenate((X_train, aa_array_50k_pos), axis=0)\n",
    "y_train = np.concatenate((y_train, mic_series_50k_pos), axis=0)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Define a function for checking doubling dilution \n",
    "# (if needed)\n",
    "# ------------------------------------------------\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Set up the XGBoost model & parameter grid\n",
    "# ------------------------------------------------\n",
    "# Create a base model\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Define a parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3], \n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    # 'reg_alpha': [0, 1, 10],       # optionally add\n",
    "    # 'reg_lambda': [0, 1, 10],     # optionally add\n",
    "}\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Run GridSearchCV\n",
    "# ------------------------------------------------\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',      # You can choose other metrics, e.g., 'roc_auc'\n",
    "    cv=3,                    # 3-fold cross-validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1               # Use all available CPU cores\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Evaluate on the test set\n",
    "# ------------------------------------------------\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Binarize predictions at cutoff=4 (per your example)\n",
    "cutoff = 4\n",
    "test_target_bi = y_test.astype(int)\n",
    "test_predictions_bi = y_pred.astype(int)\n",
    "\n",
    "# Compute AUC\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Compute confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_target_bi, test_predictions_bi))\n",
    "\n",
    "# Compute sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Compute specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8865435356200527\n",
      "AUC: 0.8705622572773826\n",
      "[[853 101]\n",
      " [ 28 155]]\n",
      "Sensitivity: 0.8469945355191257\n",
      "Specificity: 0.8941299790356394\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_data = np.column_stack((train_data, train_target['EMB_MIC_y'].values))\n",
    "# test_data = np.column_stack((test_data, test_target['EMB_MIC_y'].values))\n",
    "train_target_y = train_target['EMB_MIC_y'].values\n",
    "test_target_y = test_target['EMB_MIC_y'].values\n",
    "\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min-1, target_max+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_data, test_data, train_target_y, test_target_y\n",
    "X_test = test_data\n",
    "y_test = test_target['EMB_MIC_y'].values\n",
    "X_train = np.concatenate((X_train, aa_array_50k_pos), axis=0)\n",
    "y_train = np.concatenate((y_train, mic_series_50k_pos), axis=0)\n",
    "# # Create the XGBoost model\n",
    "# model_bi = xgb.XGBClassifier(    \n",
    "#     max_depth=2,\n",
    "#     learning_rate=0.9,\n",
    "#     n_estimators=4,\n",
    "#     # gamma=0.1,\n",
    "#     # min_child_weight=24,\n",
    "#     # subsample=0.2,\n",
    "#     # colsample_bytree=1,\n",
    "#     # reg_alpha=15, reg_lambda=15,\n",
    "#     random_state=42 \n",
    "#     )\n",
    "# # Create the XGBoost model\n",
    "\n",
    "# model_bi = xgb.XGBClassifier(colsample_bytree= 0.5, gamma= 5, learning_rate= 0.9, max_depth= 13, min_child_weight= 1, n_estimators= 200, subsample= 0.8,random_state=42)\n",
    "model_bi = xgb.XGBClassifier(colsample_bytree= 0.9, gamma= 5, learning_rate= 2, max_depth= 13, min_child_weight= 1, n_estimators= 200, subsample= 0.8,random_state=42, reg_alpha=100, reg_lambda=100)\n",
    "\n",
    "model_bi.fit(X_train, y_train)\n",
    "\n",
    "# model_bi = pickle.load(open(\"xgb_bi_mix1.pkl\", \"rb\"))\n",
    "# model_bi = pickle.load(open(\"xgb_bi_mix.pkl\", \"rb\"))\n",
    "# model_bi = pickle.load(open(\"xgb_bi_50kbalanced.pkl\", \"rb\"))\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model_bi.predict(X_test)\n",
    "\n",
    "# Evaluate the model_bi\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "#testing\n",
    "cutoff = 4\n",
    "test_target_bi = y_test.astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "test_predictions_bi = y_pred.astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "print(confusion_matrix(test_target_bi, test_predictions_bi))\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)  \n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)\n",
    "\n",
    "# with open(f\"/saved_model1115/xgb_bin_model-{drug}.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(model_bi, file)\n",
    "    \n",
    "model_bi.save_model(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC3/individual_models/saved_model1115/xgb_bin_model-{drug}.json')  # or \"xgb_model.bin\"\n",
    "# loaded_model.load_model('/mnt/storageG1/lwang/Projects/tb_dr_MIC3/individual_models/saved_model1115/xgb_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_bi.save_model('/mnt/storageG1/lwang/Projects/tb_dr_MIC3/individual_models/saved_model1115/xgb_model_bi.json')  # or \"xgb_model.bin\"\n",
    "# loaded_model.load_model('/mnt/storageG1/lwang/Projects/tb_dr_MIC3/individual_models/saved_model1115/xgb_model.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target['EMB_MIC_y'] = test_predictions_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14232"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "from collections import Counter\n",
    "\n",
    "N_samples = train_data.shape[0]\n",
    "DRUGS = train_target.columns\n",
    "# LOCI = train_data.columns\n",
    "assert set(DRUGS) == set(train_target.columns)\n",
    "N_drugs = len(DRUGS)\n",
    "#%%\n",
    "\n",
    "def my_padding(seq_tuple):\n",
    "    list_x_ = list(seq_tuple)\n",
    "    max_len = len(max(list_x_, key=len))\n",
    "    for i, x in enumerate(list_x_):\n",
    "        list_x_[i] = x + \"N\"*(max_len-len(x))\n",
    "    return list_x_\n",
    "\n",
    "#! faster than my_padding try to incorporate\n",
    "def collate_padded_batch(batch):\n",
    "    # get max length of seqs in batch\n",
    "    max_len = max([x[0].shape[1] for x in batch])\n",
    "    return torch.utils.data.default_collate(\n",
    "        [(F.pad(x[0], (0, max_len - x[0].shape[1])), x[1]) for x in batch] #how does F.pad work\n",
    "    )\n",
    "\n",
    "# Julian's code - implement this, might be faster\n",
    "class Dataset(torch.utils.data.Dataset): #? what's the difference between using inheritance and not?\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_df,\n",
    "        res_df,\n",
    "        # target_loci=LOCI,\n",
    "        target_mic,\n",
    "        target_res,\n",
    "        one_hot_dtype=torch.int8,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        # self.seq_df = seq_df[target_loci]\n",
    "        self.seq_df = seq_df\n",
    "        self.res_df = res_df[target_res]\n",
    "        self.mic_df = res_df[target_mic]\n",
    "        # if not self.seq_df.index.equals(self.res_df.index):\n",
    "        #     raise ValueError(\n",
    "        #         \"Indices of sequence and resistance dataframes don't match up\"\n",
    "        #     )\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        numerical index --> get `index`-th sample\n",
    "        string index --> get sample with name `index`\n",
    "        \"\"\"\n",
    "        index = int(index)\n",
    "        if isinstance(index, int):\n",
    "            seqs_comb = self.seq_df[index]\n",
    "            res = self.res_df.iloc[index]\n",
    "            mic = self.mic_df.iloc[index]\n",
    "        elif isinstance(index, str):\n",
    "            seqs_comb = self.seq_df[int(index)]\n",
    "            res = self.res_df.loc[index]\n",
    "            mic = self.mic_df.loc[index]\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Index needs to be an integer or a sample name present in the dataset\"\n",
    "            )\n",
    "\n",
    "        if self.transform:\n",
    "            res = np.log(res)\n",
    "            \n",
    "            # self.res_mean = self.res_df.mean()\n",
    "            # self.res_std = self.res_df.std()\n",
    "            # res = (res - self.res_mean) / self.res_std\n",
    "            # res = self.transform(res)\n",
    "        return torch.unsqueeze(torch.tensor(seqs_comb).float(), 0), torch.tensor(mic).long().flatten().squeeze(), torch.tensor(res).long().flatten().squeeze()\n",
    "    def __len__(self):\n",
    "        return self.res_df.shape[0]\n",
    "\n",
    "training_dataset = Dataset(train_data, train_target, 'EMB_MIC_x','EMB_MIC_y', one_hot_dtype=torch.float, transform=False)\n",
    "# train_dataset, val_dataset = random_split(training_dataset, [int(len(training_dataset)*0.9), len(training_dataset)-int(len(training_dataset)*0.9)])\n",
    "\n",
    "# test_data = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/snps_crypticTest_emb.npy')\n",
    "# train_data = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/drs_crypticTest_emb.npy')\n",
    "testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "\n",
    "train_idx, validation_idx = train_test_split(np.arange(len(train_data)),\n",
    "                                             test_size=0.1,\n",
    "                                             random_state=42,\n",
    "                                             shuffle=True,\n",
    "                                             stratify=train_target)\n",
    "\n",
    "# Subset dataset for train and val\n",
    "train_dataset = Subset(training_dataset, train_idx)\n",
    "val_dataset = Subset(training_dataset, validation_idx)\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# # device = 'cpu'\n",
    "\n",
    "y_true = train_target\n",
    "# y_true = pd.concat([train_target, test_target])\n",
    "\n",
    "column_weight_maps = {}\n",
    "\n",
    "for column in y_true.columns:\n",
    "    column_values = y_true[column].dropna().values\n",
    "    values, counts = np.unique(column_values, return_counts=True)\n",
    "    frequency = counts / len(column_values)\n",
    "    \n",
    "    # Calculate weights as the inverse of frequencies\n",
    "    weights_inverse = 1/frequency\n",
    "    # weights_inverse = 1 - frequency\n",
    "    \n",
    "    # Normalize weights to ensure they sum up to 1\n",
    "    weights_normalized = weights_inverse / np.sum(weights_inverse)\n",
    "    \n",
    "    # Map each MIC value to its corresponding weight\n",
    "    weight_map = {value: weight for value, weight in zip(values, weights_normalized)}\n",
    "    \n",
    "    column_weight_maps[column] = weight_map\n",
    "\n",
    "def get_weighted_masked_cross_entropy_loss(column_weight_maps):\n",
    "    \"\"\"\n",
    "    Creates a loss function that computes a weighted cross entropy loss, taking into account class imbalances.\n",
    "    :param column_weight_maps: Dictionary mapping column names to their corresponding class weight maps.\n",
    "    \"\"\"\n",
    "    def weighted_masked_cross_entropy_loss(y_pred, y_true):\n",
    "        # weighted_losses = torch.Tensor().to(device)\n",
    "        weighted_losses = []\n",
    "        col_weight_map = column_weight_maps\n",
    "        # print(col_weight_map)\n",
    "        mean_weight = np.mean(list(col_weight_map.values())) # just in case if a number is not recognised and the loss doesn't go crazy\n",
    "\n",
    "        # print(y_pred.size())\n",
    "        # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        weights_col = [col_weight_map.get(y.item(), mean_weight) for y in y_true]\n",
    "        # print(weights_col)\n",
    "        # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        loss_fn = F.cross_entropy\n",
    "        col_loss = loss_fn(y_pred, y_true, reduction = 'none').to(device)\n",
    "        \n",
    "        # loss_fn = nn.CrossEntropyLoss(reduction = 'none')\n",
    "        # col_loss = loss_fn(y_pred, y_true)\n",
    "        # print(y_true.dtype)\n",
    "        # print(col_loss)\n",
    "        weights_col = torch.Tensor(weights_col).to(device)\n",
    "        # print(weights_col)\n",
    "        # print(col_loss)\n",
    "        weighted_col_loss = weights_col * col_loss\n",
    "        # print(weighted_col_loss)\n",
    "        weighted_losses.append(weighted_col_loss.mean())\n",
    "\n",
    "        total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        \n",
    "        # for i, column in enumerate(column_weight_maps.keys()):\n",
    "        #     col_weight_map = column_weight_maps[column]\n",
    "        #     print(y_pred.size())\n",
    "        #     # Assuming y_true is a tensor of class indices for each column and y_pred are the logits\n",
    "        #     weights_col = torch.tensor([col_weight_map[y.item()] for y in y_true[:, i]], dtype=torch.float32, device=y_true.device)\n",
    "        #     print(weights_col)\n",
    "        #     # CrossEntropyLoss expects class indices as y_true, and logits as y_pred\n",
    "        #     loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        #     col_loss = loss_fn(y_pred[:, i,], y_true[:, i])\n",
    "            \n",
    "        #     weighted_col_loss = weights_col * col_loss\n",
    "        #     weighted_losses.append(weighted_col_loss.mean())\n",
    "        \n",
    "        # total_weighted_loss = torch.stack(weighted_losses).mean()\n",
    "        return total_weighted_loss\n",
    "\n",
    "    return weighted_masked_cross_entropy_loss\n",
    "\n",
    "# Also assuming `columns` is a list of your target column names corresponding to y_true and y_pred\n",
    "weighted_cross_entropy_loss_fn = get_weighted_masked_cross_entropy_loss(column_weight_maps['EMB_MIC_x'])\n",
    "# loss = weighted_cross_entropy_loss_fn(y_true_tensor, y_pred_logits, columns)\n",
    "\n",
    "def save_to_file(file_path, appendix, epoch, lr, cnndr, fcdr, l2, train_loss, test_loss, optimizer, model):\n",
    "    train_loss = [float(arr) for arr in train_loss]\n",
    "    test_loss = [float(arr) for arr in test_loss]\n",
    "    with open(file_path, \"a\") as f:\n",
    "        f.write(f\"#>> {appendix}, Epoch: {epoch}, LR: {lr}, fcDR: {fcdr}\\n\")\n",
    "        f.write(f\"Train_Loss= {train_loss}\\n\")\n",
    "        f.write(f\"Test_Loss= {test_loss}\\n\")\n",
    "        f.write(f\"lossGraph(Train_Loss, Test_Loss, '{appendix}-Epoch-{epoch}-LR-{lr}-fcDR-{fcdr}')\\n\")\n",
    "\n",
    "    torch.save({\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model': model.state_dict(),\n",
    "    }, f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/seq-{appendix}-{epoch}-{lr}-{cnndr}-{fcdr}-{l2}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_data = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/snps_crypticTest_emb.npy')\n",
    "# test_target = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/drs_crypticTest_emb.npy')\n",
    "# testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "# testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        num_classes=6,\n",
    "        num_filters=64,\n",
    "        filter_length=25,\n",
    "        num_conv_layers=2,\n",
    "        filter_scaling_factor=1,  # New parameter\n",
    "        num_dense_neurons=256,\n",
    "        num_dense_layers=2,\n",
    "        conv_dropout_rate=0.0,\n",
    "        dense_dropout_rate=0.2,\n",
    "        l1_strength = 0.1,\n",
    "        return_logits=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_length = filter_length\n",
    "        self.num_conv_layers = num_conv_layers\n",
    "        self.num_dense_layers = num_dense_layers\n",
    "        self.conv_dropout_rate = conv_dropout_rate\n",
    "        self.dense_dropout_rate = dense_dropout_rate\n",
    "        self.return_logits = return_logits\n",
    "        \n",
    "        # now define the actual model\n",
    "        # self.feature_extraction_layer = self._conv_layer(\n",
    "            # in_channels, num_filters, filter_length\n",
    "        # )\n",
    "        self.feature_extraction_layer = self._conv_layer_extract(\n",
    "            in_channels, num_filters, filter_length\n",
    "        )\n",
    "        #dynamic filter scaling from deepram\n",
    "        current_num_filters1 = num_filters\n",
    "        self.conv_layers1 = nn.ModuleList()\n",
    "        for i in range(num_conv_layers):\n",
    "            layer = self._conv_layer(current_num_filters1, int(current_num_filters1 * filter_scaling_factor), 3)\n",
    "            self.conv_layers1.append(layer)\n",
    "            current_num_filters1 = int(current_num_filters1 * filter_scaling_factor)\n",
    "            \n",
    "        current_num_filters2 = 32\n",
    "        self.conv_layers2 = nn.ModuleList()\n",
    "        for i in range(num_conv_layers):\n",
    "            layer = self._conv_layer(current_num_filters1, int(current_num_filters2 * filter_scaling_factor), 3)\n",
    "            self.conv_layers2.append(layer)\n",
    "            current_num_filters1 = current_num_filters2\n",
    "            \n",
    "        self.dense_layers = nn.ModuleList(\n",
    "            self._dense_layer(input_dim, num_dense_neurons)\n",
    "            for input_dim in [53568]\n",
    "            + [num_dense_neurons] * (num_dense_layers - 1) #how does this work?\n",
    "        )\n",
    "        \n",
    "        # self.dense_layers = nn.ModuleList(\n",
    "            # self._dense_layer(input_dim, num_dense_neurons)\n",
    "            # for input_dim in [current_num_filters2]\n",
    "            # + [num_dense_neurons] * (num_dense_layers - 1) #how does this work?\n",
    "        # )\n",
    "        \n",
    "        # self.prediction_layer = (\n",
    "        #     nn.Linear(num_dense_neurons, num_classes)\n",
    "        #     if return_logits\n",
    "        #     else nn.Sequential(nn.Linear(num_dense_neurons, num_classes), nn.ReLU()) #difference between sequential and nn.moduleList?\n",
    "        # )\n",
    "        \n",
    "        dense_output_size = num_dense_neurons  # Assuming dense layer output is num_dense_neurons\n",
    "        additional_input_size = 1  # Assuming additional input is a single value\n",
    "        total_input_size = dense_output_size + additional_input_size  # Total input size for the prediction layer\n",
    "\n",
    "        self.prediction_layer = (\n",
    "            nn.Linear(total_input_size, num_classes)  # If logits are returned directly\n",
    "            if return_logits\n",
    "            else nn.Sequential(\n",
    "                nn.Linear(total_input_size, int(total_input_size * 0.7)),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dense_dropout_rate),  # Dropout layer after the first ReLU activation\n",
    "                nn.Linear(int(total_input_size * 0.7), int(total_input_size * 0.5)),  # Optional additional layer with reduced size\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.dense_dropout_rate),  # Dropout layer after the second ReLU activation\n",
    "                nn.Linear(int(total_input_size * 0.5), num_classes)  # Final layer to match the number of classes\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.m = nn.MaxPool1d(3, stride=1)\n",
    "        \n",
    "        self.apply(self.init_weights)    \n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _conv_layer(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(p=self.conv_dropout_rate),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def _conv_layer_extract(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def _dense_layer(self, n_in, n_out):\n",
    "        return nn.Sequential(\n",
    "            nn.Dropout(p=self.dense_dropout_rate),\n",
    "            nn.Linear(n_in, n_out),\n",
    "            nn.BatchNorm1d(n_out),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def l1_regularization(self):\n",
    "        l1_loss_example = 0\n",
    "        for param in self.parameters():\n",
    "            l1_loss_example += torch.sum(torch.abs(param))\n",
    "        return self.l1_strength * l1_loss_example\n",
    "\n",
    "    def forward(self, x, additional_input):\n",
    "        # Feature extraction\n",
    "        x = self.feature_extraction_layer(x)\n",
    "\n",
    "        # Convolutional layers\n",
    "        for layer in self.conv_layers1:\n",
    "            x = layer(x)\n",
    "        x = self.m(x)\n",
    "        for layer in self.conv_layers2:\n",
    "            x = layer(x)\n",
    "        x = self.m(x)\n",
    "        \n",
    "        # Flatten the tensor to [batch_size, features]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Dense layers\n",
    "        for layer in self.dense_layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        additional_input = additional_input.unsqueeze(1)\n",
    "        # Concatenate additional input value\n",
    "        x = torch.cat((x, additional_input), dim=1)  # Concatenate along the feature dimension\n",
    "\n",
    "        # Prediction layer\n",
    "        x = self.prediction_layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# def l1loss(layer): # https://stackoverflow.com/questions/50054049/lack-of-sparse-solution-with-l1-regularization-in-pytorch\n",
    "#     return torch.norm(layer.weight, p=1)\n",
    "\n",
    "# def l1loss(sequence):\n",
    "#     l1_regularization = 0\n",
    "#     for module in sequence.modules():\n",
    "#         if isinstance(module, nn.Conv1d):  # Check if the module is a Conv1d layer\n",
    "#             l1_regularization += torch.norm(module.weight, p=1)\n",
    "#     return l1_regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 50/800 [03:19<50:24,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Training loss: 0.19505354762077332\n",
      "Validation loss: 0.16984732449054718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 100/800 [06:39<46:47,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100\n",
      "Training loss: 0.16412292420864105\n",
      "Validation loss: 0.1521443873643875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 150/800 [10:00<43:26,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150\n",
      "Training loss: 0.15781834721565247\n",
      "Validation loss: 0.14860525727272034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 200/800 [13:21<40:12,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200\n",
      "Training loss: 0.15225587785243988\n",
      "Validation loss: 0.1439814567565918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 250/800 [16:41<36:41,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250\n",
      "Training loss: 0.1504218578338623\n",
      "Validation loss: 0.1422388106584549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 300/800 [20:02<33:38,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300\n",
      "Training loss: 0.1489642709493637\n",
      "Validation loss: 0.1400822103023529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 350/800 [23:22<30:04,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350\n",
      "Training loss: 0.14723172783851624\n",
      "Validation loss: 0.1388765126466751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 400/800 [26:43<26:35,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400\n",
      "Training loss: 0.14648059010505676\n",
      "Validation loss: 0.13894334435462952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 450/800 [30:04<23:25,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 450\n",
      "Training loss: 0.1468563675880432\n",
      "Validation loss: 0.13936768472194672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 500/800 [33:26<20:17,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500\n",
      "Training loss: 0.14592976868152618\n",
      "Validation loss: 0.1389073133468628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 550/800 [36:48<16:42,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 550\n",
      "Training loss: 0.1456645429134369\n",
      "Validation loss: 0.13909706473350525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 600/800 [40:09<13:24,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 600\n",
      "Training loss: 0.14505527913570404\n",
      "Validation loss: 0.138798788189888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 650/800 [43:31<10:06,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 650\n",
      "Training loss: 0.14463089406490326\n",
      "Validation loss: 0.13828271627426147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 700/800 [46:53<06:41,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 700\n",
      "Training loss: 0.14581100642681122\n",
      "Validation loss: 0.1385103166103363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 750/800 [50:15<03:21,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 750\n",
      "Training loss: 0.14468100666999817\n",
      "Validation loss: 0.13898368179798126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [53:36<00:00,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 800\n",
      "Training loss: 0.14437419176101685\n",
      "Validation loss: 0.1377830058336258\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_30698/2946617280.py:174: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_0.0001_weighted_balanced.png-emb\n",
      "======================\n",
      "Optimizer details:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "Learning rate: 0.0001\n",
      "Weight decay: 0.0001\n",
      "======================\n",
      "Accuracy: 0.47493403693931396\n",
      "Mae: 0.6411609498680739\n",
      "F1 Score: 0.37911313862075313\n",
      "conf_matrix: [[ 10  96   0   3   2   1]\n",
      " [ 13 366   5   7   5   1]\n",
      " [  8 243   8  19  17   1]\n",
      " [  2  34   1  38  70   4]\n",
      " [  2   5   0  17  95  18]\n",
      " [  0   3   1   0  19  23]]\n",
      "======================\n",
      "Doubling Dilution Accuracy: 0.9155672823218998\n",
      "AUC: 0.8705622572773826\n",
      "Sensitivity: 0.8469945355191257\n",
      "Specificity: 0.8941299790356394\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#input parameter\n",
    "lr = 1e-4\n",
    "epoch = 800\n",
    "conv_dropout_rate=0.4\n",
    "dense_dropout_rate=0.7\n",
    "weight_decay=1e-4\n",
    "######################################\n",
    "\n",
    "model = Model(\n",
    "num_classes=6,\n",
    "num_filters=64,\n",
    "num_conv_layers=2,\n",
    "# num_dense_neurons=256, # batch_size = 64\n",
    "num_dense_neurons=128, # batch_size = 64\n",
    "num_dense_layers=2,\n",
    "return_logits=False,\n",
    "conv_dropout_rate=conv_dropout_rate,\n",
    "dense_dropout_rate=dense_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# model = Model( #! way too memory intensive\n",
    "# num_classes=13,\n",
    "# num_filters=128,\n",
    "# num_conv_layers=2,\n",
    "# num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "# num_dense_layers=2,\n",
    "# return_logits=True,\n",
    "# conv_dropout_rate=0,\n",
    "# dense_dropout_rate=0\n",
    "# ).to(device)\n",
    "## early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 15  # How many epochs to wait after last time validation loss improved.\n",
    "patience_counter = 0\n",
    "lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "batch_size = 64\n",
    "# lr = 0.0085\n",
    "# lr = 0.00002\n",
    "lr = lr\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "# test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = masked_weighted_MAE\n",
    "# criterion = masked_weighted_MSE\n",
    "criterion = weighted_cross_entropy_loss_fn\n",
    "# criterion = masked_MAE\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "# scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "#%%\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "# ic.enable()\n",
    "ic.disable()\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    # print(f'Epoch {e}')\n",
    "    for x_train, y_train, y_train_res in train_loader:\n",
    "        x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "        y_batch = y_train.to(device)\n",
    "        y_batch_res = y_train_res.to(device)\n",
    "        \n",
    "        x_batch = x_batch.float()\n",
    "        pred = model(x_batch.float(),y_batch_res.float())\n",
    "        # break\n",
    "        # loss_train = loss_corn(pred, y_batch, 3, class_weights)\n",
    "        # print(pred, y_batch)\n",
    "        loss_train = criterion(pred,y_batch)\n",
    "        # print(pred)\n",
    "        # print(y_batch)\n",
    "        # print(loss_train)\n",
    "        train_batch_loss.append(loss_train)        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update the learning rate\n",
    "        # break\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # print('>> test')\n",
    "        for x_test, y_test, y_test_res in test_loader:\n",
    "            x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            y_batch = y_test.to(device)\n",
    "            y_batch_res = y_test_res.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float(), y_batch_res.float())\n",
    "            loss_test = criterion(pred,y_batch)\n",
    "            # pred = pred.unsqueeze(0)\n",
    "            # print(pred[:10])\n",
    "            # print(y_batch[:10])\n",
    "            \n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "    # early stopping\n",
    "            \n",
    "            \n",
    "    if e%50 == 0:\n",
    "        print(f'Epoch {e}')\n",
    "        print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "        print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    # #! implementing early stopping\n",
    "    current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "    # print(f'Current val loss: {current_val_loss}')\n",
    "    # print(f'Best val loss: {best_val_loss}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # if current_val_loss < best_val_loss:\n",
    "    #     best_val_loss = current_val_loss\n",
    "    #     patience_counter = 0  # reset patience counter\n",
    "    #     # Save the best model\n",
    "    #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(\"!!!Early stopping triggered\")\n",
    "    #         torch.save({\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #         'model': model.state_dict(),\n",
    "    #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "    #         break  # Early stopping\n",
    "    \n",
    "    \n",
    "    \n",
    "print('==='*10)\n",
    "# torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "             train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, e+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, e+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epoch,\n",
    "    'train_loss': train_epoch_loss,\n",
    "    'test_loss': test_epoch_loss\n",
    "}, f'saved_model1115/final_MIC-{drug}_full.pth')\n",
    "\n",
    "#%%\n",
    "# testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "# testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "# testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "# Ensure the model is on the same device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test, y_test_res in testing_loader1:\n",
    "        # Move input and target data to the correct device\n",
    "        x_test = x_test.to(device).float()\n",
    "        y_test = y_test.to(device).float()\n",
    "        y_test_res = y_test_res.to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(x_test, y_test_res)\n",
    "        \n",
    "        # Append predictions and targets to lists\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "        \n",
    "# Flatten the target list\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min.values-1, target_max.values+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "# Calculate AUC\n",
    "cutoff = 4\n",
    "test_target_bi = (target_list >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sailency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = f\"saved_model1115/final_MIC-{drug}_full.pth\"  # update <DRUG> as appropriate\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "# If you're on GPU, you can load to GPU by changing map_location or ignoring it, \n",
    "# but it's often good practice to specify map_location in case the saved file was on a different device.\n",
    "\n",
    "# 4. Restore model and optimizer state\n",
    "#input parameter\n",
    "lr = 1e-4\n",
    "epoch = 800\n",
    "conv_dropout_rate=0.4\n",
    "dense_dropout_rate=0.7\n",
    "weight_decay=1e-4\n",
    "######################################\n",
    "\n",
    "model_ = Model(\n",
    "num_classes=6,\n",
    "num_filters=64,\n",
    "num_conv_layers=2,\n",
    "# num_dense_neurons=256, # batch_size = 64\n",
    "num_dense_neurons=128, # batch_size = 64\n",
    "num_dense_layers=2,\n",
    "return_logits=False,\n",
    "conv_dropout_rate=conv_dropout_rate,\n",
    "dense_dropout_rate=dense_dropout_rate\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model_.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "model_.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ex shape: torch.Size([1, 1, 1710])\n",
      "y_ex shape: torch.Size([1])\n",
      "y_ex_res shape: torch.Size([1])\n",
      "Predicted class: 4\n"
     ]
    }
   ],
   "source": [
    "model_.eval()\n",
    "\n",
    "# Make sure model is defined and on the correct device\n",
    "# Make sure x_ex and y_ex_res are also on the same device.\n",
    "\n",
    "# Get one batch of size=1 or a small batch from testing_loader1\n",
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "test_iter = iter(testing_loader1)\n",
    "x_ex, y_ex, y_ex_res = next(test_iter)\n",
    "\n",
    "# Move data to GPU if available\n",
    "x_ex = x_ex.to(device).float()\n",
    "y_ex = y_ex.to(device).float()\n",
    "y_ex_res = y_ex_res.to(device).float()\n",
    "\n",
    "model_.eval()  # ensure model is in eval mode\n",
    "\n",
    "print(\"x_ex shape:\", x_ex.shape)        # e.g., [1, 1, seq_len]\n",
    "print(\"y_ex shape:\", y_ex.shape)        # e.g., [1]\n",
    "print(\"y_ex_res shape:\", y_ex_res.shape)# e.g., [1]\n",
    "\n",
    "# 2) Do a forward pass to get logits\n",
    "logits = model_(x_ex, y_ex_res)  # shape = [batch_size, num_classes]\n",
    "\n",
    "# 3) Now you can define predicted_class\n",
    "predicted_class = logits.argmax(dim=1).item()  # for batch_size=1\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_wrapper(inputs):\n",
    "    \"\"\"\n",
    "    inputs: a tuple (x_batch, res_batch)\n",
    "    \"\"\"\n",
    "    x_batch, res_batch = inputs\n",
    "    # Forward pass\n",
    "    logits = model_(x_batch, res_batch)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 1 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward_wrapper() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_56087/1375472604.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Captum call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m saliency_map = saliency.attribute(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mx_ex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ex_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# the input tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredicted_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/captum/log/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/captum/attr/_core/saliency.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, target, abs, additional_forward_args)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# No need to format additional_forward_args here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# They are being formated in the `_run_forward` function in `common.py`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         gradients = self.gradient_func(\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         )\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/captum/_utils/gradient.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# runs forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         assert outputs[0].numel() == 1, (\n\u001b[1;32m    114\u001b[0m             \u001b[0;34m\"Target not provided when necessary, cannot\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/ml-workshop/lib/python3.9/site-packages/captum/_utils/common.py\u001b[0m in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0madditional_forward_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_format_additional_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madditional_forward_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m     output = forward_func(\n\u001b[0m\u001b[1;32m    532\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_forward_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madditional_forward_args\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward_wrapper() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "saliency = Saliency(forward_wrapper)\n",
    "\n",
    "# We'll pass the tuple (x_ex, y_ex_res) and specify a target\n",
    "predicted_class = logits.argmax(dim=1).item()\n",
    "\n",
    "# NOTE: We typically do a separate forward pass first to get predicted_class\n",
    "# but you already have `logits` from above:\n",
    "predicted_class = logits.argmax(dim=1).item()\n",
    "\n",
    "# Captum call\n",
    "saliency_map = saliency.attribute(\n",
    "    (x_ex, y_ex_res),  # the input tuple\n",
    "    target=predicted_class\n",
    ")\n",
    "\n",
    "# saliency_map is a tuple of attributions for each input element:\n",
    "#   (attribution for x_ex, attribution for y_ex_res)\n",
    "attribution_x, attribution_res = saliency_map\n",
    "attribution_x = attribution_x.abs().squeeze()\n",
    "\n",
    "print(\"Saliency map (x) shape:\", attribution_x.shape)  # e.g. [seq_len]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'forward_wrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_56087/727934969.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIntegratedGradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# We'll reuse the same 'predicted_class' from above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredicted_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'forward_wrapper' is not defined"
     ]
    }
   ],
   "source": [
    "ig = IntegratedGradients(forward_wrapper)\n",
    "\n",
    "# We'll reuse the same 'predicted_class' from above\n",
    "predicted_class = logits.argmax(dim=1).item()\n",
    "\n",
    "# attribute returns a tuple (attr_x, attr_res)\n",
    "# n_steps controls how many steps to use in the integration\n",
    "ig_attr = ig.attribute(\n",
    "    (x_ex, y_ex_res),\n",
    "    target=predicted_class,\n",
    "    n_steps=50,  # typical range is [20..200]\n",
    ")\n",
    "\n",
    "attr_x_ig, attr_res_ig = ig_attr\n",
    "\n",
    "# We often take absolute value or keep signed values\n",
    "attr_x_ig = attr_x_ig.squeeze().detach().cpu()\n",
    "\n",
    "print(\"Integrated Gradients shape for x:\", attr_x_ig.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = model.conv_layers2[-1][1]  # \n",
    "# Explanation:\n",
    "# model.conv_layers2[-1] is a Sequential: (Dropout, Conv1D, BatchNorm1d, ReLU)\n",
    "# If you just want the Conv1d layer, that might be index [1] in that sub-Sequential.\n",
    "cam = LayerGradCam(forward_wrapper, target_layer)\n",
    "\n",
    "# same predicted_class as before\n",
    "predicted_class = logits.argmax(dim=1).item()\n",
    "\n",
    "cam_attribution = cam.attribute(\n",
    "    (x_ex, y_ex_res),\n",
    "    target=predicted_class\n",
    ")\n",
    "\n",
    "# cam_attribution is the \"Grad-CAM map\" for the chosen conv layer\n",
    "# shape might be [1, number_of_filters, conv_output_length]\n",
    "print(\"Grad-CAM raw shape:\", cam_attribution.shape)\n",
    "# Suppose cam_attribution is [1, C, L] for 1D\n",
    "# We can average across channels or keep them separate\n",
    "cam_map = cam_attribution.mean(dim=1, keepdim=True)  # [1, 1, L]\n",
    "\n",
    "# Now upsample to the original seq length, e.g. using F.interpolate\n",
    "seq_len = x_ex.shape[-1]\n",
    "cam_map_upsampled = F.interpolate(cam_map, size=seq_len, mode='linear', align_corners=False)\n",
    "\n",
    "gradcam_1d = cam_map_upsampled.squeeze().detach().cpu().numpy()  # shape [seq_len]\n",
    "print(\"Grad-CAM final shape:\", gradcam_1d.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_0.0001_weighted_balanced.png-emb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30698/3325110700.py:16: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "Optimizer details:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "Learning rate: 0.0001\n",
      "Weight decay: 0.0001\n",
      "======================\n",
      "Accuracy: 0.47845206684256814\n",
      "Mae: 0.6314863676341249\n",
      "F1 Score: 0.38225601712238505\n",
      "conf_matrix: [[ 10  95   1   3   2   1]\n",
      " [ 17 366   2   6   5   1]\n",
      " [  7 245   9  17  18   0]\n",
      " [  1  35   2  37  73   1]\n",
      " [  1   6   0  17 102  11]\n",
      " [  0   3   1   0  22  20]]\n",
      "======================\n",
      "Doubling Dilution Accuracy: 0.9190853122251539\n",
      "AUC: 0.8705622572773826\n",
      "Sensitivity: 0.8469945355191257\n",
      "Specificity: 0.8941299790356394\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, e+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, e+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epoch,\n",
    "    'train_loss': train_epoch_loss,\n",
    "    'test_loss': test_epoch_loss\n",
    "}, f'saved_model1115/final_MIC-{drug}_full.pth')\n",
    "\n",
    "#%%\n",
    "# testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "# testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "# testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "# Ensure the model is on the same device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test, y_test_res in testing_loader1:\n",
    "        # Move input and target data to the correct device\n",
    "        x_test = x_test.to(device).float()\n",
    "        y_test = y_test.to(device).float()\n",
    "        y_test_res = y_test_res.to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(x_test, y_test_res)\n",
    "        \n",
    "        # Append predictions and targets to lists\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "        \n",
    "# Flatten the target list\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "\n",
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min.values-1, target_max.values+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "# Calculate AUC\n",
    "cutoff = 4\n",
    "test_target_bi = (target_list >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30698/1703092247.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdoubling_dilution_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mis_within_doubling_dilution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_max\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Doubling Dilution Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoubling_dilution_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_list' is not defined"
     ]
    }
   ],
   "source": [
    "def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "    _ = np.arange(target_min.values-1, target_max.values+2, 1)\n",
    "    index = [i for i, x in enumerate(_) if x == target][0]\n",
    "    return (_[index-1] <= pred <= _[index+1])\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8752362786541569\n",
      "Sensitivity: 0.8961748633879781\n",
      "Specificity: 0.8542976939203354\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate AUC\n",
    "cutoff = 4\n",
    "test_target_bi = (target_list >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './saved_model1115/emb_resFeed_working17122024.pth'\n",
    "# torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred, true in zip(pred_list, target_list):\n",
    "    print(pred, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_y = []\n",
    "errors_y_ = []\n",
    "for y_, y in zip(pred_list, target_list):\n",
    "    # if y_ != y:\n",
    "    if y not in [y_, y_+1, y_-1]:\n",
    "        errors_y.append(y)\n",
    "        errors_y_.append(y_)\n",
    "        \n",
    "for a, b in zip(errors_y, errors_y_):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7.,  0., 48.,  0.,  2.,  0.,  3.,  0., 51., 12.]),\n",
       " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(errors_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = []\n",
    "for a, b in zip(errors_y, errors_y_):\n",
    "    joint.append(f'{a}_{b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
       " [Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, '')])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(15, 6), dpi=80)\n",
    "plt.hist(joint, width=0.8)\n",
    "plt.xticks(rotation=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([13.,  0.,  9.,  0., 44.,  0., 42.,  0.,  9.,  6.]),\n",
       " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ],\n",
       "       dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(errors_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0 1\n",
      "2.0 4\n",
      "2.0 5\n",
      "3.0 5\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "2.0 4\n",
      "5.0 1\n",
      "4.0 1\n",
      "3.0 1\n",
      "0.0 4\n",
      "2.0 0\n",
      "3.0 1\n",
      "4.0 1\n",
      "2.0 0\n",
      "3.0 1\n",
      "2.0 0\n",
      "5.0 1\n",
      "3.0 1\n",
      "2.0 4\n",
      "5.0 1\n",
      "3.0 1\n",
      "3.0 1\n",
      "0.0 5\n",
      "1.0 5\n",
      "2.0 4\n",
      "0.0 3\n",
      "0.0 4\n",
      "2.0 4\n",
      "1.0 4\n",
      "1.0 4\n",
      "3.0 1\n",
      "0.0 3\n",
      "3.0 1\n",
      "2.0 4\n",
      "0.0 2\n",
      "3.0 1\n",
      "0.0 4\n",
      "1.0 3\n",
      "4.0 1\n",
      "3.0 1\n",
      "0.0 4\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "3.0 5\n",
      "2.0 4\n",
      "4.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "3.0 1\n",
      "3.0 1\n",
      "3.0 1\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 0\n",
      "4.0 1\n",
      "2.0 4\n",
      "3.0 5\n",
      "0.0 4\n",
      "2.0 4\n",
      "0.0 4\n",
      "2.0 4\n",
      "2.0 4\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "3.0 1\n",
      "5.0 0\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "2.0 4\n",
      "0.0 5\n",
      "3.0 1\n",
      "2.0 4\n",
      "3.0 1\n",
      "3.0 1\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "3.0 5\n",
      "2.0 4\n",
      "3.0 1\n",
      "3.0 1\n",
      "1.0 4\n",
      "2.0 5\n",
      "3.0 5\n",
      "3.0 1\n",
      "2.0 0\n",
      "2.0 4\n",
      "3.0 0\n",
      "0.0 4\n",
      "1.0 4\n",
      "3.0 1\n",
      "3.0 5\n",
      "4.0 1\n",
      "3.0 1\n",
      "2.0 4\n",
      "3.0 1\n",
      "2.0 4\n",
      "3.0 1\n",
      "4.0 1\n",
      "3.0 5\n",
      "3.0 1\n",
      "3.0 1\n",
      "3.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "4.0 1\n",
      "2.0 4\n",
      "3.0 1\n",
      "2.0 4\n",
      "0.0 2\n",
      "1.0 4\n",
      "2.0 4\n",
      "1.0 4\n",
      "4.0 1\n",
      "5.0 1\n",
      "2.0 4\n",
      "2.0 4\n",
      "1.0 4\n"
     ]
    }
   ],
   "source": [
    "for a, b in zip(errors_y, errors_y_):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './saved_model1115/resFeed1.pth'\n",
    "torch.save(model.state_dict(), PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================\n",
      "Optimizer details:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    lr: 0.0001\n",
      "    maximize: False\n",
      "    weight_decay: 0.0001\n",
      ")\n",
      "Learning rate: 0.0001\n",
      "Weight decay: 0.0001\n",
      "======================\n",
      "Accuracy: 0.5224274406332454\n",
      "Mae: 0.503957783641161\n",
      "F1 Score: 0.48083488925470447\n",
      "conf_matrix: [[ 18  86   8   0   0   0]\n",
      " [ 48 330  19   0   0   0]\n",
      " [ 15 230  51   0   0   0]\n",
      " [  0   0   0 104  44   1]\n",
      " [  0   0   0  52  65  20]\n",
      " [  0   0   0   6  14  26]]\n",
      "======================\n",
      "Doubling Dilution Accuracy: 0.9736147757255936\n",
      "AUC: 0.8179451489844314\n",
      "Sensitivity: 0.6830601092896175\n",
      "Specificity: 0.9528301886792453\n"
     ]
    }
   ],
   "source": [
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "# Ensure the model is on the same device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test, y_test_res in testing_loader1:\n",
    "        # Move input and target data to the correct device\n",
    "        x_test = x_test.to(device).float()\n",
    "        y_test = y_test.to(device).float()\n",
    "        y_test_res = y_test_res.to(device).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(x_test, y_test_res)\n",
    "        \n",
    "        # Append predictions and targets to lists\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "        \n",
    "# Flatten the target list\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "\n",
    "# Calculate AUC\n",
    "cutoff = 4\n",
    "test_target_bi = (target_list >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypterparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 50/400 [02:56<20:30,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Training loss: 0.1510869264602661\n",
      "Validation loss: 0.13646160066127777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 100/400 [05:52<17:28,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100\n",
      "Training loss: 0.12380748987197876\n",
      "Validation loss: 0.11622773110866547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 114/400 [06:41<16:51,  3.54s/it]"
     ]
    }
   ],
   "source": [
    "#input parameter\n",
    "for _ in [1e-4, 5e-4,1e-3, 5e-3,1e-2]:\n",
    "    lr = 1e-4\n",
    "    epoch = 400\n",
    "    conv_dropout_rate=0.4\n",
    "    dense_dropout_rate=0.7\n",
    "    weight_decay= _ #1e-4\n",
    "    ######################################\n",
    "\n",
    "    model = Model(\n",
    "    num_classes=6,\n",
    "    num_filters=64,\n",
    "    num_conv_layers=2,\n",
    "    # num_dense_neurons=256, # batch_size = 64\n",
    "    num_dense_neurons=128, # batch_size = 64\n",
    "    num_dense_layers=2,\n",
    "    return_logits=False,\n",
    "    conv_dropout_rate=conv_dropout_rate,\n",
    "    dense_dropout_rate=dense_dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "    # model = Model( #! way too memory intensive\n",
    "    # num_classes=13,\n",
    "    # num_filters=128,\n",
    "    # num_conv_layers=2,\n",
    "    # num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "    # num_dense_layers=2,\n",
    "    # return_logits=True,\n",
    "    # conv_dropout_rate=0,\n",
    "    # dense_dropout_rate=0\n",
    "    # ).to(device)\n",
    "    ## early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "    patience_counter = 0\n",
    "    lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "    batch_size = 64\n",
    "    # lr = 0.0085\n",
    "    # lr = 0.00002\n",
    "    lr = lr\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "    test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "    # train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "    # test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "    # criterion = nn.MSELoss()\n",
    "    # criterion = masked_weighted_MAE\n",
    "    # criterion = masked_weighted_MSE\n",
    "    criterion = weighted_cross_entropy_loss_fn\n",
    "    # criterion = masked_MAE\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "    # scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "    #%%\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    import gc; gc.collect()\n",
    "    # ic.enable()\n",
    "    ic.disable()\n",
    "\n",
    "    train_epoch_loss = []\n",
    "    test_epoch_loss = []\n",
    "\n",
    "    for e in tqdm(range(1, epoch+1)):\n",
    "        model.train()\n",
    "        train_batch_loss = []\n",
    "        test_batch_loss = []\n",
    "        # print(f'Epoch {e}')\n",
    "        for x_train, y_train, y_train_res in train_loader:\n",
    "            x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "            y_batch = y_train.to(device)\n",
    "            y_batch_res = y_train_res.to(device)\n",
    "            \n",
    "            x_batch = x_batch.float()\n",
    "            pred = model(x_batch.float(),y_batch_res.float())\n",
    "\n",
    "            # break\n",
    "            # loss_train = loss_corn(pred, y_batch, 3, class_weights)\n",
    "            # print(pred, y_batch)\n",
    "            loss_train = criterion(pred,y_batch)\n",
    "            # print(pred)\n",
    "            # print(y_batch)\n",
    "            # print(loss_train)\n",
    "            train_batch_loss.append(loss_train)        \n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()  # Update the learning rate\n",
    "            # break\n",
    "        train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # print('>> test')\n",
    "            for x_test, y_test, y_test_res in test_loader:\n",
    "                x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "                x_batch = x_batch.float()\n",
    "                y_batch = y_test.to(device)\n",
    "                y_batch_res = y_test_res.to(device)\n",
    "                # print(x_batch.size())\n",
    "                # y_batch = torch.Tensor.float(y).to(device)\n",
    "                # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "                pred = model(x_batch.float(), y_batch_res.float())\n",
    "                loss_test = criterion(pred,y_batch)\n",
    "                # pred = pred.unsqueeze(0)\n",
    "                # print(pred[:10])\n",
    "                # print(y_batch[:10])\n",
    "\n",
    "                # loss_test = loss_corn(pred, y_batch, 3, class_weights)\n",
    "                test_batch_loss.append(loss_test)\n",
    "            test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "        if e%50 == 0:\n",
    "            print(f'Epoch {e}')\n",
    "            print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "            print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "        # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "        # print(train_batch_loss)\n",
    "        # print(test_batch_loss)\n",
    "        # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "        # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "        # #! implementing early stopping\n",
    "        # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "        # print(f'Current val loss: {current_val_loss}')\n",
    "        # print(f'Best val loss: {best_val_loss}')\n",
    "        # if current_val_loss < best_val_loss:\n",
    "        #     best_val_loss = current_val_loss\n",
    "        #     patience_counter = 0  # reset patience counter\n",
    "        #     # Save the best model\n",
    "        #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     if patience_counter >= patience:\n",
    "        #         print(\"Early stopping triggered\")\n",
    "        #         torch.save({\n",
    "        #         'optimizer': optimizer.state_dict(),\n",
    "        #         'model': model.state_dict(),\n",
    "        #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "        #         break  # Early stopping\n",
    "            \n",
    "    print('==='*10)\n",
    "    # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "    save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "                train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    x = np.arange(1, epoch+1, 1)\n",
    "    ax.plot(x, train_epoch_loss,label='Training')\n",
    "    ax.plot(x, test_epoch_loss,label='Validation')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Number of Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "    ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "    # ax_2 = ax.twinx()\n",
    "    # ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "    # ax_2.set_yscale(\"log\")\n",
    "    # ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "    ax.grid(axis=\"x\")\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "    print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "    #%%\n",
    "    # testing_dataset = Dataset(test_data, test_target, 'EMB_MIC_x','EMB_MIC_y',one_hot_dtype=torch.float, transform=False)\n",
    "    testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Ensure the model is on the same device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    ic.disable()\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    target_list  = []\n",
    "    mse_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test, y_test_res in testing_loader1:\n",
    "            # Move input and target data to the correct device\n",
    "            x_test = x_test.to(device).float()\n",
    "            y_test = y_test.to(device).float()\n",
    "            y_test_res = y_test_res.to(device).float()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(x_test, y_test_res)\n",
    "            \n",
    "            # Append predictions and targets to lists\n",
    "            pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "            target_list.append(y_test.detach().cpu().numpy())\n",
    "            \n",
    "    # Flatten the target list\n",
    "    target_list = np.array(target_list).flatten()\n",
    "\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "    from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "    def calculate_metrics(true_labels, predictions):\n",
    "        \"\"\"\n",
    "        Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "        Parameters:\n",
    "        - true_labels: List or array of true labels\n",
    "        - predictions: List or array of predicted labels\n",
    "\n",
    "        Returns:\n",
    "        - accuracy: Overall accuracy of predictions\n",
    "        - f1: Weighted average F1 score\n",
    "        - conf_matrix: Multiclass confusion matrix\n",
    "        - mae: Mean Absolute Error of predictions\n",
    "        \"\"\"\n",
    "        # Ensure inputs are numpy arrays for consistency\n",
    "        true_labels = np.array(true_labels)\n",
    "        predictions = np.array(predictions)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "        # Calculate MAE\n",
    "        mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "        return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "    # Example usage\n",
    "    # true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "    # predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "    accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "    print(\"======================\")\n",
    "    # print(\"Model's Named Parameters:\")\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     print(f\"Name: {name}\")\n",
    "    #     print(f\"Shape: {param.size()}\")\n",
    "    #     print(f\"Requires grad: {param.requires_grad}\")\n",
    "    #     print('-----')\n",
    "    print(\"Optimizer details:\")\n",
    "    print(optimizer)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"Learning rate:\", param_group['lr'])\n",
    "        print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "        \n",
    "    print(\"======================\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Mae: {mae}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"conf_matrix: {conf_matrix}\")\n",
    "    print(\"======================\")\n",
    "    doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "    print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)\n",
    "\n",
    "\n",
    "    # Calculate AUC\n",
    "    cutoff = 4\n",
    "    test_target_bi = (target_list >= cutoff).astype(int)\n",
    "    test_predictions_bi = (np.array(pred_list) >= cutoff).astype(int)\n",
    "\n",
    "    auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "    print(\"AUC:\", auc)\n",
    "\n",
    "    # Calculate confusion matrix components\n",
    "    tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "    # Calculate sensitivity (recall)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "    # Calculate specificity\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(\"Specificity:\", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One cycle lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 50/400 [02:28<17:21,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "Training loss: 0.11537401378154755\n",
      "Validation loss: 0.14441072940826416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 100/400 [04:58<14:56,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100\n",
      "Training loss: 0.11210744082927704\n",
      "Validation loss: 0.1536770612001419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 150/400 [07:28<12:28,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150\n",
      "Training loss: 0.11056072264909744\n",
      "Validation loss: 0.16507422924041748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 200/400 [09:57<09:53,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200\n",
      "Training loss: 0.1097438856959343\n",
      "Validation loss: 0.17116251587867737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 250/400 [12:32<07:39,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250\n",
      "Training loss: 0.10918270796537399\n",
      "Validation loss: 0.1748015433549881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 300/400 [15:02<04:53,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300\n",
      "Training loss: 0.10868095606565475\n",
      "Validation loss: 0.17704497277736664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 350/400 [17:41<02:47,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350\n",
      "Training loss: 0.10827459394931793\n",
      "Validation loss: 0.18684180080890656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [20:27<00:00,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400\n",
      "Training loss: 0.10818441957235336\n",
      "Validation loss: 0.17728930711746216\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_1e-07_weighted_balanced.png-emb\n",
      "======================\n",
      "Optimizer details:\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    base_momentum: 0.85\n",
      "    betas: (0.9499999993756804, 0.999)\n",
      "    capturable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    initial_lr: 0.0004\n",
      "    lr: 4.006243171376002e-08\n",
      "    max_lr: 0.01\n",
      "    max_momentum: 0.95\n",
      "    maximize: False\n",
      "    min_lr: 4e-08\n",
      "    weight_decay: 0\n",
      ")\n",
      "Learning rate: 4.006243171376002e-08\n",
      "Weight decay: 0\n",
      "======================\n",
      "Accuracy: 0.47568523430592397\n",
      "Mae: 0.6578249336870027\n",
      "F1 Score: 0.44584107920478183\n",
      "conf_matrix: [[ 34  66   7   3   1   1]\n",
      " [ 63 285  32   9   5   0]\n",
      " [ 40 188  41  19   3   2]\n",
      " [  4  20   7  75  42   1]\n",
      " [  2   8   0  34  77  16]\n",
      " [  0   3   0   5  12  26]]\n",
      "======================\n",
      "Doubling Dilution Accuracy: 0.8992042440318302\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#input parameter\n",
    "lr = 1e-7\n",
    "epoch = 400\n",
    "conv_dropout_rate=0\n",
    "dense_dropout_rate=0\n",
    "# weight_decay=1e-8\n",
    "weight_decay=0\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "patience_counter = 0\n",
    "lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "batch_size = 128\n",
    "# lr = 0.0085\n",
    "# lr = 0.00002\n",
    "lr = lr\n",
    "######################################\n",
    "\n",
    "model = Model(\n",
    "num_classes=6,\n",
    "num_filters=64,\n",
    "num_conv_layers=2,\n",
    "# num_dense_neurons=256, # batch_size = 64\n",
    "num_dense_neurons=128, # batch_size = 64\n",
    "num_dense_layers=2,\n",
    "return_logits=False,\n",
    "conv_dropout_rate=conv_dropout_rate,\n",
    "dense_dropout_rate=dense_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "criterion = weighted_cross_entropy_loss_fn\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=epoch)\n",
    "# model = Model( #! way too memory intensive\n",
    "# num_classes=13,\n",
    "# num_filters=128,\n",
    "# num_conv_layers=2,\n",
    "# num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "# num_dense_layers=2,\n",
    "# return_logits=True,\n",
    "# conv_dropout_rate=0,\n",
    "# dense_dropout_rate=0\n",
    "# ).to(device)\n",
    "## early stopping\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "# test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = masked_weighted_MAE\n",
    "# criterion = masked_weighted_MSE\n",
    "# criterion = weighted_cross_entropy_loss_fn\n",
    "\n",
    "\n",
    "# criterion = masked_MAE\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "#%%\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "# ic.enable()\n",
    "ic.disable()\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    # print(f'Epoch {e}')\n",
    "    for x_train, y_train in train_loader:\n",
    "        x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "        y_batch = y_train.to(device)\n",
    "        x_batch = x_batch.float()\n",
    "        pred = model(x_batch.float())\n",
    "\n",
    "        # break\n",
    "        loss_train = criterion(pred,y_batch)\n",
    "\n",
    "        train_batch_loss.append(loss_train)        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update the learning rate\n",
    "\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # print('>> test')\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            y_batch = y_test.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float())\n",
    "\n",
    "            # pred = pred.unsqueeze(0)\n",
    "            # print(pred[:10])\n",
    "            # print(y_batch[:10])\n",
    "\n",
    "            loss_test = criterion(pred,y_batch)\n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "    if e % 50 == 0:\n",
    "        print(f'Epoch {e}')\n",
    "        print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "        print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    # #! implementing early stopping\n",
    "    # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "    # print(f'Current val loss: {current_val_loss}')\n",
    "    # print(f'Best val loss: {best_val_loss}')\n",
    "    # if current_val_loss < best_val_loss:\n",
    "    #     best_val_loss = current_val_loss\n",
    "    #     patience_counter = 0  # reset patience counter\n",
    "    #     # Save the best model\n",
    "    #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         torch.save({\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #         'model': model.state_dict(),\n",
    "    #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "    #         break  # Early stopping\n",
    "        \n",
    "print('==='*10)\n",
    "# torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "             train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, epoch+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "#%%\n",
    "\n",
    "model.eval()  # For inference\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in testing_loader1:\n",
    "        xtest1 = x_test.to(device).float()\n",
    "        ytest1 = y_test.to(device).float()\n",
    "        pred = model(xtest1)\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclical lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input parameter\n",
    "lr = 1e-7\n",
    "epoch = 250\n",
    "conv_dropout_rate=0.05\n",
    "dense_dropout_rate=0.5\n",
    "weight_decay=1e-8\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-9, max_lr=0.001)\n",
    "######################################\n",
    "\n",
    "model = Model(\n",
    "num_classes=3,\n",
    "num_filters=64,\n",
    "num_conv_layers=2,\n",
    "# num_dense_neurons=256, # batch_size = 64\n",
    "num_dense_neurons=128, # batch_size = 64\n",
    "num_dense_layers=2,\n",
    "return_logits=False,\n",
    "conv_dropout_rate=conv_dropout_rate,\n",
    "dense_dropout_rate=dense_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# model = Model( #! way too memory intensive\n",
    "# num_classes=13,\n",
    "# num_filters=128,\n",
    "# num_conv_layers=2,\n",
    "# num_dense_neurons=64, # batch_size = 64\n",
    "\n",
    "# num_dense_layers=2,\n",
    "# return_logits=True,\n",
    "# conv_dropout_rate=0,\n",
    "# dense_dropout_rate=0\n",
    "# ).to(device)\n",
    "## early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 8  # How many epochs to wait after last time validation loss improved.\n",
    "patience_counter = 0\n",
    "lmbda = torch.tensor(1e-4, dtype = torch.float32)\n",
    "\n",
    "batch_size = 128\n",
    "# lr = 0.0085\n",
    "# lr = 0.00002\n",
    "lr = lr\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True ,num_workers=8, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_padded_batch ,num_workers=8, drop_last=True)\n",
    "# test_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=collate_padded_batch, num_workers=8, shuffle=True, drop_last=True)\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = masked_weighted_MAE\n",
    "# criterion = masked_weighted_MSE\n",
    "# criterion = weighted_cross_entropy_loss_fn\n",
    "# criterion = masked_MAE\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,  weight_decay=weight_decay)\n",
    "# scheduler = CyclicLR(optimizer, base_lr=1e-8, max_lr=1e-4, step_size_up=200, mode='triangular', cycle_momentum=False)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbo\n",
    "#%%\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc; gc.collect()\n",
    "# ic.enable()\n",
    "ic.disable()\n",
    "\n",
    "train_epoch_loss = []\n",
    "test_epoch_loss = []\n",
    "\n",
    "for e in tqdm(range(1, epoch+1)):\n",
    "    model.train()\n",
    "    train_batch_loss = []\n",
    "    test_batch_loss = []\n",
    "    # print(f'Epoch {e}')\n",
    "    for x_train, y_train in train_loader:\n",
    "        x_batch = torch.squeeze(x_train, 0).to(device)\n",
    "        y_batch = y_train.to(device)\n",
    "        x_batch = x_batch.float()\n",
    "        pred = model(x_batch.float())\n",
    "\n",
    "        # break\n",
    "        loss_train = criterion(pred,y_batch)\n",
    "\n",
    "        train_batch_loss.append(loss_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update the learning rate\n",
    "\n",
    "    train_epoch_loss.append(torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # print('>> test')\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_batch = torch.squeeze(x_test, 0).to(device)\n",
    "            x_batch = x_batch.float()\n",
    "            y_batch = y_test.to(device)\n",
    "            # print(x_batch.size())\n",
    "            # y_batch = torch.Tensor.float(y).to(device)\n",
    "            # x_batch = x_batch.permute(0, 3, 1, 2).to(device)\n",
    "            pred = model(x_batch.float())\n",
    "            loss_test = loss_corn(pred, y_batch, 3, class_weights)\n",
    "\n",
    "            # pred = pred.unsqueeze(0)\n",
    "            # print(pred[:10])\n",
    "            # print(y_batch[:10])\n",
    "\n",
    "            test_batch_loss.append(loss_test)\n",
    "        test_epoch_loss.append(torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy())\n",
    "\n",
    "    print(f'Epoch {e}')\n",
    "    print(f\"Training loss: {torch.mean(torch.stack(train_batch_loss)).detach().cpu().numpy()}\")\n",
    "    print(f\"Validation loss: {torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()}\") \n",
    "    # scheduler.step(torch.mean(torch.stack(test_batch_loss)))\n",
    "    # print(train_batch_loss)\n",
    "    # print(test_batch_loss)\n",
    "    # print(f\"Training loss: {np.mean(train_batch_loss)}\")\n",
    "    # print(f\"Validation loss: {np.mean(test_batch_loss)}\")\n",
    "    # #! implementing early stopping\n",
    "    # current_val_loss = torch.mean(torch.stack(test_batch_loss)).detach().cpu().numpy()\n",
    "    # print(f'Current val loss: {current_val_loss}')\n",
    "    # print(f'Best val loss: {best_val_loss}')\n",
    "    # if current_val_loss < best_val_loss:\n",
    "    #     best_val_loss = current_val_loss\n",
    "    #     patience_counter = 0  # reset patience counter\n",
    "    #     # Save the best model\n",
    "    #     # torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/aa-model_final.pth')\n",
    "\n",
    "    # else:\n",
    "    #     patience_counter += 1\n",
    "    #     if patience_counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         torch.save({\n",
    "    #         'optimizer': optimizer.state_dict(),\n",
    "    #         'model': model.state_dict(),\n",
    "    #     }, '/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/saved_models/aa-model_weighted_balanced_binned_aa_newdata.pth')\n",
    "    #         break  # Early stopping\n",
    "        \n",
    "print('==='*10)\n",
    "# torch.save(model.state_dict(), '/mnt/storageG1/lwang/Projects/tb_dr_MIC/saved_models/final_seq_model1-44ep.pt')\n",
    "save_to_file('trials3.txt', 'aa-training_weighted_balanced_ce-binned-EMB_newdata_corn' ,epoch, lr=lr, fcdr=dense_dropout_rate, l2=weight_decay, cnndr=conv_dropout_rate, \n",
    "             train_loss = train_epoch_loss, test_loss = test_epoch_loss, optimizer=optimizer, model = model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, epoch+1, 1)\n",
    "ax.plot(x, train_epoch_loss,label='Training')\n",
    "# ax.plot(x, test_epoch_loss,label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(0, epoch+1, 10))\n",
    "ax.set_title(f'Loss: Learning_rate:{lr}')\n",
    "# ax_2 = ax.twinx()\n",
    "# ax_2.plot(history[\"lr\"], \"k--\", lw=1)\n",
    "# ax_2.set_yscale(\"log\")\n",
    "# ax.set_ylim(ax.get_ylim()[0], history[\"training_losses\"][0])\n",
    "ax.grid(axis=\"x\")\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "fig.savefig(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced-emb.png')\n",
    "print(f'/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/graphs3/aa-loss_lr_{lr}_weighted_balanced.png-emb')\n",
    "\n",
    "#%%\n",
    "testing_dataset = Dataset(test_data, test_target, one_hot_dtype=torch.float, transform=False)\n",
    "testing_loader1 = DataLoader(dataset=testing_dataset, batch_size=1, collate_fn=collate_padded_batch, num_workers=1, shuffle=True, drop_last=True)\n",
    "\n",
    "model.eval()  # For inference\n",
    "\n",
    "ic.disable()\n",
    "model.eval()\n",
    "pred_list = []\n",
    "target_list  = []\n",
    "mse_list = []\n",
    "with torch.no_grad():\n",
    "    for x_test, y_test in testing_loader1:\n",
    "        xtest1 = x_test.to(device).float()\n",
    "        ytest1 = y_test.to(device).float()\n",
    "        pred = model(xtest1)\n",
    "        pred_list.append(np.argmax(pred.detach().cpu().numpy())) \n",
    "        target_list.append(y_test.detach().cpu().numpy())\n",
    "target_list = np.array(target_list).flatten()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error\n",
    "\n",
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, F1 score, confusion matrix, and MAE for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List or array of true labels\n",
    "    - predictions: List or array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: Overall accuracy of predictions\n",
    "    - f1: Weighted average F1 score\n",
    "    - conf_matrix: Multiclass confusion matrix\n",
    "    - mae: Mean Absolute Error of predictions\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays for consistency\n",
    "    true_labels = np.array(true_labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(true_labels, predictions)\n",
    "\n",
    "    return accuracy, f1, conf_matrix, mae\n",
    "\n",
    "# Example usage\n",
    "# true_labels = [0, 1, 2, 1, 0, 2, 1, 0]\n",
    "# predictions = [0, 2, 2, 1, 0, 0, 1, 0]\n",
    "\n",
    "accuracy, f1, conf_matrix, mae = calculate_metrics(target_list, pred_list)\n",
    "\n",
    "print(\"======================\")\n",
    "# print(\"Model's Named Parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Shape: {param.size()}\")\n",
    "#     print(f\"Requires grad: {param.requires_grad}\")\n",
    "#     print('-----')\n",
    "print(\"Optimizer details:\")\n",
    "print(optimizer)\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "    print(\"Weight decay:\", param_group.get('weight_decay', 'Not set'))\n",
    "    \n",
    "print(\"======================\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Mae: {mae}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"conf_matrix: {conf_matrix}\")\n",
    "print(\"======================\")\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(pred_list, target_list)])\n",
    "print(\"Doubling Dilution Accuracy:\", doubling_dilution_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_corn(logits, y_train, num_classes):\n",
    "    sets = []\n",
    "    for i in range(num_classes-1):\n",
    "        label_mask = y_train > i-1\n",
    "        label_tensor = (y_train[label_mask] > i).to(torch.int64)\n",
    "        sets.append((label_mask, label_tensor))\n",
    "\n",
    "    num_examples = 0\n",
    "    losses = 0.\n",
    "    for task_index, s in enumerate(sets):\n",
    "        train_examples = s[0]\n",
    "        train_labels = s[1]\n",
    "\n",
    "        if len(train_labels) < 1:\n",
    "            continue\n",
    "\n",
    "        num_examples += len(train_labels)\n",
    "        pred = logits[train_examples, task_index]\n",
    "\n",
    "        loss = -torch.sum(F.logsigmoid(pred)*train_labels\n",
    "                          + (F.logsigmoid(pred) - pred)*(1-train_labels)\n",
    "                          )\n",
    "        losses += loss\n",
    "    return losses/num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred  = torch.tensor([[0.0000, 0.5111],\n",
    "        [0.1329, 1.1051]], device='cuda:0')\n",
    "target = torch.tensor([0, 0], device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7275, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "out = loss_corn(pred, target, 3)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost with snps and fed in res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# def is_within_doubling_dilution(pred, target, target_min, target_max):\n",
    "#     # Ensure target_min and target_max are scalars\n",
    "#     target_min = target_min.item() if isinstance(target_min, np.ndarray) or isinstance(target_min, pd.Series) else target_min\n",
    "#     target_max = target_max.item() if isinstance(target_max, np.ndarray) or isinstance(target_max, pd.Series) else target_max\n",
    "\n",
    "#     # Create a range based on the scalar values of target_min and target_max\n",
    "#     dilution_range = np.arange(target_min - 1, target_max + 2, 1)\n",
    "    \n",
    "#     # Find the index of the target value\n",
    "#     index = np.where(dilution_range == target)[0][0]  # Use np.where to find the index\n",
    "    \n",
    "#     # Check if prediction is within the acceptable range\n",
    "#     return dilution_range[index - 1] <= pred <= dilution_range[index + 1]\n",
    "\n",
    "# Example usage\n",
    "target_min, target_max = cryptic_drs.min().values, cryptic_drs.max()\n",
    "\n",
    "# Load the data\n",
    "cryptic_drs = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_sample_drs_cryptic_emb.npy')\n",
    "cryptic_snps = np.load('/mnt/storageG1/lwang/Projects/tb_dr_MIC_v2/individual_models/1473_snps_cryptic_emb.npy')\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "cryptic_drs = pd.DataFrame(cryptic_drs)\n",
    "\n",
    "# Combine the features and target variable\n",
    "data = cryptic_snps\n",
    "target = cryptic_drs\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the XGBoost model\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "target_min, target_max = cryptic_drs.min().values, cryptic_drs.max().values\n",
    "\n",
    "doubling_dilution_accuracy = np.mean([is_within_doubling_dilution(pred, true, target_min, target_max) for pred, true in zip(y_pred, y_test)])\n",
    "print(f\"Doubling Dilution Accuracy: {doubling_dilution_accuracy * 100:.2f}%\")\n",
    "\n",
    "#testing\n",
    "cutoff = 4\n",
    "test_target_bi = (np.squeeze(np.array(y_test)) >= cutoff).astype(int) #(target_mic_list  >= cutoff).astype(int)\n",
    "test_predictions_bi = (np.squeeze(np.array(y_pred)) >= cutoff).astype(int)  #(np.array(pred_mic_list) >= cutoff).astype(int)\n",
    "\n",
    "auc = roc_auc_score(test_target_bi, test_predictions_bi)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "# Calculate confusion matrix components\n",
    "tn, fp, fn, tp = confusion_matrix(test_target_bi, test_predictions_bi).ravel()\n",
    "\n",
    "# Calculate sensitivity (recall)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
